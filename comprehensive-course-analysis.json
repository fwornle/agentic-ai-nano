{
  "analysis_timestamp": "1756565301.7451673",
  "target": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session6_Graph_Based_RAG.md",
  "overall_metrics": {
    "total_files": 1,
    "code_block_score": 60.0,
    "formatting_score": 100.0,
    "explanation_score": 73.63636363636363,
    "overall_score": 73.45454545454545,
    "critical_issues": 8,
    "total_issues": 29
  },
  "detailed_results": {
    "detect_large_code_blocks": {
      "success": true,
      "data": {
        "summary": {
          "total_files": 1,
          "files_needing_refactoring": 1,
          "total_large_blocks": 8
        },
        "files": [
          {
            "file": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session6_Graph_Based_RAG.md",
            "total_code_blocks": 110,
            "large_blocks_count": 8,
            "code_blocks": [
              {
                "start_line": 30,
                "end_line": 33,
                "language": "",
                "content": [
                  "Traditional RAG: Document \u2192 Chunks \u2192 Uniform Embeddings \u2192 Similarity Search",
                  "NodeRAG: Document \u2192 Specialized Nodes \u2192 Heterogeneous Graph \u2192 Reasoning Pathways"
                ],
                "line_count": 2
              },
              {
                "start_line": 75,
                "end_line": 93,
                "language": "python",
                "content": [
                  "def noderag_decomposition(document):",
                  "    \"\"\"NodeRAG Stage 1: Multi-granularity knowledge decomposition\"\"\"",
                  "",
                  "    # Parallel specialized extraction",
                  "    semantic_units = extract_semantic_concepts(document)     # Abstract themes and topics",
                  "    entities = extract_canonical_entities(document)          # People, orgs, locations with metadata",
                  "    relationships = extract_typed_relationships(document)     # Explicit connections with evidence",
                  "    attributes = extract_entity_properties(document)         # Quantitative and qualitative properties",
                  "    document_nodes = create_document_segments(document)      # Contextual source information",
                  "",
                  "    return {",
                  "        'semantic_units': semantic_units,",
                  "        'entities': entities,",
                  "        'relationships': relationships,",
                  "        'attributes': attributes,",
                  "        'document_nodes': document_nodes",
                  "    }"
                ],
                "line_count": 17
              },
              {
                "start_line": 99,
                "end_line": 108,
                "language": "python",
                "content": [
                  "def noderag_augmentation(decomposition_result):",
                  "    \"\"\"NodeRAG Stage 2: Heterogeneous graph construction\"\"\"",
                  "    ",
                  "    # Create typed connections between specialized nodes",
                  "    semantic_entity_links = link_concepts_to_entities(",
                  "        decomposition_result['semantic_units'],",
                  "        decomposition_result['entities']",
                  "    )"
                ],
                "line_count": 8
              },
              {
                "start_line": 112,
                "end_line": 118,
                "language": "python",
                "content": [
                  "    # Build HNSW similarity edges within the graph structure",
                  "    hnsw_similarity_edges = build_hnsw_graph_edges(",
                  "        all_nodes=decomposition_result,",
                  "        similarity_threshold=0.75",
                  "    )"
                ],
                "line_count": 5
              },
              {
                "start_line": 122,
                "end_line": 132,
                "language": "python",
                "content": [
                  "    # Cross-reference integration across node types",
                  "    cross_references = integrate_cross_type_references(decomposition_result)",
                  "",
                  "    return build_heterogeneous_graph(",
                  "        nodes=decomposition_result,",
                  "        typed_connections=semantic_entity_links,",
                  "        similarity_edges=hnsw_similarity_edges,",
                  "        cross_references=cross_references",
                  "    )"
                ],
                "line_count": 9
              },
              {
                "start_line": 138,
                "end_line": 154,
                "language": "python",
                "content": [
                  "def noderag_enrichment(heterogeneous_graph):",
                  "    \"\"\"NodeRAG Stage 3: Reasoning pathway construction\"\"\"",
                  "",
                  "    # Apply Personalized PageRank for semantic importance",
                  "    pagerank_scores = personalized_pagerank(",
                  "        graph=heterogeneous_graph,",
                  "        node_type_weights={",
                  "            'semantic_unit': 0.25,",
                  "            'entity': 0.30,",
                  "            'relationship': 0.20,",
                  "            'attribute': 0.10,",
                  "            'document': 0.10,",
                  "            'summary': 0.05",
                  "        }",
                  "    )"
                ],
                "line_count": 15
              },
              {
                "start_line": 158,
                "end_line": 165,
                "language": "python",
                "content": [
                  "    # Construct logical reasoning pathways",
                  "    reasoning_pathways = build_reasoning_pathways(",
                  "        graph=heterogeneous_graph,",
                  "        pagerank_scores=pagerank_scores,",
                  "        max_pathway_length=5",
                  "    )"
                ],
                "line_count": 6
              },
              {
                "start_line": 169,
                "end_line": 180,
                "language": "python",
                "content": [
                  "    # Optimize graph structure for reasoning performance",
                  "    optimized_graph = optimize_for_reasoning(",
                  "        heterogeneous_graph, reasoning_pathways",
                  "    )",
                  "",
                  "    return {",
                  "        'enriched_graph': optimized_graph,",
                  "        'reasoning_pathways': reasoning_pathways,",
                  "        'pagerank_scores': pagerank_scores",
                  "    }"
                ],
                "line_count": 10
              },
              {
                "start_line": 188,
                "end_line": 207,
                "language": "python",
                "content": [
                  "class NodeRAGPageRank:",
                  "    \"\"\"Personalized PageRank optimized for heterogeneous NodeRAG graphs\"\"\"",
                  "",
                  "    def compute_semantic_pathways(self, query_context, heterogeneous_graph):",
                  "        \"\"\"Compute query-aware semantic pathways using personalized PageRank\"\"\"",
                  "        ",
                  "        # Create personalization vector based on query relevance and node types",
                  "        personalization_vector = self.create_query_personalization(",
                  "            query_context=query_context,",
                  "            graph=heterogeneous_graph,",
                  "            node_type_weights={",
                  "                'semantic_unit': 0.3,  # High weight for concepts relevant to query",
                  "                'entity': 0.25,        # Moderate weight for concrete entities",
                  "                'relationship': 0.2,   # Important for connection discovery",
                  "                'attribute': 0.15,     # Properties provide specificity",
                  "                'summary': 0.1         # Synthesized insights",
                  "            }",
                  "        )"
                ],
                "line_count": 18
              },
              {
                "start_line": 211,
                "end_line": 220,
                "language": "python",
                "content": [
                  "        # Compute personalized PageRank with query bias",
                  "        pagerank_scores = nx.pagerank(",
                  "            heterogeneous_graph,",
                  "            alpha=0.85,  # Standard damping factor",
                  "            personalization=personalization_vector,",
                  "            max_iter=100,",
                  "            tol=1e-6",
                  "        )"
                ],
                "line_count": 8
              },
              {
                "start_line": 224,
                "end_line": 234,
                "language": "python",
                "content": [
                  "        # Extract top semantic pathways",
                  "        semantic_pathways = self.extract_top_pathways(",
                  "            graph=heterogeneous_graph,",
                  "            pagerank_scores=pagerank_scores,",
                  "            query_context=query_context,",
                  "            max_pathways=10",
                  "        )",
                  "        ",
                  "        return semantic_pathways"
                ],
                "line_count": 9
              },
              {
                "start_line": 240,
                "end_line": 250,
                "language": "python",
                "content": [
                  "    def extract_top_pathways(self, graph, pagerank_scores, query_context, max_pathways):",
                  "        \"\"\"Extract the most relevant semantic pathways for the query\"\"\"",
                  "        ",
                  "        # Find high-scoring nodes as pathway anchors",
                  "        top_nodes = sorted(",
                  "            pagerank_scores.items(),",
                  "            key=lambda x: x[1],",
                  "            reverse=True",
                  "        )[:50]"
                ],
                "line_count": 9
              },
              {
                "start_line": 254,
                "end_line": 268,
                "language": "python",
                "content": [
                  "        pathways = []",
                  "        for start_node, score in top_nodes:",
                  "            if len(pathways) >= max_pathways:",
                  "                break",
                  "",
                  "            # Use BFS to find semantic pathways from this anchor",
                  "            pathway = self.find_semantic_pathway(",
                  "                graph=graph,",
                  "                start_node=start_node,",
                  "                query_context=query_context,",
                  "                max_depth=4,",
                  "                pagerank_scores=pagerank_scores",
                  "            )"
                ],
                "line_count": 13
              },
              {
                "start_line": 272,
                "end_line": 288,
                "language": "python",
                "content": [
                  "            if pathway and len(pathway) > 1:",
                  "                pathways.append({",
                  "                    'pathway': pathway,",
                  "                    'anchor_score': score,",
                  "                    'pathway_coherence': self.calculate_pathway_coherence(pathway),",
                  "                    'query_relevance': self.calculate_query_relevance(pathway, query_context)",
                  "                })",
                  "",
                  "        # Rank pathways by combined score",
                  "        pathways.sort(",
                  "            key=lambda p: (p['pathway_coherence'] * p['query_relevance'] * p['anchor_score']),",
                  "            reverse=True",
                  "        )",
                  "",
                  "        return pathways[:max_pathways]"
                ],
                "line_count": 15
              },
              {
                "start_line": 294,
                "end_line": 318,
                "language": "python",
                "content": [
                  "class NodeRAGHNSW:",
                  "    \"\"\"HNSW similarity edges integrated into NodeRAG heterogeneous graphs\"\"\"",
                  "",
                  "    def build_hnsw_graph_integration(self, heterogeneous_graph, embedding_model):",
                  "        \"\"\"Build HNSW similarity edges within the existing graph structure\"\"\"",
                  "",
                  "        # Extract embeddings for all nodes by type",
                  "        node_embeddings = {}",
                  "        node_types = {}",
                  "",
                  "        for node_id, node_data in heterogeneous_graph.nodes(data=True):",
                  "            node_type = node_data.get('node_type')",
                  "            node_content = self.get_node_content_for_embedding(node_data, node_type)",
                  "",
                  "            # Generate specialized embeddings based on node type",
                  "            embedding = self.generate_typed_embedding(",
                  "                content=node_content,",
                  "                node_type=node_type,",
                  "                embedding_model=embedding_model",
                  "            )",
                  "",
                  "            node_embeddings[node_id] = embedding",
                  "            node_types[node_id] = node_type"
                ],
                "line_count": 23
              },
              {
                "start_line": 322,
                "end_line": 333,
                "language": "python",
                "content": [
                  "        # Build HNSW index with type-aware similarity",
                  "        hnsw_index = self.build_typed_hnsw_index(",
                  "            embeddings=node_embeddings,",
                  "            node_types=node_types,",
                  "            M=16,  # Number of bi-directional links for each node",
                  "            ef_construction=200,  # Size of the dynamic candidate list",
                  "            max_m=16,",
                  "            max_m0=32,",
                  "            ml=1 / np.log(2.0)",
                  "        )"
                ],
                "line_count": 10
              },
              {
                "start_line": 337,
                "end_line": 346,
                "language": "python",
                "content": [
                  "        # Add similarity edges to the existing heterogeneous graph",
                  "        similarity_edges_added = 0",
                  "        for node_id in heterogeneous_graph.nodes():",
                  "            # Find k most similar nodes using HNSW",
                  "            similar_nodes = hnsw_index.knn_query(",
                  "                node_embeddings[node_id],",
                  "                k=10  # Top-10 most similar nodes",
                  "            )[1][0]  # Get node indices"
                ],
                "line_count": 8
              },
              {
                "start_line": 350,
                "end_line": 360,
                "language": "python",
                "content": [
                  "            node_list = list(node_embeddings.keys())",
                  "            for similar_idx in similar_nodes[1:]:  # Skip self",
                  "                similar_node_id = node_list[similar_idx]",
                  "",
                  "                # Calculate similarity score",
                  "                similarity = cosine_similarity(",
                  "                    [node_embeddings[node_id]],",
                  "                    [node_embeddings[similar_node_id]]",
                  "                )[0][0]"
                ],
                "line_count": 9
              },
              {
                "start_line": 364,
                "end_line": 381,
                "language": "python",
                "content": [
                  "                # Add similarity edge if above threshold and type-compatible",
                  "                if similarity > 0.7 and self.are_types_compatible(",
                  "                    node_types[node_id],",
                  "                    node_types[similar_node_id]",
                  "                ):",
                  "                    heterogeneous_graph.add_edge(",
                  "                        node_id,",
                  "                        similar_node_id,",
                  "                        edge_type='similarity',",
                  "                        similarity_score=float(similarity),",
                  "                        hnsw_based=True",
                  "                    )",
                  "                    similarity_edges_added += 1",
                  "",
                  "        print(f\"Added {similarity_edges_added} HNSW similarity edges to heterogeneous graph\")",
                  "        return heterogeneous_graph"
                ],
                "line_count": 16
              },
              {
                "start_line": 387,
                "end_line": 402,
                "language": "python",
                "content": [
                  "    def are_types_compatible(self, type1, type2):",
                  "        \"\"\"Determine if two node types should have similarity connections\"\"\"",
                  "",
                  "        # Define type compatibility matrix",
                  "        compatibility_matrix = {",
                  "            'semantic_unit': ['semantic_unit', 'entity', 'summary'],",
                  "            'entity': ['entity', 'semantic_unit', 'attribute'],",
                  "            'relationship': ['relationship', 'entity'],",
                  "            'attribute': ['attribute', 'entity'],",
                  "            'document': ['document', 'summary'],",
                  "            'summary': ['summary', 'semantic_unit', 'document']",
                  "        }",
                  "",
                  "        return type2 in compatibility_matrix.get(type1, [])"
                ],
                "line_count": 14
              },
              {
                "start_line": 434,
                "end_line": 448,
                "language": "python",
                "content": [
                  "",
                  "# NodeRAG: Heterogeneous Graph Architecture for Advanced Knowledge Representation",
                  "",
                  "import spacy",
                  "from typing import List, Dict, Any, Tuple, Set, Union",
                  "import networkx as nx",
                  "from neo4j import GraphDatabase",
                  "import json",
                  "import re",
                  "from enum import Enum",
                  "from dataclasses import dataclass",
                  "import numpy as np",
                  "from sklearn.metrics.pairwise import cosine_similarity"
                ],
                "line_count": 13
              },
              {
                "start_line": 452,
                "end_line": 460,
                "language": "python",
                "content": [
                  "class NodeType(Enum):",
                  "    \"\"\"Specialized node types in heterogeneous NodeRAG architecture.\"\"\"",
                  "    ENTITY = \"entity\"",
                  "    CONCEPT = \"concept\"",
                  "    DOCUMENT = \"document\"",
                  "    RELATIONSHIP = \"relationship\"",
                  "    CLUSTER = \"cluster\""
                ],
                "line_count": 7
              },
              {
                "start_line": 464,
                "end_line": 475,
                "language": "python",
                "content": [
                  "@dataclass",
                  "class NodeRAGNode:",
                  "    \"\"\"Structured node representation for heterogeneous graph.\"\"\"",
                  "    node_id: str",
                  "    node_type: NodeType",
                  "    content: str",
                  "    metadata: Dict[str, Any]",
                  "    embeddings: Dict[str, np.ndarray]",
                  "    connections: List[str]",
                  "    confidence: float"
                ],
                "line_count": 10
              },
              {
                "start_line": 481,
                "end_line": 493,
                "language": "python",
                "content": [
                  "class NodeRAGExtractor:",
                  "    \"\"\"NodeRAG heterogeneous graph construction with three-stage processing.",
                  "",
                  "    This extractor implements the breakthrough NodeRAG architecture that addresses",
                  "    traditional GraphRAG limitations through specialized node types and advanced",
                  "    graph reasoning capabilities.",
                  "    \"\"\"",
                  "",
                  "    def __init__(self, llm_model, spacy_model: str = \"en_core_web_lg\"):",
                  "        self.llm_model = llm_model",
                  "        self.nlp = spacy.load(spacy_model)"
                ],
                "line_count": 11
              },
              {
                "start_line": 499,
                "end_line": 508,
                "language": "python",
                "content": [
                  "        # NodeRAG specialized processors for different node types",
                  "        self.node_processors = {",
                  "            NodeType.ENTITY: self._extract_entity_nodes,",
                  "            NodeType.CONCEPT: self._extract_concept_nodes,",
                  "            NodeType.DOCUMENT: self._extract_document_nodes,",
                  "            NodeType.RELATIONSHIP: self._extract_relationship_nodes,",
                  "            NodeType.CLUSTER: self._extract_cluster_nodes",
                  "        }"
                ],
                "line_count": 8
              },
              {
                "start_line": 514,
                "end_line": 521,
                "language": "python",
                "content": [
                  "        # Three-stage processing pipeline",
                  "        self.processing_stages = {",
                  "            'decomposition': self._decomposition_stage,",
                  "            'augmentation': self._augmentation_stage,",
                  "            'enrichment': self._enrichment_stage",
                  "        }"
                ],
                "line_count": 6
              },
              {
                "start_line": 525,
                "end_line": 535,
                "language": "python",
                "content": [
                  "        # Advanced graph components",
                  "        self.heterogeneous_graph = nx.MultiDiGraph()  # Supports multiple node types",
                  "        self.node_registry = {}  # Central registry of all nodes",
                  "        self.pagerank_processor = PersonalizedPageRankProcessor()",
                  "        self.hnsw_similarity = HNSWSimilarityProcessor()",
                  "",
                  "        # Reasoning integration components",
                  "        self.reasoning_pathways = {}  # Store logical reasoning pathways",
                  "        self.coherence_validator = CoherenceValidator()"
                ],
                "line_count": 9
              },
              {
                "start_line": 541,
                "end_line": 553,
                "language": "python",
                "content": [
                  "    def extract_noderag_graph(self, documents: List[str],",
                  "                               extraction_config: Dict = None) -> Dict[str, Any]:",
                  "        \"\"\"Extract NodeRAG heterogeneous graph using three-stage processing.",
                  "",
                  "        The NodeRAG extraction process follows the breakthrough three-stage approach:",
                  "",
                  "        **Stage 1 - Decomposition:**",
                  "        1. Multi-granularity analysis to extract different knowledge structures",
                  "        2. Specialized node creation for entities, concepts, documents, and relationships",
                  "        3. Hierarchical structuring at multiple abstraction levels",
                  "        \"\"\""
                ],
                "line_count": 11
              },
              {
                "start_line": 557,
                "end_line": 566,
                "language": "python",
                "content": [
                  "        config = extraction_config or {",
                  "            'node_types': ['entity', 'concept', 'document', 'relationship'],  # Heterogeneous node types",
                  "            'enable_pagerank': True,                     # Personalized PageRank traversal",
                  "            'enable_hnsw_similarity': True,              # High-performance similarity edges",
                  "            'reasoning_integration': True,               # Enable reasoning pathway construction",
                  "            'confidence_threshold': 0.75,                # Higher threshold for NodeRAG quality",
                  "            'max_pathway_depth': 5                       # Maximum reasoning pathway depth",
                  "        }"
                ],
                "line_count": 8
              },
              {
                "start_line": 570,
                "end_line": 577,
                "language": "python",
                "content": [
                  "        print(f\"Extracting NodeRAG heterogeneous graph from {len(documents)} documents...\")",
                  "        print(f\"Node types: {config['node_types']}, Reasoning integration: {config['reasoning_integration']}\")",
                  "",
                  "        # NodeRAG three-stage processing pipeline",
                  "        print(\"\\n=== STAGE 1: DECOMPOSITION ===\")",
                  "        decomposition_result = self.processing_stages['decomposition'](documents, config)"
                ],
                "line_count": 6
              },
              {
                "start_line": 581,
                "end_line": 584,
                "language": "python",
                "content": [
                  "        print(\"=== STAGE 2: AUGMENTATION ===\")",
                  "        augmentation_result = self.processing_stages['augmentation'](decomposition_result, config)"
                ],
                "line_count": 2
              },
              {
                "start_line": 588,
                "end_line": 592,
                "language": "python",
                "content": [
                  "        print(\"=== STAGE 3: ENRICHMENT ===\")",
                  "        enrichment_result = self.processing_stages['enrichment'](augmentation_result, config)",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 596,
                "end_line": 600,
                "language": "python",
                "content": [
                  "        # Build heterogeneous graph structure",
                  "        print(\"Constructing heterogeneous graph with specialized node types...\")",
                  "        self._build_heterogeneous_graph(enrichment_result)"
                ],
                "line_count": 3
              },
              {
                "start_line": 604,
                "end_line": 614,
                "language": "python",
                "content": [
                  "        # Apply Personalized PageRank for semantic traversal",
                  "        if config.get('enable_pagerank', True):",
                  "            print(\"Computing Personalized PageRank for semantic traversal...\")",
                  "            pagerank_scores = self.pagerank_processor.compute_pagerank(",
                  "                self.heterogeneous_graph, self.node_registry",
                  "            )",
                  "        else:",
                  "            pagerank_scores = {}",
                  ""
                ],
                "line_count": 9
              },
              {
                "start_line": 618,
                "end_line": 627,
                "language": "python",
                "content": [
                  "        # Build HNSW similarity edges for high-performance retrieval",
                  "        if config.get('enable_hnsw_similarity', True):",
                  "            print(\"Constructing HNSW similarity edges...\")",
                  "            similarity_edges = self.hnsw_similarity.build_similarity_graph(",
                  "                self.node_registry, self.heterogeneous_graph",
                  "            )",
                  "        else:",
                  "            similarity_edges = {}"
                ],
                "line_count": 8
              },
              {
                "start_line": 631,
                "end_line": 640,
                "language": "python",
                "content": [
                  "        # Construct reasoning pathways if enabled",
                  "        reasoning_pathways = {}",
                  "        if config.get('reasoning_integration', True):",
                  "            print(\"Building reasoning pathways for logical coherence...\")",
                  "            reasoning_pathways = self._construct_reasoning_pathways(",
                  "                enrichment_result, config",
                  "            )",
                  ""
                ],
                "line_count": 8
              },
              {
                "start_line": 644,
                "end_line": 655,
                "language": "python",
                "content": [
                  "        # Calculate comprehensive NodeRAG statistics",
                  "        noderag_stats = self._calculate_noderag_statistics()",
                  "",
                  "        return {",
                  "            'heterogeneous_nodes': self.node_registry,",
                  "            'reasoning_pathways': reasoning_pathways,",
                  "            'pagerank_scores': pagerank_scores,",
                  "            'similarity_edges': similarity_edges,",
                  "            'heterogeneous_graph': self.heterogeneous_graph,",
                  "            'noderag_stats': noderag_stats,"
                ],
                "line_count": 10
              },
              {
                "start_line": 659,
                "end_line": 674,
                "language": "python",
                "content": [
                  "            'extraction_metadata': {",
                  "                'document_count': len(documents),",
                  "                'total_nodes': len(self.node_registry),",
                  "                'node_type_distribution': self._get_node_type_distribution(),",
                  "                'reasoning_pathways_count': len(reasoning_pathways),",
                  "                'extraction_config': config,",
                  "                'processing_stages_completed': ['decomposition', 'augmentation', 'enrichment'],",
                  "                'quality_metrics': {",
                  "                    'avg_node_confidence': self._calculate_avg_node_confidence(),",
                  "                    'pathway_coherence_score': self._calculate_pathway_coherence(),",
                  "                    'graph_connectivity_score': self._calculate_connectivity_score()",
                  "                }",
                  "            }",
                  "        }"
                ],
                "line_count": 14
              },
              {
                "start_line": 678,
                "end_line": 774,
                "language": "python",
                "content": [
                  "    def _decomposition_stage(self, documents: List[str], config: Dict) -> Dict[str, Any]:",
                  "        \"\"\"Stage 1: Multi-granularity decomposition with specialized node creation.\"\"\"",
                  "",
                  "        print(\"Performing multi-granularity analysis...\")",
                  "        decomposition_results = {",
                  "            'entity_nodes': [],",
                  "            'concept_nodes': [],",
                  "            'document_nodes': [],",
                  "            'relationship_nodes': [],",
                  "            'hierarchical_structures': {}",
                  "        }",
                  "",
                  "        for doc_idx, document in enumerate(documents):",
                  "            print(f\"Decomposing document {doc_idx + 1}/{len(documents)}\")",
                  "",
                  "            # Extract entity nodes with rich metadata",
                  "            if 'entity' in config['node_types']:",
                  "                entity_nodes = self._extract_entity_nodes(document, doc_idx)",
                  "                decomposition_results['entity_nodes'].extend(entity_nodes)",
                  "",
                  "            # Extract concept nodes for abstract concepts and topics",
                  "            if 'concept' in config['node_types']:",
                  "                concept_nodes = self._extract_concept_nodes(document, doc_idx)",
                  "                decomposition_results['concept_nodes'].extend(concept_nodes)",
                  "",
                  "            # Extract document nodes for text segments",
                  "            if 'document' in config['node_types']:",
                  "                document_nodes = self._extract_document_nodes(document, doc_idx)",
                  "                decomposition_results['document_nodes'].extend(document_nodes)",
                  "",
                  "            # Extract explicit relationship nodes",
                  "            if 'relationship' in config['node_types']:",
                  "                relationship_nodes = self._extract_relationship_nodes(document, doc_idx)",
                  "                decomposition_results['relationship_nodes'].extend(relationship_nodes)",
                  "",
                  "        # Build hierarchical structures at multiple abstraction levels",
                  "        decomposition_results['hierarchical_structures'] = self._build_hierarchical_structures(",
                  "            decomposition_results",
                  "        )",
                  "",
                  "        print(f\"Decomposition complete: {sum(len(nodes) for nodes in decomposition_results.values() if isinstance(nodes, list))} nodes created\")",
                  "        return decomposition_results",
                  "",
                  "    def _augmentation_stage(self, decomposition_result: Dict, config: Dict) -> Dict[str, Any]:",
                  "        \"\"\"Stage 2: Cross-reference integration and HNSW similarity edge construction.\"\"\"",
                  "",
                  "        print(\"Performing cross-reference integration...\")",
                  "",
                  "        # Cross-link related nodes across different types",
                  "        cross_references = self._build_cross_references(decomposition_result)",
                  "",
                  "        # Build HNSW similarity edges for high-performance retrieval",
                  "        if config.get('enable_hnsw_similarity', True):",
                  "            print(\"Constructing HNSW similarity edges...\")",
                  "            similarity_edges = self._build_hnsw_similarity_edges(decomposition_result)",
                  "        else:",
                  "            similarity_edges = {}",
                  "",
                  "        # Semantic enrichment with contextual metadata",
                  "        enriched_nodes = self._apply_semantic_enrichment(decomposition_result)",
                  "",
                  "        return {",
                  "            'enriched_nodes': enriched_nodes,",
                  "            'cross_references': cross_references,",
                  "            'similarity_edges': similarity_edges,",
                  "            'augmentation_metadata': {",
                  "                'cross_references_count': len(cross_references),",
                  "                'similarity_edges_count': len(similarity_edges),",
                  "                'enrichment_applied': True",
                  "            }",
                  "        }",
                  "",
                  "    def _enrichment_stage(self, augmentation_result: Dict, config: Dict) -> Dict[str, Any]:",
                  "        \"\"\"Stage 3: Personalized PageRank and reasoning pathway construction.\"\"\"",
                  "",
                  "        print(\"Constructing reasoning pathways...\")",
                  "",
                  "        # Build reasoning pathways for logically coherent contexts",
                  "        reasoning_pathways = {}",
                  "        if config.get('reasoning_integration', True):",
                  "            reasoning_pathways = self._construct_reasoning_pathways_stage3(",
                  "                augmentation_result, config",
                  "            )",
                  "",
                  "        # Apply graph-centric optimization",
                  "        optimized_structure = self._apply_graph_optimization(",
                  "            augmentation_result, reasoning_pathways",
                  "        )",
                  "",
                  "        return {",
                  "            'final_nodes': optimized_structure['nodes'],",
                  "            'reasoning_pathways': reasoning_pathways,",
                  "            'optimization_metadata': optimized_structure['metadata'],",
                  "            'enrichment_complete': True",
                  "        }"
                ],
                "line_count": 95
              },
              {
                "start_line": 778,
                "end_line": 887,
                "language": "python",
                "content": [
                  "class PersonalizedPageRankProcessor:",
                  "    \"\"\"Personalized PageRank for semantic traversal in NodeRAG.\"\"\"",
                  "",
                  "    def __init__(self, damping_factor: float = 0.85):",
                  "        self.damping_factor = damping_factor",
                  "        self.pagerank_cache = {}",
                  "",
                  "    def compute_pagerank(self, graph: nx.MultiDiGraph, node_registry: Dict) -> Dict[str, float]:",
                  "        \"\"\"Compute personalized PageRank scores for semantic traversal.\"\"\"",
                  "",
                  "        if not graph.nodes():",
                  "            return {}",
                  "",
                  "        # Create personalization vector based on node types and importance",
                  "        personalization = self._create_personalization_vector(graph, node_registry)",
                  "",
                  "        # Compute Personalized PageRank",
                  "        try:",
                  "            pagerank_scores = nx.pagerank(",
                  "                graph,",
                  "                alpha=self.damping_factor,",
                  "                personalization=personalization,",
                  "                max_iter=100,",
                  "                tol=1e-6",
                  "            )",
                  "",
                  "            # Normalize scores by node type for better semantic traversal",
                  "            normalized_scores = self._normalize_scores_by_type(",
                  "                pagerank_scores, node_registry",
                  "            )",
                  "",
                  "            return normalized_scores",
                  "",
                  "        except Exception as e:",
                  "            print(f\"PageRank computation error: {e}\")",
                  "            return {}",
                  "",
                  "    def _create_personalization_vector(self, graph: nx.MultiDiGraph,",
                  "                                     node_registry: Dict) -> Dict[str, float]:",
                  "        \"\"\"Create personalization vector emphasizing important node types.\"\"\"",
                  "",
                  "        personalization = {}",
                  "        total_nodes = len(graph.nodes())",
                  "",
                  "        # Weight different node types for semantic importance",
                  "        type_weights = {",
                  "            NodeType.ENTITY: 0.3,      # High weight for entities",
                  "            NodeType.CONCEPT: 0.25,    # High weight for concepts",
                  "            NodeType.RELATIONSHIP: 0.2, # Medium weight for relationships",
                  "            NodeType.DOCUMENT: 0.15,   # Medium weight for documents",
                  "            NodeType.CLUSTER: 0.1      # Lower weight for clusters",
                  "        }",
                  "",
                  "        for node_id in graph.nodes():",
                  "            if node_id in node_registry:",
                  "                node_type = node_registry[node_id].node_type",
                  "                base_weight = type_weights.get(node_type, 0.1)",
                  "",
                  "                # Boost weight based on node confidence and connections",
                  "                confidence_boost = node_registry[node_id].confidence * 0.2",
                  "                connection_boost = min(len(node_registry[node_id].connections) * 0.1, 0.3)",
                  "",
                  "                final_weight = base_weight + confidence_boost + connection_boost",
                  "                personalization[node_id] = final_weight",
                  "            else:",
                  "                personalization[node_id] = 0.1  # Default weight",
                  "",
                  "        # Normalize to sum to 1.0",
                  "        total_weight = sum(personalization.values())",
                  "        if total_weight > 0:",
                  "            for node_id in personalization:",
                  "                personalization[node_id] /= total_weight",
                  "",
                  "        return personalization",
                  "",
                  "    def get_semantic_pathway(self, graph: nx.MultiDiGraph, start_node: str,",
                  "                           target_concepts: List[str], max_depth: int = 5) -> List[str]:",
                  "        \"\"\"Find semantic pathway using PageRank-guided traversal.\"\"\"",
                  "",
                  "        if start_node not in graph:",
                  "            return []",
                  "",
                  "        # Use PageRank scores to guide pathway exploration",
                  "        pagerank_scores = self.pagerank_cache.get(id(graph))",
                  "        if not pagerank_scores:",
                  "            return []",
                  "",
                  "        visited = set()",
                  "        pathway = [start_node]",
                  "        current_node = start_node",
                  "        depth = 0",
                  "",
                  "        while depth < max_depth and current_node:",
                  "            visited.add(current_node)",
                  "",
                  "            # Find best next node based on PageRank scores and target concepts",
                  "            next_node = self._find_best_next_node(",
                  "                graph, current_node, target_concepts, pagerank_scores, visited",
                  "            )",
                  "",
                  "            if next_node and next_node not in visited:",
                  "                pathway.append(next_node)",
                  "                current_node = next_node",
                  "                depth += 1",
                  "            else:",
                  "                break",
                  "",
                  "        return pathway"
                ],
                "line_count": 108
              },
              {
                "start_line": 891,
                "end_line": 960,
                "language": "python",
                "content": [
                  "    def _merge_similar_entities(self, entities: Dict[str, Any],",
                  "                               similarity_threshold: float = 0.85) -> Dict[str, Any]:",
                  "        \"\"\"Merge semantically similar entities.\"\"\"",
                  "",
                  "        from sentence_transformers import SentenceTransformer",
                  "        from sklearn.metrics.pairwise import cosine_similarity",
                  "        import numpy as np",
                  "",
                  "        # Load embedding model for similarity computation",
                  "        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')",
                  "",
                  "        entity_names = list(entities.keys())",
                  "        if len(entity_names) < 2:",
                  "            return entities",
                  "",
                  "        # Generate embeddings for entity canonical forms",
                  "        entity_embeddings = embedding_model.encode(entity_names)",
                  "",
                  "        # Calculate similarity matrix",
                  "        similarity_matrix = cosine_similarity(entity_embeddings)",
                  "",
                  "        # Find similar entity pairs",
                  "        merged_entities = {}",
                  "        entity_clusters = []",
                  "        processed_entities = set()",
                  "",
                  "        for i, entity1 in enumerate(entity_names):",
                  "            if entity1 in processed_entities:",
                  "                continue",
                  "",
                  "            # Find similar entities",
                  "            cluster = [entity1]",
                  "            for j, entity2 in enumerate(entity_names):",
                  "                if i != j and entity2 not in processed_entities:",
                  "                    if similarity_matrix[i][j] > similarity_threshold:",
                  "                        cluster.append(entity2)",
                  "",
                  "            # Create merged entity",
                  "            if len(cluster) > 1:",
                  "                # Choose canonical form (highest confidence entity)",
                  "                canonical_entity = max(",
                  "                    cluster,",
                  "                    key=lambda x: entities[x].get('confidence', 0.5)",
                  "                )",
                  "",
                  "                # Merge information",
                  "                merged_entity = entities[canonical_entity].copy()",
                  "                merged_entity['text_variants'] = []",
                  "                merged_entity['merged_from'] = cluster",
                  "",
                  "                for entity_name in cluster:",
                  "                    entity_data = entities[entity_name]",
                  "                    merged_entity['text_variants'].extend(",
                  "                        entity_data.get('text_variants', [entity_name])",
                  "                    )",
                  "                    processed_entities.add(entity_name)",
                  "",
                  "                # Remove duplicates in text variants",
                  "                merged_entity['text_variants'] = list(set(merged_entity['text_variants']))",
                  "                merged_entities[canonical_entity] = merged_entity",
                  "",
                  "            else:",
                  "                # Single entity, no merging needed",
                  "                merged_entities[entity1] = entities[entity1]",
                  "                processed_entities.add(entity1)",
                  "",
                  "        print(f\"Entity merging: {len(entities)} -> {len(merged_entities)} entities\")",
                  "        return merged_entities"
                ],
                "line_count": 68
              },
              {
                "start_line": 989,
                "end_line": 1002,
                "language": "python",
                "content": [
                  "# Neo4j integration for production GraphRAG",
                  "",
                  "class Neo4jGraphManager:",
                  "    \"\"\"Production Neo4j integration for GraphRAG systems.",
                  "",
                  "    This manager implements production-grade patterns for graph storage:",
                  "",
                  "    - Batch Operations: Minimizes transaction overhead for large-scale ingestion",
                  "    - Strategic Indexing: Optimizes common query patterns (entity lookup, type filtering)",
                  "    - Connection Pooling: Handles concurrent access efficiently",
                  "    - Error Recovery: Robust handling of network and database issues",
                  "    \"\"\""
                ],
                "line_count": 12
              },
              {
                "start_line": 1006,
                "end_line": 1024,
                "language": "python",
                "content": [
                  "    # Performance characteristics:",
                  "    # - Batch entity storage: ~10,000 entities/second",
                  "    # - Relationship insertion: ~5,000 relationships/second",
                  "    # - Query response: <100ms for 3-hop traversals on 100K+ entity graphs",
                  "",
                  "    def __init__(self, uri: str, username: str, password: str):",
                  "        # Neo4j driver with production settings",
                  "        self.driver = GraphDatabase.driver(",
                  "            uri,",
                  "            auth=(username, password),",
                  "            # Production optimizations",
                  "            max_connection_pool_size=50,  # Handle concurrent access",
                  "            connection_acquisition_timeout=30,  # Timeout for busy periods",
                  "        )",
                  "",
                  "        # Create performance-critical indices",
                  "        self._create_indices()"
                ],
                "line_count": 17
              },
              {
                "start_line": 1028,
                "end_line": 1039,
                "language": "python",
                "content": [
                  "    def _create_indices(self):",
                  "        \"\"\"Create necessary indices for GraphRAG performance.",
                  "",
                  "        These indices are critical for production performance:",
                  "        - entity_canonical: Enables O(1) entity lookup by canonical name",
                  "        - entity_type: Supports filtering by entity type in traversals",
                  "        - document_id: Fast document-based queries for provenance",
                  "",
                  "        Index creation is idempotent - safe to run multiple times.",
                  "        \"\"\""
                ],
                "line_count": 10
              },
              {
                "start_line": 1043,
                "end_line": 1051,
                "language": "python",
                "content": [
                  "        with self.driver.session() as session:",
                  "            print(\"Creating performance indices...\")",
                  "",
                  "            # Entity indices - critical for fast lookups",
                  "            session.run(\"CREATE INDEX entity_canonical IF NOT EXISTS FOR (e:Entity) ON (e.canonical)\")",
                  "            session.run(\"CREATE INDEX entity_type IF NOT EXISTS FOR (e:Entity) ON (e.type)\")",
                  "            session.run(\"CREATE INDEX entity_confidence IF NOT EXISTS FOR (e:Entity) ON (e.confidence)\")"
                ],
                "line_count": 7
              },
              {
                "start_line": 1055,
                "end_line": 1064,
                "language": "python",
                "content": [
                  "            # Relationship indices - optimize traversal queries",
                  "            session.run(\"CREATE INDEX relationship_type IF NOT EXISTS FOR ()-[r:RELATED]-() ON (r.type)\")",
                  "            session.run(\"CREATE INDEX relationship_confidence IF NOT EXISTS FOR ()-[r:RELATED]-() ON (r.confidence)\")",
                  "",
                  "            # Document indices - support provenance and source tracking",
                  "            session.run(\"CREATE INDEX document_id IF NOT EXISTS FOR (d:Document) ON (d.doc_id)\")",
                  "",
                  "            print(\"Neo4j indices created successfully - GraphRAG queries optimized\")"
                ],
                "line_count": 8
              },
              {
                "start_line": 1068,
                "end_line": 1084,
                "language": "python",
                "content": [
                  "    def store_knowledge_graph(self, entities: Dict[str, Any],",
                  "                             relationships: List[Dict],",
                  "                             document_metadata: Dict = None) -> Dict[str, Any]:",
                  "        \"\"\"Store knowledge graph in Neo4j with optimized batch operations.",
                  "",
                  "        This method implements production-grade storage patterns:",
                  "",
                  "        1. Batch Processing: Groups operations to minimize transaction overhead",
                  "        2. Transactional Safety: Ensures data consistency during storage",
                  "        3. Performance Monitoring: Tracks storage rates for optimization",
                  "        4. Error Recovery: Handles failures gracefully without data corruption",
                  "",
                  "        Storage performance scales linearly with batch size up to optimal thresholds.",
                  "        Large knowledge graphs (100K+ entities) typically store in 10-30 seconds.",
                  "        \"\"\""
                ],
                "line_count": 15
              },
              {
                "start_line": 1088,
                "end_line": 1098,
                "language": "python",
                "content": [
                  "        import time",
                  "        start_time = time.time()",
                  "",
                  "        with self.driver.session() as session:",
                  "            print(f\"Storing knowledge graph: {len(entities)} entities, {len(relationships)} relationships\")",
                  "",
                  "            # Store entities in optimized batches",
                  "            print(\"Storing entities...\")",
                  "            entity_count = self._store_entities_batch(session, entities)"
                ],
                "line_count": 9
              },
              {
                "start_line": 1102,
                "end_line": 1113,
                "language": "python",
                "content": [
                  "            # Store relationships in optimized batches",
                  "            # Must happen after entities to maintain referential integrity",
                  "            print(\"Storing relationships...\")",
                  "            relationship_count = self._store_relationships_batch(session, relationships)",
                  "",
                  "            # Store document metadata for provenance tracking",
                  "            doc_count = 0",
                  "            if document_metadata:",
                  "                print(\"Storing document metadata...\")",
                  "                doc_count = self._store_document_metadata(session, document_metadata)"
                ],
                "line_count": 10
              },
              {
                "start_line": 1117,
                "end_line": 1135,
                "language": "python",
                "content": [
                  "        storage_duration = time.time() - start_time",
                  "        entities_per_second = len(entities) / storage_duration if storage_duration > 0 else 0",
                  "",
                  "        storage_result = {",
                  "            'entities_stored': entity_count,",
                  "            'relationships_stored': relationship_count,",
                  "            'documents_stored': doc_count,",
                  "            'storage_timestamp': time.time(),",
                  "            'performance_metrics': {",
                  "                'storage_duration_seconds': storage_duration,",
                  "                'entities_per_second': entities_per_second,",
                  "                'relationships_per_second': len(relationships) / storage_duration if storage_duration > 0 else 0",
                  "            }",
                  "        }",
                  "",
                  "        print(f\"Storage complete in {storage_duration:.2f}s - {entities_per_second:.0f} entities/sec\")",
                  "        return storage_result"
                ],
                "line_count": 17
              },
              {
                "start_line": 1139,
                "end_line": 1143,
                "language": "",
                "content": [
                  "",
                  "#### Step 4: Batch Entity Storage",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1155,
                "end_line": 1159,
                "language": "",
                "content": [
                  "",
                  "**High-performance batch storage** is essential for enterprise knowledge graphs containing millions of entities. Single-entity inserts create massive overhead - each requires a separate database transaction. Batch processing reduces this overhead by 100x, transforming hours-long ingestion processes into minutes.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1171,
                "end_line": 1175,
                "language": "",
                "content": [
                  "",
                  "**Entity normalization for graph storage** transforms the internal entity representation into a Neo4j-optimized format. The canonical name serves as the primary key, while text variants enable fuzzy matching. The context limit (500 characters) prevents memory issues while preserving essential contextual information for entity disambiguation.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1183,
                "end_line": 1187,
                "language": "",
                "content": [
                  "",
                  "**Intelligent batching logic** divides large entity sets into manageable chunks that balance memory usage with transaction efficiency. The batch size of 1000 entities typically represents 50-100KB of data - optimal for Neo4j's transaction handling while preventing memory exhaustion on large datasets.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1200,
                "end_line": 1204,
                "language": "",
                "content": [
                  "",
                  "**MERGE-based upsert pattern** handles both new entities and updates elegantly. The MERGE operation creates entities that don't exist while updating existing ones, making the system idempotent - crucial for robust data ingestion pipelines that may encounter duplicates or need to reprocess data.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1213,
                "end_line": 1217,
                "language": "",
                "content": [
                  "",
                  "**Production-grade monitoring and feedback** provides essential visibility into long-running ingestion processes. Progress reporting every 10 batches prevents log spam while ensuring operators can monitor system health and estimate completion times for large knowledge graph construction jobs.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1227,
                "end_line": 1231,
                "language": "",
                "content": [
                  "",
                  "**Relationship storage complexity** far exceeds entity storage because relationships require existing entities as anchors. Graph databases must verify entity existence before creating connections, making relationship insertion computationally expensive. This constraint makes batch optimization even more critical for relationship processing.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1248,
                "end_line": 1252,
                "language": "python",
                "content": [
                  "",
                  "**Relationship validation and normalization** ensures data quality before expensive graph operations. The validation step prevents database errors that could occur from malformed relationships, while normalization adds essential metadata like confidence scores and evidence trails that enable relationship quality assessment.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1262,
                "end_line": 1266,
                "language": "",
                "content": [
                  "",
                  "**Optimized batching for relationship complexity** uses smaller batch sizes than entity storage because relationship operations are computationally heavier. Each relationship requires two MATCH operations (find subject and object entities) plus a MERGE operation, making the processing 3x more expensive than simple entity storage.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1279,
                "end_line": 1283,
                "language": "",
                "content": [
                  "",
                  "**Sophisticated Cypher query pattern** implements the UNWIND-MATCH-MERGE pattern for efficient batch relationship creation. UNWIND processes the batch as individual items, MATCH finds existing entities, and MERGE creates relationships while preventing duplicates. This pattern is essential for maintaining referential integrity in knowledge graphs.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1298,
                "end_line": 1312,
                "language": "",
                "content": [
                  "",
                  "**Robust error handling with partial failure recovery** ensures that one failed batch doesn't stop the entire ingestion process. This resilience is critical for enterprise deployments where large knowledge graphs may contain some malformed data. The system continues processing valid batches while logging failures for later investigation.",
                  "",
                  "---",
                  "",
                  "## Part 3: Code GraphRAG Implementation - Understanding Software Knowledge",
                  "",
                  "### AST-Based Code Analysis",
                  "",
                  "Traditional GraphRAG works well for general documents, but code repositories require specialized understanding. Code has unique relationship patterns: functions call other functions, classes inherit from base classes, modules import dependencies, and variables have scope relationships. A code-specific GraphRAG system needs to understand these programming language semantics to enable queries like \"show me all functions that depend on this deprecated API\" or \"what would break if I modify this class interface?\"",
                  "",
                  "This specialized approach transforms code repositories from file collections into navigable knowledge graphs that understand software architecture.",
                  ""
                ],
                "line_count": 13
              },
              {
                "start_line": 1404,
                "end_line": 1408,
                "language": "",
                "content": [
                  "",
                  "#### Step 5: Python AST Analysis",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1503,
                "end_line": 1507,
                "language": "",
                "content": [
                  "",
                  "#### Step 6: Call Graph Construction",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1579,
                "end_line": 1597,
                "language": "",
                "content": [
                  "",
                  "---",
                  "",
                  "## Part 4: Graph Traversal and Multi-Hop Reasoning - Connecting the Dots",
                  "",
                  "### Intelligent Graph Traversal",
                  "",
                  "Building knowledge graphs is only half the challenge \u2013 the real power emerges in traversal algorithms that can follow logical reasoning pathways to connect disparate information. While vector RAG performs one-hop similarity searches, graph traversal enables multi-hop reasoning that answers complex questions by following relationship chains through the knowledge structure.",
                  "",
                  "### The Power of Multi-Hop Reasoning",
                  "",
                  "Consider the query \"What security vulnerabilities affect systems used by Apple's automotive partners?\" This requires four logical steps: (1) finding Apple's partners, (2) identifying which work in automotive, (3) determining their technology systems, (4) checking for security vulnerabilities. Graph traversal makes this reasoning path explicit and traceable, enabling answers that no single document could provide.",
                  "",
                  "### GraphRAG vs Vector RAG: The Multi-Hop Advantage",
                  "",
                  "This is where GraphRAG truly shines compared to vector search. Traditional RAG would search for separate document collections about Apple, automotive partners, and technologies, but struggles to connect these concepts logically. GraphRAG follows explicit relationship chains:",
                  ""
                ],
                "line_count": 17
              },
              {
                "start_line": 1599,
                "end_line": 1635,
                "language": "",
                "content": [
                  "",
                  "This multi-hop traversal discovers information that no single document contains, synthesizing knowledge from the relationship structure itself. The graph becomes a reasoning engine, not just a retrieval system.",
                  "",
                  "### Graph Traversal Strategies for Different Query Types",
                  "",
                  "Different queries require different traversal approaches, each optimized for specific reasoning patterns:",
                  "",
                  "**Direct Relationship Queries** use **Breadth-First Traversal** to find immediate connections (\"Who works with Apple?\"). This strategy explores all first-hop neighbors before moving to second-hop, ensuring comprehensive coverage of direct relationships.",
                  "",
                  "**Connection Discovery Queries** use **Depth-First Traversal** to explore deep relationship chains (\"What's the connection between Apple and Tesla?\"). This strategy follows paths to their conclusion, ideal for finding indirect connections through multiple intermediaries.",
                  "",
                  "**Semantic Reasoning Queries** use **Semantic-Guided Traversal** that follows paths most relevant to query semantics, filtering relationships based on their relevance to the question context. This enables focused exploration of semantically coherent pathways.",
                  "",
                  "### Advanced Traversal Strategies",
                  "",
                  "**Relevance-Ranked Traversal** prioritizes high-confidence, important relationships using PageRank scores and relationship confidence levels. This strategy ensures that the most reliable knowledge pathways are explored first, improving answer quality.",
                  "",
                  "**Community-Focused Traversal** explores dense clusters of related entities, useful for questions about industry sectors or technology ecosystems. This approach leverages graph community structure to find comprehensive related information.",
                  "",
                  "Our traversal engine adaptively selects strategies based on query characteristics, ensuring optimal exploration for each use case.",
                  "",
                  "### Performance vs Completeness Trade-offs",
                  "",
                  "Graph traversal faces the \"explosion problem\" - the number of possible paths grows exponentially with hop count. Our engine implements sophisticated pruning strategies to manage this complexity:",
                  "",
                  "**Semantic Filtering** ensures only paths semantically related to the query are explored, dramatically reducing the search space while maintaining relevance. **Confidence Thresholding** ignores low-quality relationships, focusing computational resources on reliable knowledge connections.",
                  "",
                  "**Path Length Limits** prevent infinite traversal while enabling meaningful multi-hop reasoning. **Relevance Scoring** ranks paths by likely usefulness, ensuring the most promising reasoning pathways are explored first.",
                  "",
                  "This multi-layered approach ensures comprehensive coverage while maintaining reasonable response times, making GraphRAG practical for real-time applications.",
                  "",
                  "### Advanced Graph Traversal Engine",
                  "",
                  "The heart of GraphRAG's multi-hop reasoning capability lies in intelligent traversal algorithms that can navigate complex knowledge graphs to answer sophisticated queries. Our traversal engine combines multiple strategies and implements sophisticated pruning to balance comprehensiveness with performance.",
                  ""
                ],
                "line_count": 35
              },
              {
                "start_line": 1645,
                "end_line": 1649,
                "language": "",
                "content": [
                  "",
                  "**Intelligent graph exploration** addresses the exponential path explosion problem that makes naive graph traversal computationally intractable. Without intelligent pruning, a 3-hop traversal in a dense graph can generate millions of paths. This engine reduces that to hundreds of high-quality paths through semantic guidance and multi-criteria filtering.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1664,
                "end_line": 1668,
                "language": "",
                "content": [
                  "",
                  "**High-performance graph traversal architecture** demonstrates enterprise-grade design where sub-200ms response times are achieved on large knowledge graphs through intelligent algorithmic choices. The 80-95% path space reduction is crucial - it makes the difference between computationally impossible and real-time responsive GraphRAG systems.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1677,
                "end_line": 1681,
                "language": "python",
                "content": [
                  "",
                  "**Multiple traversal strategies** address the reality that different query types benefit from different exploration approaches. Breadth-first works well for finding nearby related concepts, while depth-first excels at following causal chains. Semantic guidance is optimal when query relevance is paramount, while community-focused traversal discovers tightly interconnected concept clusters.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1689,
                "end_line": 1693,
                "language": "",
                "content": [
                  "",
                  "**Multi-dimensional path quality assessment** goes beyond simple hop counting to evaluate path value across multiple criteria. Semantic coherence ensures paths remain relevant to the query, entity importance emphasizes well-connected nodes, and relationship confidence weights paths by the quality of their underlying relationships.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1706,
                "end_line": 1710,
                "language": "",
                "content": [
                  "",
                  "**Five-stage multi-hop reasoning pipeline** represents the most sophisticated approach to graph-based information retrieval. Each stage serves a critical purpose: discovery ensures comprehensive exploration, filtering prevents information overload, ranking prioritizes quality, extraction converts graph structure to natural language, and synthesis creates coherent answers.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1719,
                "end_line": 1723,
                "language": "",
                "content": [
                  "",
                  "**Balanced configuration for enterprise performance** reflects hard-learned lessons about graph traversal optimization. Three hops capture 95% of valuable relationships while preventing exponential explosion. Fifty paths provide comprehensive coverage without overwhelming downstream processing. The 0.7 semantic threshold ensures quality while maintaining reasonable recall.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1738,
                "end_line": 1742,
                "language": "python",
                "content": [
                  "",
                  "**Systematic path discovery from multiple seed entities** ensures comprehensive exploration of the knowledge space. By exploring from each seed entity independently, the system captures different perspectives and relationship angles, preventing single-entity bias from limiting the breadth of discovered insights.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1751,
                "end_line": 1755,
                "language": "",
                "content": [
                  "",
                  "**Intelligent path ranking and quality filtering** transforms the raw path discovery results into prioritized, high-quality insights. The ranking system evaluates each path across multiple dimensions - semantic relevance, relationship confidence, entity importance, and path coherence - to ensure only the most valuable paths contribute to the final answer.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1764,
                "end_line": 1768,
                "language": "",
                "content": [
                  "",
                  "**Path-to-language transformation and synthesis** converts structured graph paths into natural language narratives that LLMs can effectively process. This stage bridges the gap between graph structure and textual reasoning, enabling the final synthesis step to create coherent, comprehensive responses that leverage all discovered relationship insights.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1784,
                "end_line": 1790,
                "language": "",
                "content": [
                  "",
                  "**Comprehensive result structure with rich metadata** provides complete transparency into the multi-hop reasoning process. This detailed information enables downstream systems to understand how insights were discovered, what exploration strategies were used, and what the quality characteristics of the paths were. The metadata is crucial for debugging, optimization, and building user trust in graph-based reasoning.",
                  "",
                  "#### Step 7: Semantic-Guided Traversal",
                  ""
                ],
                "line_count": 5
              },
              {
                "start_line": 1889,
                "end_line": 1893,
                "language": "",
                "content": [
                  "",
                  "#### Step 8: Path Context Synthesis",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1980,
                "end_line": 2022,
                "language": "",
                "content": [
                  "",
                  "---",
                  "",
                  "## Part 5: Hybrid Graph-Vector Search",
                  "",
                  "### Integrated Graph and Vector Retrieval",
                  "",
                  "### Why Hybrid Search Outperforms Pure Approaches",
                  "",
                  "Neither graph-only nor vector-only search is optimal for all scenarios. Each approach has distinct strengths and limitations that make hybrid systems significantly more powerful than either approach alone.",
                  "",
                  "### Vector Search: Semantic Similarity Powerhouse",
                  "",
                  "**Vector search excels at semantic understanding:** It provides excellent semantic similarity matching, naturally handles synonyms and paraphrasing, enables fast retrieval for well-defined concepts, and works effectively with isolated facts. For queries like \"What is machine learning?\", vector search quickly finds relevant content based on semantic similarity.",
                  "",
                  "**However, vector search has critical limitations:** It cannot traverse relationships between concepts, misses connections requiring multi-step reasoning, struggles with queries requiring synthesis across sources, and has limited understanding of entity relationships. Vector search treats knowledge as isolated fragments rather than connected information.",
                  "",
                  "### Graph Search: Relationship and Reasoning Excellence",
                  "",
                  "**Graph search enables sophisticated reasoning:** It discovers implicit connections through relationships, enables multi-hop reasoning and inference, understands structural importance and centrality, and reveals information not contained in any single document. For queries requiring connection discovery, graph search is unmatched.",
                  "",
                  "**But graph search also has limitations:** It depends heavily on explicit relationship extraction quality, may miss semantically similar but unconnected information, can be computationally expensive for large traversals, and requires comprehensive entity recognition. Graph search can miss obvious semantic connections if relationships weren't explicitly extracted.",
                  "",
                  "### The Hybrid Advantage: Best of Both Worlds",
                  "",
                  "Hybrid search combines both approaches strategically to overcome individual limitations:",
                  "",
                  "**Stage 1: Vector Search** identifies semantically relevant content using embedding similarity, capturing documents and entities that match the query's semantic intent. This provides comprehensive coverage of directly relevant information.",
                  "",
                  "**Stage 2: Graph Traversal** discovers related information through relationships, following logical pathways to find connected knowledge that vector search might miss. This adds multi-hop reasoning capabilities.",
                  "",
                  "**Stage 3: Intelligent Fusion** combines results based on query characteristics, balancing vector similarity scores with graph centrality and relationship confidence. The fusion process ensures optimal result ranking.",
                  "",
                  "**Stage 4: Adaptive Weighting** emphasizes the most effective approach for each query type - vector search for semantic queries, graph traversal for relationship queries, balanced weighting for complex analytical queries.",
                  "",
                  "This results in 30-40% improvement in answer quality over pure approaches, especially for complex queries requiring both semantic understanding and relational reasoning.",
                  "",
                  "### Hybrid Graph-Vector RAG Architecture",
                  "",
                  "The state-of-the-art approach combines the complementary strengths of graph and vector search into a unified system that adaptively leverages both methods based on query characteristics and content structure.",
                  ""
                ],
                "line_count": 41
              },
              {
                "start_line": 2032,
                "end_line": 2036,
                "language": "",
                "content": [
                  "",
                  "**State-of-the-art hybrid architecture** addresses the fundamental limitation that has plagued RAG systems since their inception: no single retrieval method can handle the full spectrum of information needs. Vector search excels at semantic similarity but fails at relationship reasoning. Graph search excels at multi-hop reasoning but struggles with semantic nuance. The hybrid approach leverages each method's strengths while mitigating their weaknesses.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2052,
                "end_line": 2056,
                "language": "python",
                "content": [
                  "",
                  "**Adaptive fusion strategy** represents a major architectural innovation. Rather than static weighting, the system analyzes each query to determine the optimal combination of retrieval methods. Factual queries like \"What is Tesla's market cap?\" benefit from vector similarity, while analytical queries like \"How do Tesla's partnerships affect their supply chain?\" require graph traversal to understand multi-entity relationships.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2068,
                "end_line": 2072,
                "language": "",
                "content": [
                  "",
                  "**Component integration architecture** brings together four critical components: Neo4j for graph storage and queries, vector store for semantic similarity, embedding model for semantic encoding, and LLM for query analysis and response generation. This multi-component approach enables the system to leverage the best available technology for each aspect of the retrieval process.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2083,
                "end_line": 2087,
                "language": "",
                "content": [
                  "",
                  "**Multiple fusion strategies** provide flexibility for different use cases and query patterns. Weighted combination uses learned weights for linear blending, rank fusion applies reciprocal rank fusion (popular in information retrieval), cascade retrieval performs sequential refinement, and adaptive selection dynamically chooses the best approach based on query analysis.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2095,
                "end_line": 2101,
                "language": "python",
                "content": [
                  "",
                  "**Production-ready performance monitoring** tracks key metrics across the hybrid pipeline. This enables performance optimization, bottleneck identification, and SLA monitoring in enterprise deployments. Tracking each component's latency separately helps identify whether performance issues stem from vector search, graph traversal, or fusion logic.",
                  "",
                  "### The Core Hybrid Search Pipeline",
                  ""
                ],
                "line_count": 5
              },
              {
                "start_line": 2113,
                "end_line": 2117,
                "language": "",
                "content": [
                  "",
                  "**Five-stage hybrid pipeline** represents the culmination of graph-based RAG research. Each stage serves a specific purpose: parallel retrieval maximizes coverage, entity bridging connects semantic and structural search, intelligent fusion leverages query-specific strengths, quality assurance ensures relevance, and response generation provides comprehensive answers.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2129,
                "end_line": 2133,
                "language": "",
                "content": [
                  "",
                  "**Real-world query complexity** demonstrates why hybrid approaches are essential for enterprise applications. The Tesla supplier example requires semantic understanding of \"environmental impacts\" (vector search strength) plus multi-hop graph traversal through company relationships (graph search strength). Neither approach alone can handle this complexity effectively.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2144,
                "end_line": 2148,
                "language": "",
                "content": [
                  "",
                  "**Configuration-driven flexibility** allows fine-tuning for different domains and use cases. The default configuration favors graph search (0.6 vs 0.4 weight) because relationship reasoning often provides more valuable insights than semantic similarity alone. The similarity threshold of 0.7 ensures high-quality results while the top-k limits prevent information overload.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2151,
                "end_line": 2155,
                "language": "",
                "content": [
                  "",
                  "**Transparent execution logging** provides essential debugging and monitoring capabilities for production systems. Understanding which strategy and weights were applied for each query enables performance analysis and system optimization.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2162,
                "end_line": 2166,
                "language": "",
                "content": [
                  "",
                  "**Step 1: Vector similarity foundation** performs traditional semantic search to establish a baseline of relevant documents. This stage leverages the embedding model's semantic understanding to find content that matches the query's meaning, regardless of exact word matches. Vector search excels at finding conceptually related content across different vocabularies and phrasings.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2172,
                "end_line": 2176,
                "language": "python",
                "content": [
                  "",
                  "**Step 2: Entity bridging innovation** represents a key breakthrough in hybrid RAG architecture. Rather than running graph and vector search independently, this approach uses vector results to identify relevant entities in the knowledge graph. This ensures graph traversal starts from contextually relevant nodes, dramatically improving the quality and relevance of discovered relationships.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2183,
                "end_line": 2187,
                "language": "python",
                "content": [
                  "",
                  "**Step 3: Relationship discovery through graph traversal** explores multi-hop connections from the seed entities identified in step 2. This stage discovers insights that semantic similarity alone cannot find - causal relationships, hierarchical structures, and complex interdependencies. The timing metrics reveal graph traversal is typically 2-3x slower than vector search but provides uniquely valuable relationship insights.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2197,
                "end_line": 2201,
                "language": "",
                "content": [
                  "",
                  "**Step 4: Intelligent fusion - the hybrid breakthrough** combines vector and graph results using sophisticated algorithms that understand each method's strengths. This isn't simple concatenation - the fusion strategy analyzes query characteristics to determine optimal weighting, removes redundancy, and ensures complementary insights enhance rather than compete with each other.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2210,
                "end_line": 2214,
                "language": "python",
                "content": [
                  "",
                  "**Step 5: Comprehensive response synthesis** generates the final answer using the expertly fused context from both vector and graph sources. This stage leverages an LLM to synthesize insights from semantic matches and relationship discoveries into a coherent, comprehensive response that addresses the query's complexity.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2222,
                "end_line": 2226,
                "language": "",
                "content": [
                  "",
                  "**Performance tracking for production optimization** captures timing data for each pipeline stage. This enables identification of bottlenecks, performance trending, and optimization opportunities. The final log message provides immediate feedback on search completion time and context count - key indicators of search effectiveness.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2246,
                "end_line": 2270,
                "language": "",
                "content": [
                  "",
                  "**Comprehensive result structure** provides complete transparency into the hybrid search process. This rich metadata enables downstream systems to understand how the answer was generated, which sources contributed most significantly, and what performance characteristics were observed. This level of detail is essential for production systems requiring explainability and audit trails.",
                  "",
                  "#### Step 9: Adaptive Fusion Strategy",
                  "",
                  "### Understanding Adaptive Selection",
                  "",
                  "This sophisticated fusion strategy implements query-aware combination of results. Different queries benefit from different retrieval emphasis:",
                  "",
                  "- **Factual queries** (\"What is X?\") \u2192 Higher vector weight  ",
                  "- **Analytical queries** (\"How does X affect Y?\") \u2192 Balanced combination",
                  "- **Relational queries** (\"What connects X to Y?\") \u2192 Higher graph weight",
                  "- **Complex synthesis** (\"Analyze X's impact on Y through Z\") \u2192 Dynamic weighting",
                  "",
                  "The fusion process implements key innovations:",
                  "",
                  "1. **Query Analysis**: LLM-based understanding of query intent and complexity",
                  "2. **Dynamic Weighting**: Adaptive weights based on query characteristics  ",
                  "3. **Diversity Selection**: Ensures varied perspectives in final context",
                  "4. **Quality Assurance**: Validates context relevance and coherence",
                  "",
                  "### Core Adaptive Selection Method",
                  ""
                ],
                "line_count": 23
              },
              {
                "start_line": 2289,
                "end_line": 2295,
                "language": "",
                "content": [
                  "",
                  "### Context Extraction and Processing",
                  "",
                  "Extract and prepare contexts from both vector and graph retrieval:",
                  ""
                ],
                "line_count": 5
              },
              {
                "start_line": 2316,
                "end_line": 2322,
                "language": "",
                "content": [
                  "",
                  "### Adaptive Scoring and Context Processing",
                  "",
                  "Now apply adaptive weights and query-specific boosts to all contexts:",
                  ""
                ],
                "line_count": 5
              },
              {
                "start_line": 2345,
                "end_line": 2351,
                "language": "",
                "content": [
                  "",
                  "### Graph Context Processing with Adaptive Scoring",
                  "",
                  "Process graph contexts with relationship-aware scoring:",
                  ""
                ],
                "line_count": 5
              },
              {
                "start_line": 2358,
                "end_line": 2362,
                "language": "",
                "content": [
                  "",
                  "This section demonstrates **relationship-aware scoring**, a key advancement in graph-based RAG systems. Unlike traditional RAG that treats all retrieved chunks equally, graph RAG incorporates relationship confidence and path complexity into scoring. The `fusion_weights['graph_weight']` represents the query-specific importance of graph traversal results versus vector similarity results.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2369,
                "end_line": 2373,
                "language": "",
                "content": [
                  "",
                  "**Multi-hop reasoning enhancement** is critical for enterprise knowledge graphs where valuable insights often require traversing multiple relationship hops. Complex queries like \"How do Tesla's supplier relationships impact their environmental sustainability goals?\" require 3+ hop reasoning: Tesla \u2192 Suppliers \u2192 Supply Chain Practices \u2192 Environmental Impact. The 1.3x boost ensures these valuable multi-hop insights aren't overshadowed by simpler single-hop facts.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2384,
                "end_line": 2388,
                "language": "",
                "content": [
                  "",
                  "**Context enrichment with graph metadata** provides crucial transparency for enterprise applications. Unlike vector RAG where context sources are opaque embeddings, graph RAG maintains provenance through path metadata. This enables audit trails showing exactly how conclusions were reached through relationship traversals - essential for compliance in regulated industries.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2398,
                "end_line": 2402,
                "language": "",
                "content": [
                  "",
                  "**Diversity-aware selection** addresses a critical limitation in traditional RAG systems where top-k selection often returns redundant information. Graph-based diversity selection considers both semantic similarity and structural diversity - ensuring the final context includes different relationship types, entity categories, and reasoning paths. This prevents the common RAG problem of highly similar chunks dominating the context window.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2459,
                "end_line": 2463,
                "language": "python",
                "content": [
                  "",
                  "**LLM-powered query analysis** represents a paradigm shift from rule-based query classification to intelligent, context-aware analysis. Traditional RAG systems use brittle pattern matching, while this approach leverages large language models to understand query intent, complexity, and optimal retrieval strategy. The structured JSON output ensures consistent classification across diverse query types.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2472,
                "end_line": 2476,
                "language": "",
                "content": [
                  "",
                  "**Benefit scoring guidelines** provide the LLM with explicit criteria for determining when graph versus vector search will be most effective. This addresses the core challenge in hybrid RAG - automatically determining the optimal balance between semantic similarity (vector strength) and relationship reasoning (graph strength) for each unique query.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2488,
                "end_line": 2492,
                "language": "",
                "content": [
                  "",
                  "**Robust error handling with intelligent defaults** ensures the system remains functional even when LLM analysis fails or returns incomplete results. The fallback logic incorporates query characteristics like length and interrogative structure to provide reasonable defaults. This production-ready approach prevents system failures while maintaining acceptable performance degradation.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2501,
                "end_line": 2505,
                "language": "",
                "content": [
                  "",
                  "**Heuristic-based defaults** provide reasonable fallback values when LLM analysis is incomplete. Notice how \"how\" and \"why\" questions get higher graph benefit scores (0.6) because they typically require causal reasoning and relationship traversal, while factual questions favor vector similarity with higher vector benefit scores (0.7).",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2511,
                "end_line": 2515,
                "language": "",
                "content": [
                  "",
                  "**Value normalization and bounds checking** prevents invalid benefit scores that could destabilize the fusion algorithm. This defensive programming approach is essential in production systems where LLMs might occasionally return unexpected values or formats.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2526,
                "end_line": 2530,
                "language": "",
                "content": [
                  "",
                  "**Pattern-based fallback analysis** provides a comprehensive safety net when LLM analysis completely fails. This approach uses linguistic patterns to classify queries - words like \"how\", \"why\", and \"analyze\" typically indicate complex queries requiring multi-step reasoning and relationship understanding.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2544,
                "end_line": 2548,
                "language": "",
                "content": [
                  "",
                  "**Hierarchical query type classification** follows a decision tree approach where more specific patterns (comparative, analytical, relational) are checked before falling back to the general \"factual\" category. This ensures accurate classification even with simple pattern matching when LLM analysis is unavailable.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2559,
                "end_line": 2565,
                "language": "",
                "content": [
                  "",
                  "**Comprehensive fallback return structure** ensures the same data format regardless of whether LLM analysis succeeded or failed. Notice the benefit scoring logic: analytical and relational queries get higher graph benefits (0.7) because they typically require relationship understanding, while factual queries get higher vector benefits (0.8) for direct semantic matching.",
                  "",
                  "### Step 10: Comprehensive Response Generation",
                  ""
                ],
                "line_count": 5
              },
              {
                "start_line": 2625,
                "end_line": 2645,
                "language": "",
                "content": [
                  "",
                  "---",
                  "",
                  "## Hands-On Exercise: Build Production GraphRAG System",
                  "",
                  "### Your Mission",
                  "",
                  "Create a production-ready GraphRAG system that combines document analysis with code repository understanding.",
                  "",
                  "### Requirements",
                  "",
                  "1. **Knowledge Graph Construction**: Build KG from documents with entity/relationship extraction",
                  "2. **Code Analysis**: Implement AST-based analysis for software repositories",
                  "3. **Graph Storage**: Deploy Neo4j with optimized schema and indices",
                  "4. **Multi-Hop Retrieval**: Implement semantic-guided graph traversal",
                  "5. **Hybrid Search**: Combine graph and vector search with adaptive fusion",
                  "",
                  "### Implementation Architecture",
                  ""
                ],
                "line_count": 19
              },
              {
                "start_line": 2716,
                "end_line": 2891,
                "language": "",
                "content": [
                  "",
                  "---",
                  "",
                  "## Chapter Summary",
                  "",
                  "### What You've Built: Complete GraphRAG System",
                  "",
                  "**Advanced Graph Architectures:**",
                  "- \u2705 **NodeRAG Architecture**: Heterogeneous graph system with specialized node types and three-stage processing",
                  "- \u2705 **Structured Brain Architecture**: Six specialized node types mimicking human knowledge organization",
                  "- \u2705 **Advanced Graph Algorithms**: Personalized PageRank and HNSW similarity integration",
                  "",
                  "**Graph Construction Systems:**",
                  "- \u2705 **Traditional GraphRAG**: Knowledge graph construction from unstructured documents with LLM-enhanced extraction",
                  "- \u2705 **Code GraphRAG**: AST parsing and call graph analysis for software repositories",
                  "- \u2705 **Production Neo4j Integration**: Optimized batch operations and performance-critical indexing",
                  "",
                  "**Intelligent Retrieval Systems:**",
                  "- \u2705 **Multi-hop Graph Traversal**: Semantic guidance, path ranking, and coherent reasoning pathways",
                  "- \u2705 **Hybrid Graph-Vector Search**: Adaptive fusion strategies combining graph reasoning with vector similarity",
                  "",
                  "### Key Technical Skills Learned",
                  "",
                  "**Graph Architecture & Algorithms:**",
                  "1. **NodeRAG Architecture**: Heterogeneous graph design, specialized node processing, three-stage pipeline implementation",
                  "2. **Advanced Graph Algorithms**: Personalized PageRank implementation, HNSW integration, semantic pathway construction",
                  "3. **Graph Traversal**: Multi-hop reasoning, semantic-guided exploration, coherent path synthesis",
                  "",
                  "**Knowledge Engineering & Analysis:**",
                  "4. **Knowledge Graph Engineering**: Traditional entity extraction, relationship mapping, graph construction",
                  "5. **Code Analysis**: AST parsing, dependency analysis, call graph construction",
                  "6. **Graph Databases**: Neo4j schema design, performance optimization, batch operations",
                  "",
                  "**Hybrid Retrieval Systems:**",
                  "7. **Hybrid Retrieval**: Graph-vector fusion, adaptive weighting, comprehensive response generation",
                  "",
                  "### Performance Characteristics",
                  "",
                  "- **NodeRAG Processing**: 3-stage pipeline processes 10K+ documents with 85-95% pathway coherence",
                  "- **Personalized PageRank**: Sub-100ms semantic pathway computation on 100K+ heterogeneous graphs",
                  "- **HNSW Graph Integration**: 200-500ms similarity edge construction with 80-90% type compatibility",
                  "- **Traditional Entity Extraction**: 80-90% precision with LLM-enhanced methods",
                  "- **Graph Traversal**: Sub-second multi-hop queries on graphs with 100K+ entities",
                  "- **Hybrid Search**: 30-40% improvement in complex query answering over pure vector search",
                  "- **Code Analysis**: Comprehensive repository analysis with relationship extraction",
                  "",
                  "### When to Choose NodeRAG, GraphRAG, or Vector RAG",
                  "",
                  "### Use NodeRAG when",
                  "",
                  "- **Complex reasoning** requires understanding different knowledge types (concepts, entities, relationships)",
                  "- **Coherent narratives** needed from fragmented information sources",
                  "- **Educational applications** where understanding knowledge structure is important",
                  "- **Multi-domain knowledge** needs specialized processing (technical + business + regulatory)",
                  "- **Advanced query types** requiring synthesis across different knowledge structures",
                  "",
                  "### Use Traditional GraphRAG when",
                  "",
                  "- **Multi-hop reasoning** is required (\"What technologies do Apple's partners' suppliers use?\")",
                  "- **Relationship discovery** is key (\"How are these companies connected?\")",
                  "- **Comprehensive exploration** needed (\"Find all related information\")",
                  "- **Complex analytical queries** (\"Analyze the supply chain impact of X on Y\")",
                  "- **Domain has rich entity relationships** (business networks, scientific literature, code repositories)",
                  "",
                  "### Use Vector RAG when",
                  "",
                  "- **Direct factual lookup** (\"What is quantum computing?\")",
                  "- **Semantic similarity** is primary concern (\"Find similar concepts\")",
                  "- **Simple Q&A** scenarios (\"When was X founded?\")",
                  "- **Limited relationship structure** in domain",
                  "- **Fast response time** is critical",
                  "",
                  "### Use Hybrid GraphRAG when",
                  "",
                  "- **Query types vary** (production systems with diverse users)",
                  "- **Maximum coverage** is needed (research and analysis scenarios)",
                  "- **Both factual accuracy and insight discovery** are important",
                  "- **You want the best of both worlds** (most real-world applications)",
                  "",
                  "### GraphRAG vs Vector RAG: Concrete Examples",
                  "",
                  "**Example Query**: \"What are the environmental impacts of technologies used by Apple's automotive partners?\"",
                  "",
                  "### Vector RAG Approach",
                  "",
                  "1. Search for \"environmental impacts technologies\"",
                  "2. Search for \"Apple automotive partners\"",
                  "3. Try to connect results manually",
                  "4. **Result**: Finds documents about each topic separately, but struggles to connect them",
                  "",
                  "### GraphRAG Approach",
                  "",
                  "1. Find Apple entity in knowledge graph",
                  "2. Traverse: Apple \u2192 partners_with \u2192 [Automotive Companies]",
                  "3. Traverse: [Automotive Companies] \u2192 uses_technology \u2192 [Technologies]",
                  "4. Traverse: [Technologies] \u2192 has_environmental_impact \u2192 [Impacts]",
                  "5. **Result**: Discovers specific impact chains that no single document contains",
                  "",
                  "### Hybrid Approach",
                  "",
                  "1. Uses vector search to understand \"environmental impacts\" semantically",
                  "2. Uses graph traversal to follow the relationship chain",
                  "3. Combines both to provide comprehensive, accurate answers",
                  "4. **Result**: Best coverage with highest accuracy",
                  "",
                  "---",
                  "",
                  "## \ud83d\udcdd Multiple Choice Test - Session 6",
                  "",
                  "Test your understanding of graph-based RAG systems and GraphRAG implementations.",
                  "",
                  "**Question 1:** What is the primary advantage of GraphRAG over traditional vector-based RAG?  ",
                  "A) Faster query processing  ",
                  "B) Lower computational requirements  ",
                  "C) Multi-hop reasoning through explicit relationship modeling  ",
                  "D) Simpler system architecture  ",
                  "",
                  "**Question 2:** In knowledge graph construction, what is the purpose of entity standardization?  ",
                  "A) To reduce memory usage  ",
                  "B) To merge different mentions of the same entity (e.g., \"Apple Inc.\" and \"Apple\")  ",
                  "C) To improve query speed  ",
                  "D) To compress graph storage  ",
                  "",
                  "**Question 3:** Which graph traversal algorithm is most suitable for finding related entities within a limited number of hops?  ",
                  "A) Depth-First Search (DFS)  ",
                  "B) Breadth-First Search (BFS)  ",
                  "C) Dijkstra's algorithm  ",
                  "D) A* search  ",
                  "",
                  "**Question 4:** In Code GraphRAG, what information is typically extracted from Abstract Syntax Trees (ASTs)?  ",
                  "A) Only function definitions  ",
                  "B) Function calls, imports, class hierarchies, and variable dependencies  ",
                  "C) Only variable names  ",
                  "D) Just file names and sizes  ",
                  "",
                  "**Question 5:** What is the key benefit of hybrid graph-vector search?  ",
                  "A) Reduced computational cost  ",
                  "B) Combining structural relationships with semantic similarity  ",
                  "C) Simpler implementation  ",
                  "D) Faster indexing  ",
                  "",
                  "**Question 6:** When should you choose Neo4j over a simple graph data structure for GraphRAG?  ",
                  "A) Always, regardless of scale  ",
                  "B) When you need persistent storage and complex queries at scale  ",
                  "C) Only for small datasets  ",
                  "D) Never, simple structures are always better  ",
                  "",
                  "**Question 7:** What is the primary challenge in multi-hop graph traversal for RAG?  ",
                  "A) Memory limitations  ",
                  "B) Balancing comprehensiveness with relevance and avoiding information explosion  ",
                  "C) Slow database queries  ",
                  "D) Complex code implementation  ",
                  "",
                  "**Question 8:** In production GraphRAG systems, what is the most important consideration for incremental updates?  ",
                  "A) Minimizing downtime while maintaining graph consistency  ",
                  "B) Reducing storage costs  ",
                  "C) Maximizing query speed  ",
                  "D) Simplifying the codebase  ",
                  "",
                  "[**\ud83d\uddc2\ufe0f View Test Solutions \u2192**](Session6_Test_Solutions.md)",
                  "",
                  "---",
                  "",
                  "## \ud83e\udded Navigation",
                  "",
                  "**Previous:** [Session 5 - RAG Evaluation & Quality Assessment](Session5_RAG_Evaluation_Quality_Assessment.md)",
                  "",
                  "## Optional Deep Dive Modules",
                  "",
                  "- \ud83d\udd2c **[Module A: Advanced Graph Algorithms](Session6_ModuleA_Advanced_Algorithms.md)** - Complex graph traversal and reasoning patterns",
                  "- \ud83c\udfed **[Module B: Production GraphRAG](Session6_ModuleB_Production_Systems.md)** - Enterprise graph database deployment",
                  "",
                  "**Next:** [Session 7 - Agentic RAG Systems \u2192](Session7_Agentic_RAG_Systems.md)",
                  ""
                ],
                "line_count": 174
              }
            ],
            "large_blocks": [
              {
                "start_line": 294,
                "end_line": 318,
                "language": "python",
                "content": [
                  "class NodeRAGHNSW:",
                  "    \"\"\"HNSW similarity edges integrated into NodeRAG heterogeneous graphs\"\"\"",
                  "",
                  "    def build_hnsw_graph_integration(self, heterogeneous_graph, embedding_model):",
                  "        \"\"\"Build HNSW similarity edges within the existing graph structure\"\"\"",
                  "",
                  "        # Extract embeddings for all nodes by type",
                  "        node_embeddings = {}",
                  "        node_types = {}",
                  "",
                  "        for node_id, node_data in heterogeneous_graph.nodes(data=True):",
                  "            node_type = node_data.get('node_type')",
                  "            node_content = self.get_node_content_for_embedding(node_data, node_type)",
                  "",
                  "            # Generate specialized embeddings based on node type",
                  "            embedding = self.generate_typed_embedding(",
                  "                content=node_content,",
                  "                node_type=node_type,",
                  "                embedding_model=embedding_model",
                  "            )",
                  "",
                  "            node_embeddings[node_id] = embedding",
                  "            node_types[node_id] = node_type"
                ],
                "line_count": 23
              },
              {
                "start_line": 678,
                "end_line": 774,
                "language": "python",
                "content": [
                  "    def _decomposition_stage(self, documents: List[str], config: Dict) -> Dict[str, Any]:",
                  "        \"\"\"Stage 1: Multi-granularity decomposition with specialized node creation.\"\"\"",
                  "",
                  "        print(\"Performing multi-granularity analysis...\")",
                  "        decomposition_results = {",
                  "            'entity_nodes': [],",
                  "            'concept_nodes': [],",
                  "            'document_nodes': [],",
                  "            'relationship_nodes': [],",
                  "            'hierarchical_structures': {}",
                  "        }",
                  "",
                  "        for doc_idx, document in enumerate(documents):",
                  "            print(f\"Decomposing document {doc_idx + 1}/{len(documents)}\")",
                  "",
                  "            # Extract entity nodes with rich metadata",
                  "            if 'entity' in config['node_types']:",
                  "                entity_nodes = self._extract_entity_nodes(document, doc_idx)",
                  "                decomposition_results['entity_nodes'].extend(entity_nodes)",
                  "",
                  "            # Extract concept nodes for abstract concepts and topics",
                  "            if 'concept' in config['node_types']:",
                  "                concept_nodes = self._extract_concept_nodes(document, doc_idx)",
                  "                decomposition_results['concept_nodes'].extend(concept_nodes)",
                  "",
                  "            # Extract document nodes for text segments",
                  "            if 'document' in config['node_types']:",
                  "                document_nodes = self._extract_document_nodes(document, doc_idx)",
                  "                decomposition_results['document_nodes'].extend(document_nodes)",
                  "",
                  "            # Extract explicit relationship nodes",
                  "            if 'relationship' in config['node_types']:",
                  "                relationship_nodes = self._extract_relationship_nodes(document, doc_idx)",
                  "                decomposition_results['relationship_nodes'].extend(relationship_nodes)",
                  "",
                  "        # Build hierarchical structures at multiple abstraction levels",
                  "        decomposition_results['hierarchical_structures'] = self._build_hierarchical_structures(",
                  "            decomposition_results",
                  "        )",
                  "",
                  "        print(f\"Decomposition complete: {sum(len(nodes) for nodes in decomposition_results.values() if isinstance(nodes, list))} nodes created\")",
                  "        return decomposition_results",
                  "",
                  "    def _augmentation_stage(self, decomposition_result: Dict, config: Dict) -> Dict[str, Any]:",
                  "        \"\"\"Stage 2: Cross-reference integration and HNSW similarity edge construction.\"\"\"",
                  "",
                  "        print(\"Performing cross-reference integration...\")",
                  "",
                  "        # Cross-link related nodes across different types",
                  "        cross_references = self._build_cross_references(decomposition_result)",
                  "",
                  "        # Build HNSW similarity edges for high-performance retrieval",
                  "        if config.get('enable_hnsw_similarity', True):",
                  "            print(\"Constructing HNSW similarity edges...\")",
                  "            similarity_edges = self._build_hnsw_similarity_edges(decomposition_result)",
                  "        else:",
                  "            similarity_edges = {}",
                  "",
                  "        # Semantic enrichment with contextual metadata",
                  "        enriched_nodes = self._apply_semantic_enrichment(decomposition_result)",
                  "",
                  "        return {",
                  "            'enriched_nodes': enriched_nodes,",
                  "            'cross_references': cross_references,",
                  "            'similarity_edges': similarity_edges,",
                  "            'augmentation_metadata': {",
                  "                'cross_references_count': len(cross_references),",
                  "                'similarity_edges_count': len(similarity_edges),",
                  "                'enrichment_applied': True",
                  "            }",
                  "        }",
                  "",
                  "    def _enrichment_stage(self, augmentation_result: Dict, config: Dict) -> Dict[str, Any]:",
                  "        \"\"\"Stage 3: Personalized PageRank and reasoning pathway construction.\"\"\"",
                  "",
                  "        print(\"Constructing reasoning pathways...\")",
                  "",
                  "        # Build reasoning pathways for logically coherent contexts",
                  "        reasoning_pathways = {}",
                  "        if config.get('reasoning_integration', True):",
                  "            reasoning_pathways = self._construct_reasoning_pathways_stage3(",
                  "                augmentation_result, config",
                  "            )",
                  "",
                  "        # Apply graph-centric optimization",
                  "        optimized_structure = self._apply_graph_optimization(",
                  "            augmentation_result, reasoning_pathways",
                  "        )",
                  "",
                  "        return {",
                  "            'final_nodes': optimized_structure['nodes'],",
                  "            'reasoning_pathways': reasoning_pathways,",
                  "            'optimization_metadata': optimized_structure['metadata'],",
                  "            'enrichment_complete': True",
                  "        }"
                ],
                "line_count": 95
              },
              {
                "start_line": 778,
                "end_line": 887,
                "language": "python",
                "content": [
                  "class PersonalizedPageRankProcessor:",
                  "    \"\"\"Personalized PageRank for semantic traversal in NodeRAG.\"\"\"",
                  "",
                  "    def __init__(self, damping_factor: float = 0.85):",
                  "        self.damping_factor = damping_factor",
                  "        self.pagerank_cache = {}",
                  "",
                  "    def compute_pagerank(self, graph: nx.MultiDiGraph, node_registry: Dict) -> Dict[str, float]:",
                  "        \"\"\"Compute personalized PageRank scores for semantic traversal.\"\"\"",
                  "",
                  "        if not graph.nodes():",
                  "            return {}",
                  "",
                  "        # Create personalization vector based on node types and importance",
                  "        personalization = self._create_personalization_vector(graph, node_registry)",
                  "",
                  "        # Compute Personalized PageRank",
                  "        try:",
                  "            pagerank_scores = nx.pagerank(",
                  "                graph,",
                  "                alpha=self.damping_factor,",
                  "                personalization=personalization,",
                  "                max_iter=100,",
                  "                tol=1e-6",
                  "            )",
                  "",
                  "            # Normalize scores by node type for better semantic traversal",
                  "            normalized_scores = self._normalize_scores_by_type(",
                  "                pagerank_scores, node_registry",
                  "            )",
                  "",
                  "            return normalized_scores",
                  "",
                  "        except Exception as e:",
                  "            print(f\"PageRank computation error: {e}\")",
                  "            return {}",
                  "",
                  "    def _create_personalization_vector(self, graph: nx.MultiDiGraph,",
                  "                                     node_registry: Dict) -> Dict[str, float]:",
                  "        \"\"\"Create personalization vector emphasizing important node types.\"\"\"",
                  "",
                  "        personalization = {}",
                  "        total_nodes = len(graph.nodes())",
                  "",
                  "        # Weight different node types for semantic importance",
                  "        type_weights = {",
                  "            NodeType.ENTITY: 0.3,      # High weight for entities",
                  "            NodeType.CONCEPT: 0.25,    # High weight for concepts",
                  "            NodeType.RELATIONSHIP: 0.2, # Medium weight for relationships",
                  "            NodeType.DOCUMENT: 0.15,   # Medium weight for documents",
                  "            NodeType.CLUSTER: 0.1      # Lower weight for clusters",
                  "        }",
                  "",
                  "        for node_id in graph.nodes():",
                  "            if node_id in node_registry:",
                  "                node_type = node_registry[node_id].node_type",
                  "                base_weight = type_weights.get(node_type, 0.1)",
                  "",
                  "                # Boost weight based on node confidence and connections",
                  "                confidence_boost = node_registry[node_id].confidence * 0.2",
                  "                connection_boost = min(len(node_registry[node_id].connections) * 0.1, 0.3)",
                  "",
                  "                final_weight = base_weight + confidence_boost + connection_boost",
                  "                personalization[node_id] = final_weight",
                  "            else:",
                  "                personalization[node_id] = 0.1  # Default weight",
                  "",
                  "        # Normalize to sum to 1.0",
                  "        total_weight = sum(personalization.values())",
                  "        if total_weight > 0:",
                  "            for node_id in personalization:",
                  "                personalization[node_id] /= total_weight",
                  "",
                  "        return personalization",
                  "",
                  "    def get_semantic_pathway(self, graph: nx.MultiDiGraph, start_node: str,",
                  "                           target_concepts: List[str], max_depth: int = 5) -> List[str]:",
                  "        \"\"\"Find semantic pathway using PageRank-guided traversal.\"\"\"",
                  "",
                  "        if start_node not in graph:",
                  "            return []",
                  "",
                  "        # Use PageRank scores to guide pathway exploration",
                  "        pagerank_scores = self.pagerank_cache.get(id(graph))",
                  "        if not pagerank_scores:",
                  "            return []",
                  "",
                  "        visited = set()",
                  "        pathway = [start_node]",
                  "        current_node = start_node",
                  "        depth = 0",
                  "",
                  "        while depth < max_depth and current_node:",
                  "            visited.add(current_node)",
                  "",
                  "            # Find best next node based on PageRank scores and target concepts",
                  "            next_node = self._find_best_next_node(",
                  "                graph, current_node, target_concepts, pagerank_scores, visited",
                  "            )",
                  "",
                  "            if next_node and next_node not in visited:",
                  "                pathway.append(next_node)",
                  "                current_node = next_node",
                  "                depth += 1",
                  "            else:",
                  "                break",
                  "",
                  "        return pathway"
                ],
                "line_count": 108
              },
              {
                "start_line": 891,
                "end_line": 960,
                "language": "python",
                "content": [
                  "    def _merge_similar_entities(self, entities: Dict[str, Any],",
                  "                               similarity_threshold: float = 0.85) -> Dict[str, Any]:",
                  "        \"\"\"Merge semantically similar entities.\"\"\"",
                  "",
                  "        from sentence_transformers import SentenceTransformer",
                  "        from sklearn.metrics.pairwise import cosine_similarity",
                  "        import numpy as np",
                  "",
                  "        # Load embedding model for similarity computation",
                  "        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')",
                  "",
                  "        entity_names = list(entities.keys())",
                  "        if len(entity_names) < 2:",
                  "            return entities",
                  "",
                  "        # Generate embeddings for entity canonical forms",
                  "        entity_embeddings = embedding_model.encode(entity_names)",
                  "",
                  "        # Calculate similarity matrix",
                  "        similarity_matrix = cosine_similarity(entity_embeddings)",
                  "",
                  "        # Find similar entity pairs",
                  "        merged_entities = {}",
                  "        entity_clusters = []",
                  "        processed_entities = set()",
                  "",
                  "        for i, entity1 in enumerate(entity_names):",
                  "            if entity1 in processed_entities:",
                  "                continue",
                  "",
                  "            # Find similar entities",
                  "            cluster = [entity1]",
                  "            for j, entity2 in enumerate(entity_names):",
                  "                if i != j and entity2 not in processed_entities:",
                  "                    if similarity_matrix[i][j] > similarity_threshold:",
                  "                        cluster.append(entity2)",
                  "",
                  "            # Create merged entity",
                  "            if len(cluster) > 1:",
                  "                # Choose canonical form (highest confidence entity)",
                  "                canonical_entity = max(",
                  "                    cluster,",
                  "                    key=lambda x: entities[x].get('confidence', 0.5)",
                  "                )",
                  "",
                  "                # Merge information",
                  "                merged_entity = entities[canonical_entity].copy()",
                  "                merged_entity['text_variants'] = []",
                  "                merged_entity['merged_from'] = cluster",
                  "",
                  "                for entity_name in cluster:",
                  "                    entity_data = entities[entity_name]",
                  "                    merged_entity['text_variants'].extend(",
                  "                        entity_data.get('text_variants', [entity_name])",
                  "                    )",
                  "                    processed_entities.add(entity_name)",
                  "",
                  "                # Remove duplicates in text variants",
                  "                merged_entity['text_variants'] = list(set(merged_entity['text_variants']))",
                  "                merged_entities[canonical_entity] = merged_entity",
                  "",
                  "            else:",
                  "                # Single entity, no merging needed",
                  "                merged_entities[entity1] = entities[entity1]",
                  "                processed_entities.add(entity1)",
                  "",
                  "        print(f\"Entity merging: {len(entities)} -> {len(merged_entities)} entities\")",
                  "        return merged_entities"
                ],
                "line_count": 68
              },
              {
                "start_line": 1599,
                "end_line": 1635,
                "language": "",
                "content": [
                  "",
                  "This multi-hop traversal discovers information that no single document contains, synthesizing knowledge from the relationship structure itself. The graph becomes a reasoning engine, not just a retrieval system.",
                  "",
                  "### Graph Traversal Strategies for Different Query Types",
                  "",
                  "Different queries require different traversal approaches, each optimized for specific reasoning patterns:",
                  "",
                  "**Direct Relationship Queries** use **Breadth-First Traversal** to find immediate connections (\"Who works with Apple?\"). This strategy explores all first-hop neighbors before moving to second-hop, ensuring comprehensive coverage of direct relationships.",
                  "",
                  "**Connection Discovery Queries** use **Depth-First Traversal** to explore deep relationship chains (\"What's the connection between Apple and Tesla?\"). This strategy follows paths to their conclusion, ideal for finding indirect connections through multiple intermediaries.",
                  "",
                  "**Semantic Reasoning Queries** use **Semantic-Guided Traversal** that follows paths most relevant to query semantics, filtering relationships based on their relevance to the question context. This enables focused exploration of semantically coherent pathways.",
                  "",
                  "### Advanced Traversal Strategies",
                  "",
                  "**Relevance-Ranked Traversal** prioritizes high-confidence, important relationships using PageRank scores and relationship confidence levels. This strategy ensures that the most reliable knowledge pathways are explored first, improving answer quality.",
                  "",
                  "**Community-Focused Traversal** explores dense clusters of related entities, useful for questions about industry sectors or technology ecosystems. This approach leverages graph community structure to find comprehensive related information.",
                  "",
                  "Our traversal engine adaptively selects strategies based on query characteristics, ensuring optimal exploration for each use case.",
                  "",
                  "### Performance vs Completeness Trade-offs",
                  "",
                  "Graph traversal faces the \"explosion problem\" - the number of possible paths grows exponentially with hop count. Our engine implements sophisticated pruning strategies to manage this complexity:",
                  "",
                  "**Semantic Filtering** ensures only paths semantically related to the query are explored, dramatically reducing the search space while maintaining relevance. **Confidence Thresholding** ignores low-quality relationships, focusing computational resources on reliable knowledge connections.",
                  "",
                  "**Path Length Limits** prevent infinite traversal while enabling meaningful multi-hop reasoning. **Relevance Scoring** ranks paths by likely usefulness, ensuring the most promising reasoning pathways are explored first.",
                  "",
                  "This multi-layered approach ensures comprehensive coverage while maintaining reasonable response times, making GraphRAG practical for real-time applications.",
                  "",
                  "### Advanced Graph Traversal Engine",
                  "",
                  "The heart of GraphRAG's multi-hop reasoning capability lies in intelligent traversal algorithms that can navigate complex knowledge graphs to answer sophisticated queries. Our traversal engine combines multiple strategies and implements sophisticated pruning to balance comprehensiveness with performance.",
                  ""
                ],
                "line_count": 35
              },
              {
                "start_line": 1980,
                "end_line": 2022,
                "language": "",
                "content": [
                  "",
                  "---",
                  "",
                  "## Part 5: Hybrid Graph-Vector Search",
                  "",
                  "### Integrated Graph and Vector Retrieval",
                  "",
                  "### Why Hybrid Search Outperforms Pure Approaches",
                  "",
                  "Neither graph-only nor vector-only search is optimal for all scenarios. Each approach has distinct strengths and limitations that make hybrid systems significantly more powerful than either approach alone.",
                  "",
                  "### Vector Search: Semantic Similarity Powerhouse",
                  "",
                  "**Vector search excels at semantic understanding:** It provides excellent semantic similarity matching, naturally handles synonyms and paraphrasing, enables fast retrieval for well-defined concepts, and works effectively with isolated facts. For queries like \"What is machine learning?\", vector search quickly finds relevant content based on semantic similarity.",
                  "",
                  "**However, vector search has critical limitations:** It cannot traverse relationships between concepts, misses connections requiring multi-step reasoning, struggles with queries requiring synthesis across sources, and has limited understanding of entity relationships. Vector search treats knowledge as isolated fragments rather than connected information.",
                  "",
                  "### Graph Search: Relationship and Reasoning Excellence",
                  "",
                  "**Graph search enables sophisticated reasoning:** It discovers implicit connections through relationships, enables multi-hop reasoning and inference, understands structural importance and centrality, and reveals information not contained in any single document. For queries requiring connection discovery, graph search is unmatched.",
                  "",
                  "**But graph search also has limitations:** It depends heavily on explicit relationship extraction quality, may miss semantically similar but unconnected information, can be computationally expensive for large traversals, and requires comprehensive entity recognition. Graph search can miss obvious semantic connections if relationships weren't explicitly extracted.",
                  "",
                  "### The Hybrid Advantage: Best of Both Worlds",
                  "",
                  "Hybrid search combines both approaches strategically to overcome individual limitations:",
                  "",
                  "**Stage 1: Vector Search** identifies semantically relevant content using embedding similarity, capturing documents and entities that match the query's semantic intent. This provides comprehensive coverage of directly relevant information.",
                  "",
                  "**Stage 2: Graph Traversal** discovers related information through relationships, following logical pathways to find connected knowledge that vector search might miss. This adds multi-hop reasoning capabilities.",
                  "",
                  "**Stage 3: Intelligent Fusion** combines results based on query characteristics, balancing vector similarity scores with graph centrality and relationship confidence. The fusion process ensures optimal result ranking.",
                  "",
                  "**Stage 4: Adaptive Weighting** emphasizes the most effective approach for each query type - vector search for semantic queries, graph traversal for relationship queries, balanced weighting for complex analytical queries.",
                  "",
                  "This results in 30-40% improvement in answer quality over pure approaches, especially for complex queries requiring both semantic understanding and relational reasoning.",
                  "",
                  "### Hybrid Graph-Vector RAG Architecture",
                  "",
                  "The state-of-the-art approach combines the complementary strengths of graph and vector search into a unified system that adaptively leverages both methods based on query characteristics and content structure.",
                  ""
                ],
                "line_count": 41
              },
              {
                "start_line": 2246,
                "end_line": 2270,
                "language": "",
                "content": [
                  "",
                  "**Comprehensive result structure** provides complete transparency into the hybrid search process. This rich metadata enables downstream systems to understand how the answer was generated, which sources contributed most significantly, and what performance characteristics were observed. This level of detail is essential for production systems requiring explainability and audit trails.",
                  "",
                  "#### Step 9: Adaptive Fusion Strategy",
                  "",
                  "### Understanding Adaptive Selection",
                  "",
                  "This sophisticated fusion strategy implements query-aware combination of results. Different queries benefit from different retrieval emphasis:",
                  "",
                  "- **Factual queries** (\"What is X?\") \u2192 Higher vector weight  ",
                  "- **Analytical queries** (\"How does X affect Y?\") \u2192 Balanced combination",
                  "- **Relational queries** (\"What connects X to Y?\") \u2192 Higher graph weight",
                  "- **Complex synthesis** (\"Analyze X's impact on Y through Z\") \u2192 Dynamic weighting",
                  "",
                  "The fusion process implements key innovations:",
                  "",
                  "1. **Query Analysis**: LLM-based understanding of query intent and complexity",
                  "2. **Dynamic Weighting**: Adaptive weights based on query characteristics  ",
                  "3. **Diversity Selection**: Ensures varied perspectives in final context",
                  "4. **Quality Assurance**: Validates context relevance and coherence",
                  "",
                  "### Core Adaptive Selection Method",
                  ""
                ],
                "line_count": 23
              },
              {
                "start_line": 2716,
                "end_line": 2891,
                "language": "",
                "content": [
                  "",
                  "---",
                  "",
                  "## Chapter Summary",
                  "",
                  "### What You've Built: Complete GraphRAG System",
                  "",
                  "**Advanced Graph Architectures:**",
                  "- \u2705 **NodeRAG Architecture**: Heterogeneous graph system with specialized node types and three-stage processing",
                  "- \u2705 **Structured Brain Architecture**: Six specialized node types mimicking human knowledge organization",
                  "- \u2705 **Advanced Graph Algorithms**: Personalized PageRank and HNSW similarity integration",
                  "",
                  "**Graph Construction Systems:**",
                  "- \u2705 **Traditional GraphRAG**: Knowledge graph construction from unstructured documents with LLM-enhanced extraction",
                  "- \u2705 **Code GraphRAG**: AST parsing and call graph analysis for software repositories",
                  "- \u2705 **Production Neo4j Integration**: Optimized batch operations and performance-critical indexing",
                  "",
                  "**Intelligent Retrieval Systems:**",
                  "- \u2705 **Multi-hop Graph Traversal**: Semantic guidance, path ranking, and coherent reasoning pathways",
                  "- \u2705 **Hybrid Graph-Vector Search**: Adaptive fusion strategies combining graph reasoning with vector similarity",
                  "",
                  "### Key Technical Skills Learned",
                  "",
                  "**Graph Architecture & Algorithms:**",
                  "1. **NodeRAG Architecture**: Heterogeneous graph design, specialized node processing, three-stage pipeline implementation",
                  "2. **Advanced Graph Algorithms**: Personalized PageRank implementation, HNSW integration, semantic pathway construction",
                  "3. **Graph Traversal**: Multi-hop reasoning, semantic-guided exploration, coherent path synthesis",
                  "",
                  "**Knowledge Engineering & Analysis:**",
                  "4. **Knowledge Graph Engineering**: Traditional entity extraction, relationship mapping, graph construction",
                  "5. **Code Analysis**: AST parsing, dependency analysis, call graph construction",
                  "6. **Graph Databases**: Neo4j schema design, performance optimization, batch operations",
                  "",
                  "**Hybrid Retrieval Systems:**",
                  "7. **Hybrid Retrieval**: Graph-vector fusion, adaptive weighting, comprehensive response generation",
                  "",
                  "### Performance Characteristics",
                  "",
                  "- **NodeRAG Processing**: 3-stage pipeline processes 10K+ documents with 85-95% pathway coherence",
                  "- **Personalized PageRank**: Sub-100ms semantic pathway computation on 100K+ heterogeneous graphs",
                  "- **HNSW Graph Integration**: 200-500ms similarity edge construction with 80-90% type compatibility",
                  "- **Traditional Entity Extraction**: 80-90% precision with LLM-enhanced methods",
                  "- **Graph Traversal**: Sub-second multi-hop queries on graphs with 100K+ entities",
                  "- **Hybrid Search**: 30-40% improvement in complex query answering over pure vector search",
                  "- **Code Analysis**: Comprehensive repository analysis with relationship extraction",
                  "",
                  "### When to Choose NodeRAG, GraphRAG, or Vector RAG",
                  "",
                  "### Use NodeRAG when",
                  "",
                  "- **Complex reasoning** requires understanding different knowledge types (concepts, entities, relationships)",
                  "- **Coherent narratives** needed from fragmented information sources",
                  "- **Educational applications** where understanding knowledge structure is important",
                  "- **Multi-domain knowledge** needs specialized processing (technical + business + regulatory)",
                  "- **Advanced query types** requiring synthesis across different knowledge structures",
                  "",
                  "### Use Traditional GraphRAG when",
                  "",
                  "- **Multi-hop reasoning** is required (\"What technologies do Apple's partners' suppliers use?\")",
                  "- **Relationship discovery** is key (\"How are these companies connected?\")",
                  "- **Comprehensive exploration** needed (\"Find all related information\")",
                  "- **Complex analytical queries** (\"Analyze the supply chain impact of X on Y\")",
                  "- **Domain has rich entity relationships** (business networks, scientific literature, code repositories)",
                  "",
                  "### Use Vector RAG when",
                  "",
                  "- **Direct factual lookup** (\"What is quantum computing?\")",
                  "- **Semantic similarity** is primary concern (\"Find similar concepts\")",
                  "- **Simple Q&A** scenarios (\"When was X founded?\")",
                  "- **Limited relationship structure** in domain",
                  "- **Fast response time** is critical",
                  "",
                  "### Use Hybrid GraphRAG when",
                  "",
                  "- **Query types vary** (production systems with diverse users)",
                  "- **Maximum coverage** is needed (research and analysis scenarios)",
                  "- **Both factual accuracy and insight discovery** are important",
                  "- **You want the best of both worlds** (most real-world applications)",
                  "",
                  "### GraphRAG vs Vector RAG: Concrete Examples",
                  "",
                  "**Example Query**: \"What are the environmental impacts of technologies used by Apple's automotive partners?\"",
                  "",
                  "### Vector RAG Approach",
                  "",
                  "1. Search for \"environmental impacts technologies\"",
                  "2. Search for \"Apple automotive partners\"",
                  "3. Try to connect results manually",
                  "4. **Result**: Finds documents about each topic separately, but struggles to connect them",
                  "",
                  "### GraphRAG Approach",
                  "",
                  "1. Find Apple entity in knowledge graph",
                  "2. Traverse: Apple \u2192 partners_with \u2192 [Automotive Companies]",
                  "3. Traverse: [Automotive Companies] \u2192 uses_technology \u2192 [Technologies]",
                  "4. Traverse: [Technologies] \u2192 has_environmental_impact \u2192 [Impacts]",
                  "5. **Result**: Discovers specific impact chains that no single document contains",
                  "",
                  "### Hybrid Approach",
                  "",
                  "1. Uses vector search to understand \"environmental impacts\" semantically",
                  "2. Uses graph traversal to follow the relationship chain",
                  "3. Combines both to provide comprehensive, accurate answers",
                  "4. **Result**: Best coverage with highest accuracy",
                  "",
                  "---",
                  "",
                  "## \ud83d\udcdd Multiple Choice Test - Session 6",
                  "",
                  "Test your understanding of graph-based RAG systems and GraphRAG implementations.",
                  "",
                  "**Question 1:** What is the primary advantage of GraphRAG over traditional vector-based RAG?  ",
                  "A) Faster query processing  ",
                  "B) Lower computational requirements  ",
                  "C) Multi-hop reasoning through explicit relationship modeling  ",
                  "D) Simpler system architecture  ",
                  "",
                  "**Question 2:** In knowledge graph construction, what is the purpose of entity standardization?  ",
                  "A) To reduce memory usage  ",
                  "B) To merge different mentions of the same entity (e.g., \"Apple Inc.\" and \"Apple\")  ",
                  "C) To improve query speed  ",
                  "D) To compress graph storage  ",
                  "",
                  "**Question 3:** Which graph traversal algorithm is most suitable for finding related entities within a limited number of hops?  ",
                  "A) Depth-First Search (DFS)  ",
                  "B) Breadth-First Search (BFS)  ",
                  "C) Dijkstra's algorithm  ",
                  "D) A* search  ",
                  "",
                  "**Question 4:** In Code GraphRAG, what information is typically extracted from Abstract Syntax Trees (ASTs)?  ",
                  "A) Only function definitions  ",
                  "B) Function calls, imports, class hierarchies, and variable dependencies  ",
                  "C) Only variable names  ",
                  "D) Just file names and sizes  ",
                  "",
                  "**Question 5:** What is the key benefit of hybrid graph-vector search?  ",
                  "A) Reduced computational cost  ",
                  "B) Combining structural relationships with semantic similarity  ",
                  "C) Simpler implementation  ",
                  "D) Faster indexing  ",
                  "",
                  "**Question 6:** When should you choose Neo4j over a simple graph data structure for GraphRAG?  ",
                  "A) Always, regardless of scale  ",
                  "B) When you need persistent storage and complex queries at scale  ",
                  "C) Only for small datasets  ",
                  "D) Never, simple structures are always better  ",
                  "",
                  "**Question 7:** What is the primary challenge in multi-hop graph traversal for RAG?  ",
                  "A) Memory limitations  ",
                  "B) Balancing comprehensiveness with relevance and avoiding information explosion  ",
                  "C) Slow database queries  ",
                  "D) Complex code implementation  ",
                  "",
                  "**Question 8:** In production GraphRAG systems, what is the most important consideration for incremental updates?  ",
                  "A) Minimizing downtime while maintaining graph consistency  ",
                  "B) Reducing storage costs  ",
                  "C) Maximizing query speed  ",
                  "D) Simplifying the codebase  ",
                  "",
                  "[**\ud83d\uddc2\ufe0f View Test Solutions \u2192**](Session6_Test_Solutions.md)",
                  "",
                  "---",
                  "",
                  "## \ud83e\udded Navigation",
                  "",
                  "**Previous:** [Session 5 - RAG Evaluation & Quality Assessment](Session5_RAG_Evaluation_Quality_Assessment.md)",
                  "",
                  "## Optional Deep Dive Modules",
                  "",
                  "- \ud83d\udd2c **[Module A: Advanced Graph Algorithms](Session6_ModuleA_Advanced_Algorithms.md)** - Complex graph traversal and reasoning patterns",
                  "- \ud83c\udfed **[Module B: Production GraphRAG](Session6_ModuleB_Production_Systems.md)** - Enterprise graph database deployment",
                  "",
                  "**Next:** [Session 7 - Agentic RAG Systems \u2192](Session7_Agentic_RAG_Systems.md)",
                  ""
                ],
                "line_count": 174
              }
            ],
            "needs_refactoring": true
          }
        ]
      },
      "script": "/Users/q284340/Agentic/nano-degree/scripts/detect-large-code-blocks.py"
    },
    "check_markdown_formatting": {
      "success": true,
      "data": {
        "summary": {
          "total_files": 1,
          "files_with_issues": 0,
          "total_issues": 0
        },
        "files": [
          {
            "file": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session6_Graph_Based_RAG.md",
            "total_issues": 0,
            "issues": [],
            "needs_fixing": false
          }
        ]
      },
      "script": "/Users/q284340/Agentic/nano-degree/scripts/check-markdown-formatting.py"
    },
    "detect_insufficient_explanations": {
      "success": true,
      "data": {
        "summary": {
          "total_files": 1,
          "files_needing_improvement": 1,
          "total_issues": 29,
          "average_quality_score": 73.63636363636363
        },
        "files": [
          {
            "total_code_blocks": 110,
            "total_issues": 29,
            "issues": [
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 94,
                "code_block_size": 15,
                "word_count": 5,
                "message": "Code block (15 lines) at line 75 has insufficient explanation (5 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 94,
                "code_block_size": 15,
                "message": "Code block at line 75 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 133,
                "code_block_size": 8,
                "word_count": 5,
                "message": "Code block (8 lines) at line 122 has insufficient explanation (5 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 133,
                "code_block_size": 8,
                "message": "Code block at line 122 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 181,
                "code_block_size": 9,
                "word_count": 8,
                "message": "Code block (9 lines) at line 169 has insufficient explanation (8 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 181,
                "code_block_size": 9,
                "message": "Code block at line 169 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 289,
                "code_block_size": 13,
                "word_count": 7,
                "message": "Code block (13 lines) at line 272 has insufficient explanation (7 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 289,
                "code_block_size": 13,
                "message": "Code block at line 272 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 403,
                "code_block_size": 12,
                "word_count": 7,
                "message": "Code block (12 lines) at line 387 has insufficient explanation (7 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 675,
                "code_block_size": 14,
                "word_count": 6,
                "message": "Code block (14 lines) at line 659 has insufficient explanation (6 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 675,
                "code_block_size": 14,
                "message": "Code block at line 659 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 775,
                "code_block_size": 76,
                "word_count": 8,
                "message": "Code block (76 lines) at line 678 has insufficient explanation (8 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 775,
                "code_block_size": 76,
                "message": "Code block at line 678 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 888,
                "code_block_size": 84,
                "word_count": 9,
                "message": "Code block (84 lines) at line 778 has insufficient explanation (9 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 888,
                "code_block_size": 84,
                "message": "Code block at line 778 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 961,
                "code_block_size": 54,
                "word_count": 4,
                "message": "Code block (54 lines) at line 891 has insufficient explanation (4 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 1313,
                "code_block_size": 6,
                "word_count": 6,
                "message": "Code block (6 lines) at line 1298 has insufficient explanation (6 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 1598,
                "code_block_size": 8,
                "word_count": 16,
                "message": "Code block (8 lines) at line 1579 has insufficient explanation (16 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 1598,
                "code_block_size": 8,
                "message": "Code block at line 1579 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 1636,
                "code_block_size": 17,
                "word_count": 6,
                "message": "Code block (17 lines) at line 1599 has insufficient explanation (6 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 1636,
                "code_block_size": 17,
                "message": "Code block at line 1599 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 2023,
                "code_block_size": 20,
                "word_count": 5,
                "message": "Code block (20 lines) at line 1980 has insufficient explanation (5 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 2023,
                "code_block_size": 20,
                "message": "Code block at line 1980 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 2646,
                "code_block_size": 11,
                "word_count": 4,
                "message": "Code block (11 lines) at line 2625 has insufficient explanation (4 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 2646,
                "code_block_size": 11,
                "message": "Code block at line 2625 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "no_explanation",
                "severity": "high",
                "line": 2892,
                "code_block_size": 126,
                "message": "Code block (126 lines) at line 2716 has no explanation",
                "suggestion": "Add detailed explanation covering what the code does, why it matters, and key insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 2892,
                "code_block_size": 126,
                "message": "Code block at line 2716 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "consecutive_code_blocks",
                "severity": "medium",
                "line": 1597,
                "message": "Consecutive code blocks at lines 1579 and 1599 without intermediate explanation",
                "suggestion": "Add transitional explanation between code blocks to maintain narrative flow"
              },
              {
                "type": "consecutive_code_blocks",
                "severity": "medium",
                "line": 2148,
                "message": "Consecutive code blocks at lines 2144 and 2151 without intermediate explanation",
                "suggestion": "Add transitional explanation between code blocks to maintain narrative flow"
              }
            ],
            "needs_improvement": true,
            "quality_score": 73.63636363636363,
            "file": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session6_Graph_Based_RAG.md"
          }
        ]
      },
      "script": "/Users/q284340/Agentic/nano-degree/scripts/detect-insufficient-explanations.py"
    }
  },
  "priority_files": [
    {
      "file": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session6_Graph_Based_RAG.md",
      "file_name": "Session6_Graph_Based_RAG.md",
      "priority_score": 85.27272727272728,
      "issues": [
        "8 large code blocks",
        "73.6% explanation quality"
      ]
    }
  ]
}