{
  "analysis_timestamp": "1756565301.7451673",
  "target": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session2_ModuleA_Document_Analytics.md",
  "overall_metrics": {
    "total_files": 1,
    "code_block_score": 100.0,
    "formatting_score": 80.0,
    "explanation_score": 77.77777777777779,
    "overall_score": 87.11111111111111,
    "critical_issues": 0,
    "total_issues": 3
  },
  "detailed_results": {
    "detect_large_code_blocks": {
      "success": true,
      "data": {
        "summary": {
          "total_files": 1,
          "files_needing_refactoring": 0,
          "total_large_blocks": 0
        },
        "files": [
          {
            "file": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session2_ModuleA_Document_Analytics.md",
            "total_code_blocks": 9,
            "large_blocks_count": 0,
            "code_blocks": [
              {
                "start_line": 20,
                "end_line": 26,
                "language": "python",
                "content": [
                  "# Core imports for enterprise document analysis",
                  "import numpy as np",
                  "from typing import Dict, List, Any, Tuple",
                  "from dataclasses import dataclass",
                  "import re"
                ],
                "line_count": 5
              },
              {
                "start_line": 30,
                "end_line": 40,
                "language": "python",
                "content": [
                  "@dataclass",
                  "class DocumentComplexityScore:",
                  "    \"\"\"Comprehensive document complexity assessment.\"\"\"",
                  "    structural_complexity: float",
                  "    semantic_complexity: float",
                  "    processing_difficulty: float",
                  "    overall_score: float",
                  "    recommendations: List[str]",
                  "    confidence_level: float"
                ],
                "line_count": 9
              },
              {
                "start_line": 44,
                "end_line": 56,
                "language": "python",
                "content": [
                  "class EnterpriseComplexityAnalyzer:",
                  "    \"\"\"Production-grade document complexity analysis.\"\"\"",
                  "    ",
                  "    def __init__(self):",
                  "        # Domain-specific complexity patterns",
                  "        self.complexity_patterns = {",
                  "            \"legal\": {",
                  "                \"high_complexity\": [r\"whereas\", r\"heretofore\", r\"\u00a7\\s*\\d+\", r\"article\\s+[IVX]+\"],",
                  "                \"structural_markers\": [r\"section\\s+\\d+\", r\"clause\\s+\\d+\", r\"paragraph\\s+\\([a-z]\\)\"],",
                  "                \"citation_density\": [r\"\\d+\\s+U\\.S\\.C\\.\", r\"\\d+\\s+F\\.\\d+d\", r\"Fed\\.\\s*R\\.\\s*Civ\\.\\s*P\\.\"]",
                  "            },"
                ],
                "line_count": 11
              },
              {
                "start_line": 60,
                "end_line": 72,
                "language": "python",
                "content": [
                  "            \"technical\": {",
                  "                \"code_blocks\": [r\"```[\\s\\S]*?```\", r\"(?:^|\\n)    .*\", r\"`[^`]+`\"],",
                  "                \"api_references\": [r\"[A-Z][a-zA-Z]+\\.[a-zA-Z]+\\(\\)\", r\"HTTP[S]?://\"],",
                  "                \"version_patterns\": [r\"v?\\d+\\.\\d+\\.\\d+\", r\"version\\s+\\d+\\.\\d+\"]",
                  "            },",
                  "            \"medical\": {",
                  "                \"dosage_patterns\": [r\"\\d+\\s*mg\", r\"\\d+\\s*ml\", r\"\\d+\\s*units?\"],",
                  "                \"medical_terms\": [r\"[A-Z][a-z]+itis\", r\"[A-Z][a-z]+osis\", r\"[A-Z][a-z]+emia\"],",
                  "                \"procedure_codes\": [r\"CPT\\s*\\d+\", r\"ICD-\\d+\"]",
                  "            }",
                  "        }"
                ],
                "line_count": 11
              },
              {
                "start_line": 75,
                "end_line": 79,
                "language": "",
                "content": [
                  "",
                  "The complexity patterns dictionary is the heart of our domain-aware analysis. Each domain has specific linguistic and structural markers that indicate complexity. Legal documents use archaic terminology like \"whereas\" and formal section references. Technical documents contain code blocks and API references. Medical documents include dosage information and standardized terminology patterns. These regex patterns allow us to automatically detect domain-specific complexity indicators that human reviewers would recognize instinctively.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 88,
                "end_line": 92,
                "language": "",
                "content": [
                  "",
                  "These weights reflect the relative processing difficulty of different structural elements. Tables are weighted highest (3.0) because they require specialized parsing and maintain relationships between cells. Code blocks (2.5) need syntax preservation and special handling. The weights are calibrated based on real-world processing experience - a document with many tables will require more sophisticated chunking than one with simple lists.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 279,
                "end_line": 287,
                "language": "",
                "content": [
                  "",
                  "Confidence calculation helps you trust the complexity assessment. Size confidence increases with document length up to 1000 characters - very short documents might not contain enough patterns for reliable analysis. Pattern match confidence increases as we find more domain-specific indicators, capping at 5 matches to prevent over-confidence from pattern-heavy documents. Together, these measures give you a 0-1 confidence score indicating how reliable the complexity assessment is.",
                  "",
                  "### Enterprise Implementation Example",
                  "",
                  "The complexity analysis becomes powerful when integrated into automated processing pipelines. Instead of applying one-size-fits-all processing, you can dynamically adapt your approach based on document characteristics:",
                  ""
                ],
                "line_count": 7
              },
              {
                "start_line": 325,
                "end_line": 339,
                "language": "",
                "content": [
                  "",
                  "The return structure provides complete transparency into the adaptive processing decision. You get the full complexity analysis, the selected strategy with configuration parameters, specific processing recommendations, and confidence levels. This enables both automated processing and human oversight - operators can see why a particular strategy was chosen and adjust thresholds based on domain-specific requirements.",
                  "",
                  "---",
                  "",
                  "## Advanced Pattern 2: Multi-Domain Quality Assessment - Beyond Basic Metrics",
                  "",
                  "### Enterprise Quality Framework",
                  "",
                  "Complexity scoring tells you how hard a document is to process, but quality assessment tells you how well you actually processed it. This distinction is crucial \u2013 a document might be simple to process but result in poor-quality chunks due to inappropriate boundary decisions or missing context.",
                  "",
                  "Enterprise quality assessment goes beyond basic metrics like chunk size consistency to evaluate semantic coherence, information completeness, and domain-specific quality criteria:",
                  ""
                ],
                "line_count": 13
              },
              {
                "start_line": 505,
                "end_line": 515,
                "language": "",
                "content": [
                  "",
                  "Metadata richness evaluates whether the processing pipeline successfully extracted expected metadata fields like topics, entities, keywords, and difficulty levels. Rich metadata enables better filtering, routing, and context understanding during retrieval. The richness score averages metadata completeness across all chunks, helping identify documents where metadata extraction failed or performed poorly.",
                  "",
                  "---",
                  "",
                  "## Advanced Pattern 3: Real-Time Quality Monitoring - Continuous Intelligence",
                  "",
                  "Quality assessment is most valuable when it's continuous, not just a one-time check. In production, you need real-time monitoring that can detect when document processing quality degrades and automatically trigger corrective actions. This pattern transforms quality assessment from batch analysis into operational intelligence:",
                  ""
                ],
                "line_count": 9
              }
            ],
            "large_blocks": [],
            "needs_refactoring": false
          }
        ]
      },
      "script": "/Users/q284340/Agentic/nano-degree/scripts/detect-large-code-blocks.py"
    },
    "check_markdown_formatting": {
      "success": true,
      "data": {
        "summary": {
          "total_files": 1,
          "files_with_issues": 1,
          "total_issues": 1
        },
        "files": [
          {
            "file": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session2_ModuleA_Document_Analytics.md",
            "total_issues": 1,
            "issues": [
              {
                "type": "unclosed_code_block",
                "line": 607,
                "message": "Code block starting at line 607 is never closed",
                "suggestion": "Add closing ``` marker"
              }
            ],
            "needs_fixing": true
          }
        ]
      },
      "script": "/Users/q284340/Agentic/nano-degree/scripts/check-markdown-formatting.py"
    },
    "detect_insufficient_explanations": {
      "success": true,
      "data": {
        "summary": {
          "total_files": 1,
          "files_needing_improvement": 1,
          "total_issues": 2,
          "average_quality_score": 77.77777777777779
        },
        "files": [
          {
            "total_code_blocks": 9,
            "total_issues": 2,
            "issues": [
              {
                "type": "generic_explanation",
                "severity": "medium",
                "line": 288,
                "code_block_size": 3,
                "message": "Code block at line 279 has generic/lazy explanation",
                "suggestion": "Replace generic phrases with specific, educational explanations"
              },
              {
                "type": "consecutive_code_blocks",
                "severity": "medium",
                "line": 72,
                "message": "Consecutive code blocks at lines 60 and 75 without intermediate explanation",
                "suggestion": "Add transitional explanation between code blocks to maintain narrative flow"
              }
            ],
            "needs_improvement": true,
            "quality_score": 77.77777777777779,
            "file": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session2_ModuleA_Document_Analytics.md"
          }
        ]
      },
      "script": "/Users/q284340/Agentic/nano-degree/scripts/detect-insufficient-explanations.py"
    }
  },
  "priority_files": [
    {
      "file": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session2_ModuleA_Document_Analytics.md",
      "file_name": "Session2_ModuleA_Document_Analytics.md",
      "priority_score": 4.444444444444443,
      "issues": [
        "77.8% explanation quality"
      ]
    }
  ]
}