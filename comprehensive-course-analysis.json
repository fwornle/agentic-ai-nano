{
  "analysis_timestamp": "1756565301.7451673",
  "target": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session6_Graph_Based_RAG.md",
  "overall_metrics": {
    "total_files": 1,
    "code_block_score": 70.0,
    "formatting_score": 80.0,
    "explanation_score": 82.82208588957054,
    "overall_score": 77.12883435582822,
    "critical_issues": 6,
    "total_issues": 29
  },
  "detailed_results": {
    "detect_large_code_blocks": {
      "success": true,
      "data": {
        "summary": {
          "total_files": 1,
          "files_needing_refactoring": 1,
          "total_large_blocks": 6
        },
        "files": [
          {
            "file": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session6_Graph_Based_RAG.md",
            "total_code_blocks": 163,
            "large_blocks_count": 6,
            "code_blocks": [
              {
                "start_line": 30,
                "end_line": 33,
                "language": "",
                "content": [
                  "Traditional RAG: Document \u2192 Chunks \u2192 Uniform Embeddings \u2192 Similarity Search",
                  "NodeRAG: Document \u2192 Specialized Nodes \u2192 Heterogeneous Graph \u2192 Reasoning Pathways"
                ],
                "line_count": 2
              },
              {
                "start_line": 75,
                "end_line": 93,
                "language": "python",
                "content": [
                  "def noderag_decomposition(document):",
                  "    \"\"\"NodeRAG Stage 1: Multi-granularity knowledge decomposition\"\"\"",
                  "",
                  "    # Parallel specialized extraction",
                  "    semantic_units = extract_semantic_concepts(document)     # Abstract themes and topics",
                  "    entities = extract_canonical_entities(document)          # People, orgs, locations with metadata",
                  "    relationships = extract_typed_relationships(document)     # Explicit connections with evidence",
                  "    attributes = extract_entity_properties(document)         # Quantitative and qualitative properties",
                  "    document_nodes = create_document_segments(document)      # Contextual source information",
                  "",
                  "    return {",
                  "        'semantic_units': semantic_units,",
                  "        'entities': entities,",
                  "        'relationships': relationships,",
                  "        'attributes': attributes,",
                  "        'document_nodes': document_nodes",
                  "    }"
                ],
                "line_count": 17
              },
              {
                "start_line": 99,
                "end_line": 108,
                "language": "python",
                "content": [
                  "def noderag_augmentation(decomposition_result):",
                  "    \"\"\"NodeRAG Stage 2: Heterogeneous graph construction\"\"\"",
                  "    ",
                  "    # Create typed connections between specialized nodes",
                  "    semantic_entity_links = link_concepts_to_entities(",
                  "        decomposition_result['semantic_units'],",
                  "        decomposition_result['entities']",
                  "    )"
                ],
                "line_count": 8
              },
              {
                "start_line": 112,
                "end_line": 118,
                "language": "python",
                "content": [
                  "    # Build HNSW similarity edges within the graph structure",
                  "    hnsw_similarity_edges = build_hnsw_graph_edges(",
                  "        all_nodes=decomposition_result,",
                  "        similarity_threshold=0.75",
                  "    )"
                ],
                "line_count": 5
              },
              {
                "start_line": 122,
                "end_line": 132,
                "language": "python",
                "content": [
                  "    # Cross-reference integration across node types",
                  "    cross_references = integrate_cross_type_references(decomposition_result)",
                  "",
                  "    return build_heterogeneous_graph(",
                  "        nodes=decomposition_result,",
                  "        typed_connections=semantic_entity_links,",
                  "        similarity_edges=hnsw_similarity_edges,",
                  "        cross_references=cross_references",
                  "    )"
                ],
                "line_count": 9
              },
              {
                "start_line": 138,
                "end_line": 154,
                "language": "python",
                "content": [
                  "def noderag_enrichment(heterogeneous_graph):",
                  "    \"\"\"NodeRAG Stage 3: Reasoning pathway construction\"\"\"",
                  "",
                  "    # Apply Personalized PageRank for semantic importance",
                  "    pagerank_scores = personalized_pagerank(",
                  "        graph=heterogeneous_graph,",
                  "        node_type_weights={",
                  "            'semantic_unit': 0.25,",
                  "            'entity': 0.30,",
                  "            'relationship': 0.20,",
                  "            'attribute': 0.10,",
                  "            'document': 0.10,",
                  "            'summary': 0.05",
                  "        }",
                  "    )"
                ],
                "line_count": 15
              },
              {
                "start_line": 158,
                "end_line": 165,
                "language": "python",
                "content": [
                  "    # Construct logical reasoning pathways",
                  "    reasoning_pathways = build_reasoning_pathways(",
                  "        graph=heterogeneous_graph,",
                  "        pagerank_scores=pagerank_scores,",
                  "        max_pathway_length=5",
                  "    )"
                ],
                "line_count": 6
              },
              {
                "start_line": 169,
                "end_line": 180,
                "language": "python",
                "content": [
                  "    # Optimize graph structure for reasoning performance",
                  "    optimized_graph = optimize_for_reasoning(",
                  "        heterogeneous_graph, reasoning_pathways",
                  "    )",
                  "",
                  "    return {",
                  "        'enriched_graph': optimized_graph,",
                  "        'reasoning_pathways': reasoning_pathways,",
                  "        'pagerank_scores': pagerank_scores",
                  "    }"
                ],
                "line_count": 10
              },
              {
                "start_line": 188,
                "end_line": 207,
                "language": "python",
                "content": [
                  "class NodeRAGPageRank:",
                  "    \"\"\"Personalized PageRank optimized for heterogeneous NodeRAG graphs\"\"\"",
                  "",
                  "    def compute_semantic_pathways(self, query_context, heterogeneous_graph):",
                  "        \"\"\"Compute query-aware semantic pathways using personalized PageRank\"\"\"",
                  "        ",
                  "        # Create personalization vector based on query relevance and node types",
                  "        personalization_vector = self.create_query_personalization(",
                  "            query_context=query_context,",
                  "            graph=heterogeneous_graph,",
                  "            node_type_weights={",
                  "                'semantic_unit': 0.3,  # High weight for concepts relevant to query",
                  "                'entity': 0.25,        # Moderate weight for concrete entities",
                  "                'relationship': 0.2,   # Important for connection discovery",
                  "                'attribute': 0.15,     # Properties provide specificity",
                  "                'summary': 0.1         # Synthesized insights",
                  "            }",
                  "        )"
                ],
                "line_count": 18
              },
              {
                "start_line": 211,
                "end_line": 220,
                "language": "python",
                "content": [
                  "        # Compute personalized PageRank with query bias",
                  "        pagerank_scores = nx.pagerank(",
                  "            heterogeneous_graph,",
                  "            alpha=0.85,  # Standard damping factor",
                  "            personalization=personalization_vector,",
                  "            max_iter=100,",
                  "            tol=1e-6",
                  "        )"
                ],
                "line_count": 8
              },
              {
                "start_line": 224,
                "end_line": 234,
                "language": "python",
                "content": [
                  "        # Extract top semantic pathways",
                  "        semantic_pathways = self.extract_top_pathways(",
                  "            graph=heterogeneous_graph,",
                  "            pagerank_scores=pagerank_scores,",
                  "            query_context=query_context,",
                  "            max_pathways=10",
                  "        )",
                  "        ",
                  "        return semantic_pathways"
                ],
                "line_count": 9
              },
              {
                "start_line": 240,
                "end_line": 250,
                "language": "python",
                "content": [
                  "    def extract_top_pathways(self, graph, pagerank_scores, query_context, max_pathways):",
                  "        \"\"\"Extract the most relevant semantic pathways for the query\"\"\"",
                  "        ",
                  "        # Find high-scoring nodes as pathway anchors",
                  "        top_nodes = sorted(",
                  "            pagerank_scores.items(),",
                  "            key=lambda x: x[1],",
                  "            reverse=True",
                  "        )[:50]"
                ],
                "line_count": 9
              },
              {
                "start_line": 254,
                "end_line": 268,
                "language": "python",
                "content": [
                  "        pathways = []",
                  "        for start_node, score in top_nodes:",
                  "            if len(pathways) >= max_pathways:",
                  "                break",
                  "",
                  "            # Use BFS to find semantic pathways from this anchor",
                  "            pathway = self.find_semantic_pathway(",
                  "                graph=graph,",
                  "                start_node=start_node,",
                  "                query_context=query_context,",
                  "                max_depth=4,",
                  "                pagerank_scores=pagerank_scores",
                  "            )"
                ],
                "line_count": 13
              },
              {
                "start_line": 272,
                "end_line": 288,
                "language": "python",
                "content": [
                  "            if pathway and len(pathway) > 1:",
                  "                pathways.append({",
                  "                    'pathway': pathway,",
                  "                    'anchor_score': score,",
                  "                    'pathway_coherence': self.calculate_pathway_coherence(pathway),",
                  "                    'query_relevance': self.calculate_query_relevance(pathway, query_context)",
                  "                })",
                  "",
                  "        # Rank pathways by combined score",
                  "        pathways.sort(",
                  "            key=lambda p: (p['pathway_coherence'] * p['query_relevance'] * p['anchor_score']),",
                  "            reverse=True",
                  "        )",
                  "",
                  "        return pathways[:max_pathways]"
                ],
                "line_count": 15
              },
              {
                "start_line": 294,
                "end_line": 304,
                "language": "python",
                "content": [
                  "class NodeRAGHNSW:",
                  "    \"\"\"HNSW similarity edges integrated into NodeRAG heterogeneous graphs\"\"\"",
                  "",
                  "    def build_hnsw_graph_integration(self, heterogeneous_graph, embedding_model):",
                  "        \"\"\"Build HNSW similarity edges within the existing graph structure\"\"\"",
                  "",
                  "        # Initialize embedding storage for all graph nodes",
                  "        node_embeddings = {}",
                  "        node_types = {}"
                ],
                "line_count": 9
              },
              {
                "start_line": 308,
                "end_line": 313,
                "language": "python",
                "content": [
                  "        # Extract and process each node with type-aware embedding generation",
                  "        for node_id, node_data in heterogeneous_graph.nodes(data=True):",
                  "            node_type = node_data.get('node_type')",
                  "            node_content = self.get_node_content_for_embedding(node_data, node_type)"
                ],
                "line_count": 4
              },
              {
                "start_line": 317,
                "end_line": 327,
                "language": "python",
                "content": [
                  "            # Generate specialized embeddings based on node type",
                  "            embedding = self.generate_typed_embedding(",
                  "                content=node_content,",
                  "                node_type=node_type,",
                  "                embedding_model=embedding_model",
                  "            )",
                  "",
                  "            node_embeddings[node_id] = embedding",
                  "            node_types[node_id] = node_type"
                ],
                "line_count": 9
              },
              {
                "start_line": 331,
                "end_line": 342,
                "language": "python",
                "content": [
                  "        # Build HNSW index with type-aware similarity",
                  "        hnsw_index = self.build_typed_hnsw_index(",
                  "            embeddings=node_embeddings,",
                  "            node_types=node_types,",
                  "            M=16,  # Number of bi-directional links for each node",
                  "            ef_construction=200,  # Size of the dynamic candidate list",
                  "            max_m=16,",
                  "            max_m0=32,",
                  "            ml=1 / np.log(2.0)",
                  "        )"
                ],
                "line_count": 10
              },
              {
                "start_line": 346,
                "end_line": 355,
                "language": "python",
                "content": [
                  "        # Add similarity edges to the existing heterogeneous graph",
                  "        similarity_edges_added = 0",
                  "        for node_id in heterogeneous_graph.nodes():",
                  "            # Find k most similar nodes using HNSW",
                  "            similar_nodes = hnsw_index.knn_query(",
                  "                node_embeddings[node_id],",
                  "                k=10  # Top-10 most similar nodes",
                  "            )[1][0]  # Get node indices"
                ],
                "line_count": 8
              },
              {
                "start_line": 359,
                "end_line": 369,
                "language": "python",
                "content": [
                  "            node_list = list(node_embeddings.keys())",
                  "            for similar_idx in similar_nodes[1:]:  # Skip self",
                  "                similar_node_id = node_list[similar_idx]",
                  "",
                  "                # Calculate similarity score",
                  "                similarity = cosine_similarity(",
                  "                    [node_embeddings[node_id]],",
                  "                    [node_embeddings[similar_node_id]]",
                  "                )[0][0]"
                ],
                "line_count": 9
              },
              {
                "start_line": 373,
                "end_line": 390,
                "language": "python",
                "content": [
                  "                # Add similarity edge if above threshold and type-compatible",
                  "                if similarity > 0.7 and self.are_types_compatible(",
                  "                    node_types[node_id],",
                  "                    node_types[similar_node_id]",
                  "                ):",
                  "                    heterogeneous_graph.add_edge(",
                  "                        node_id,",
                  "                        similar_node_id,",
                  "                        edge_type='similarity',",
                  "                        similarity_score=float(similarity),",
                  "                        hnsw_based=True",
                  "                    )",
                  "                    similarity_edges_added += 1",
                  "",
                  "        print(f\"Added {similarity_edges_added} HNSW similarity edges to heterogeneous graph\")",
                  "        return heterogeneous_graph"
                ],
                "line_count": 16
              },
              {
                "start_line": 396,
                "end_line": 411,
                "language": "python",
                "content": [
                  "    def are_types_compatible(self, type1, type2):",
                  "        \"\"\"Determine if two node types should have similarity connections\"\"\"",
                  "",
                  "        # Define type compatibility matrix",
                  "        compatibility_matrix = {",
                  "            'semantic_unit': ['semantic_unit', 'entity', 'summary'],",
                  "            'entity': ['entity', 'semantic_unit', 'attribute'],",
                  "            'relationship': ['relationship', 'entity'],",
                  "            'attribute': ['attribute', 'entity'],",
                  "            'document': ['document', 'summary'],",
                  "            'summary': ['summary', 'semantic_unit', 'document']",
                  "        }",
                  "",
                  "        return type2 in compatibility_matrix.get(type1, [])"
                ],
                "line_count": 14
              },
              {
                "start_line": 443,
                "end_line": 457,
                "language": "python",
                "content": [
                  "",
                  "# NodeRAG: Heterogeneous Graph Architecture for Advanced Knowledge Representation",
                  "",
                  "import spacy",
                  "from typing import List, Dict, Any, Tuple, Set, Union",
                  "import networkx as nx",
                  "from neo4j import GraphDatabase",
                  "import json",
                  "import re",
                  "from enum import Enum",
                  "from dataclasses import dataclass",
                  "import numpy as np",
                  "from sklearn.metrics.pairwise import cosine_similarity"
                ],
                "line_count": 13
              },
              {
                "start_line": 461,
                "end_line": 469,
                "language": "python",
                "content": [
                  "class NodeType(Enum):",
                  "    \"\"\"Specialized node types in heterogeneous NodeRAG architecture.\"\"\"",
                  "    ENTITY = \"entity\"",
                  "    CONCEPT = \"concept\"",
                  "    DOCUMENT = \"document\"",
                  "    RELATIONSHIP = \"relationship\"",
                  "    CLUSTER = \"cluster\""
                ],
                "line_count": 7
              },
              {
                "start_line": 473,
                "end_line": 484,
                "language": "python",
                "content": [
                  "@dataclass",
                  "class NodeRAGNode:",
                  "    \"\"\"Structured node representation for heterogeneous graph.\"\"\"",
                  "    node_id: str",
                  "    node_type: NodeType",
                  "    content: str",
                  "    metadata: Dict[str, Any]",
                  "    embeddings: Dict[str, np.ndarray]",
                  "    connections: List[str]",
                  "    confidence: float"
                ],
                "line_count": 10
              },
              {
                "start_line": 490,
                "end_line": 502,
                "language": "python",
                "content": [
                  "class NodeRAGExtractor:",
                  "    \"\"\"NodeRAG heterogeneous graph construction with three-stage processing.",
                  "",
                  "    This extractor implements the breakthrough NodeRAG architecture that addresses",
                  "    traditional GraphRAG limitations through specialized node types and advanced",
                  "    graph reasoning capabilities.",
                  "    \"\"\"",
                  "",
                  "    def __init__(self, llm_model, spacy_model: str = \"en_core_web_lg\"):",
                  "        self.llm_model = llm_model",
                  "        self.nlp = spacy.load(spacy_model)"
                ],
                "line_count": 11
              },
              {
                "start_line": 508,
                "end_line": 517,
                "language": "python",
                "content": [
                  "        # NodeRAG specialized processors for different node types",
                  "        self.node_processors = {",
                  "            NodeType.ENTITY: self._extract_entity_nodes,",
                  "            NodeType.CONCEPT: self._extract_concept_nodes,",
                  "            NodeType.DOCUMENT: self._extract_document_nodes,",
                  "            NodeType.RELATIONSHIP: self._extract_relationship_nodes,",
                  "            NodeType.CLUSTER: self._extract_cluster_nodes",
                  "        }"
                ],
                "line_count": 8
              },
              {
                "start_line": 523,
                "end_line": 530,
                "language": "python",
                "content": [
                  "        # Three-stage processing pipeline",
                  "        self.processing_stages = {",
                  "            'decomposition': self._decomposition_stage,",
                  "            'augmentation': self._augmentation_stage,",
                  "            'enrichment': self._enrichment_stage",
                  "        }"
                ],
                "line_count": 6
              },
              {
                "start_line": 534,
                "end_line": 544,
                "language": "python",
                "content": [
                  "        # Advanced graph components",
                  "        self.heterogeneous_graph = nx.MultiDiGraph()  # Supports multiple node types",
                  "        self.node_registry = {}  # Central registry of all nodes",
                  "        self.pagerank_processor = PersonalizedPageRankProcessor()",
                  "        self.hnsw_similarity = HNSWSimilarityProcessor()",
                  "",
                  "        # Reasoning integration components",
                  "        self.reasoning_pathways = {}  # Store logical reasoning pathways",
                  "        self.coherence_validator = CoherenceValidator()"
                ],
                "line_count": 9
              },
              {
                "start_line": 550,
                "end_line": 562,
                "language": "python",
                "content": [
                  "    def extract_noderag_graph(self, documents: List[str],",
                  "                               extraction_config: Dict = None) -> Dict[str, Any]:",
                  "        \"\"\"Extract NodeRAG heterogeneous graph using three-stage processing.",
                  "",
                  "        The NodeRAG extraction process follows the breakthrough three-stage approach:",
                  "",
                  "        **Stage 1 - Decomposition:**",
                  "        1. Multi-granularity analysis to extract different knowledge structures",
                  "        2. Specialized node creation for entities, concepts, documents, and relationships",
                  "        3. Hierarchical structuring at multiple abstraction levels",
                  "        \"\"\""
                ],
                "line_count": 11
              },
              {
                "start_line": 566,
                "end_line": 575,
                "language": "python",
                "content": [
                  "        config = extraction_config or {",
                  "            'node_types': ['entity', 'concept', 'document', 'relationship'],  # Heterogeneous node types",
                  "            'enable_pagerank': True,                     # Personalized PageRank traversal",
                  "            'enable_hnsw_similarity': True,              # High-performance similarity edges",
                  "            'reasoning_integration': True,               # Enable reasoning pathway construction",
                  "            'confidence_threshold': 0.75,                # Higher threshold for NodeRAG quality",
                  "            'max_pathway_depth': 5                       # Maximum reasoning pathway depth",
                  "        }"
                ],
                "line_count": 8
              },
              {
                "start_line": 579,
                "end_line": 586,
                "language": "python",
                "content": [
                  "        print(f\"Extracting NodeRAG heterogeneous graph from {len(documents)} documents...\")",
                  "        print(f\"Node types: {config['node_types']}, Reasoning integration: {config['reasoning_integration']}\")",
                  "",
                  "        # NodeRAG three-stage processing pipeline",
                  "        print(\"\\n=== STAGE 1: DECOMPOSITION ===\")",
                  "        decomposition_result = self.processing_stages['decomposition'](documents, config)"
                ],
                "line_count": 6
              },
              {
                "start_line": 590,
                "end_line": 593,
                "language": "python",
                "content": [
                  "        print(\"=== STAGE 2: AUGMENTATION ===\")",
                  "        augmentation_result = self.processing_stages['augmentation'](decomposition_result, config)"
                ],
                "line_count": 2
              },
              {
                "start_line": 597,
                "end_line": 601,
                "language": "python",
                "content": [
                  "        print(\"=== STAGE 3: ENRICHMENT ===\")",
                  "        enrichment_result = self.processing_stages['enrichment'](augmentation_result, config)",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 605,
                "end_line": 609,
                "language": "python",
                "content": [
                  "        # Build heterogeneous graph structure",
                  "        print(\"Constructing heterogeneous graph with specialized node types...\")",
                  "        self._build_heterogeneous_graph(enrichment_result)"
                ],
                "line_count": 3
              },
              {
                "start_line": 613,
                "end_line": 623,
                "language": "python",
                "content": [
                  "        # Apply Personalized PageRank for semantic traversal",
                  "        if config.get('enable_pagerank', True):",
                  "            print(\"Computing Personalized PageRank for semantic traversal...\")",
                  "            pagerank_scores = self.pagerank_processor.compute_pagerank(",
                  "                self.heterogeneous_graph, self.node_registry",
                  "            )",
                  "        else:",
                  "            pagerank_scores = {}",
                  ""
                ],
                "line_count": 9
              },
              {
                "start_line": 627,
                "end_line": 636,
                "language": "python",
                "content": [
                  "        # Build HNSW similarity edges for high-performance retrieval",
                  "        if config.get('enable_hnsw_similarity', True):",
                  "            print(\"Constructing HNSW similarity edges...\")",
                  "            similarity_edges = self.hnsw_similarity.build_similarity_graph(",
                  "                self.node_registry, self.heterogeneous_graph",
                  "            )",
                  "        else:",
                  "            similarity_edges = {}"
                ],
                "line_count": 8
              },
              {
                "start_line": 640,
                "end_line": 649,
                "language": "python",
                "content": [
                  "        # Construct reasoning pathways if enabled",
                  "        reasoning_pathways = {}",
                  "        if config.get('reasoning_integration', True):",
                  "            print(\"Building reasoning pathways for logical coherence...\")",
                  "            reasoning_pathways = self._construct_reasoning_pathways(",
                  "                enrichment_result, config",
                  "            )",
                  ""
                ],
                "line_count": 8
              },
              {
                "start_line": 653,
                "end_line": 664,
                "language": "python",
                "content": [
                  "        # Calculate comprehensive NodeRAG statistics",
                  "        noderag_stats = self._calculate_noderag_statistics()",
                  "",
                  "        return {",
                  "            'heterogeneous_nodes': self.node_registry,",
                  "            'reasoning_pathways': reasoning_pathways,",
                  "            'pagerank_scores': pagerank_scores,",
                  "            'similarity_edges': similarity_edges,",
                  "            'heterogeneous_graph': self.heterogeneous_graph,",
                  "            'noderag_stats': noderag_stats,"
                ],
                "line_count": 10
              },
              {
                "start_line": 668,
                "end_line": 683,
                "language": "python",
                "content": [
                  "            'extraction_metadata': {",
                  "                'document_count': len(documents),",
                  "                'total_nodes': len(self.node_registry),",
                  "                'node_type_distribution': self._get_node_type_distribution(),",
                  "                'reasoning_pathways_count': len(reasoning_pathways),",
                  "                'extraction_config': config,",
                  "                'processing_stages_completed': ['decomposition', 'augmentation', 'enrichment'],",
                  "                'quality_metrics': {",
                  "                    'avg_node_confidence': self._calculate_avg_node_confidence(),",
                  "                    'pathway_coherence_score': self._calculate_pathway_coherence(),",
                  "                    'graph_connectivity_score': self._calculate_connectivity_score()",
                  "                }",
                  "            }",
                  "        }"
                ],
                "line_count": 14
              },
              {
                "start_line": 687,
                "end_line": 699,
                "language": "python",
                "content": [
                  "    def _decomposition_stage(self, documents: List[str], config: Dict) -> Dict[str, Any]:",
                  "        \"\"\"Stage 1: Multi-granularity decomposition with specialized node creation.\"\"\"",
                  "",
                  "        print(\"Performing multi-granularity analysis...\")",
                  "        decomposition_results = {",
                  "            'entity_nodes': [],",
                  "            'concept_nodes': [],",
                  "            'document_nodes': [],",
                  "            'relationship_nodes': [],",
                  "            'hierarchical_structures': {}",
                  "        }"
                ],
                "line_count": 11
              },
              {
                "start_line": 703,
                "end_line": 712,
                "language": "python",
                "content": [
                  "        # Process each document with type-specific extraction",
                  "        for doc_idx, document in enumerate(documents):",
                  "            print(f\"Decomposing document {doc_idx + 1}/{len(documents)}\")",
                  "",
                  "            # Extract entity nodes with rich metadata",
                  "            if 'entity' in config['node_types']:",
                  "                entity_nodes = self._extract_entity_nodes(document, doc_idx)",
                  "                decomposition_results['entity_nodes'].extend(entity_nodes)"
                ],
                "line_count": 8
              },
              {
                "start_line": 716,
                "end_line": 726,
                "language": "python",
                "content": [
                  "            # Extract concept nodes for abstract concepts and topics",
                  "            if 'concept' in config['node_types']:",
                  "                concept_nodes = self._extract_concept_nodes(document, doc_idx)",
                  "                decomposition_results['concept_nodes'].extend(concept_nodes)",
                  "",
                  "            # Extract document nodes for text segments",
                  "            if 'document' in config['node_types']:",
                  "                document_nodes = self._extract_document_nodes(document, doc_idx)",
                  "                decomposition_results['document_nodes'].extend(document_nodes)"
                ],
                "line_count": 9
              },
              {
                "start_line": 730,
                "end_line": 735,
                "language": "python",
                "content": [
                  "            # Extract explicit relationship nodes",
                  "            if 'relationship' in config['node_types']:",
                  "                relationship_nodes = self._extract_relationship_nodes(document, doc_idx)",
                  "                decomposition_results['relationship_nodes'].extend(relationship_nodes)"
                ],
                "line_count": 4
              },
              {
                "start_line": 739,
                "end_line": 747,
                "language": "python",
                "content": [
                  "        # Build hierarchical structures at multiple abstraction levels",
                  "        decomposition_results['hierarchical_structures'] = self._build_hierarchical_structures(",
                  "            decomposition_results",
                  "        )",
                  "",
                  "        print(f\"Decomposition complete: {sum(len(nodes) for nodes in decomposition_results.values() if isinstance(nodes, list))} nodes created\")",
                  "        return decomposition_results"
                ],
                "line_count": 7
              },
              {
                "start_line": 751,
                "end_line": 759,
                "language": "python",
                "content": [
                  "    def _augmentation_stage(self, decomposition_result: Dict, config: Dict) -> Dict[str, Any]:",
                  "        \"\"\"Stage 2: Cross-reference integration and HNSW similarity edge construction.\"\"\"",
                  "",
                  "        print(\"Performing cross-reference integration...\")",
                  "",
                  "        # Cross-link related nodes across different types",
                  "        cross_references = self._build_cross_references(decomposition_result)"
                ],
                "line_count": 7
              },
              {
                "start_line": 763,
                "end_line": 770,
                "language": "python",
                "content": [
                  "        # Build HNSW similarity edges for high-performance retrieval",
                  "        if config.get('enable_hnsw_similarity', True):",
                  "            print(\"Constructing HNSW similarity edges...\")",
                  "            similarity_edges = self._build_hnsw_similarity_edges(decomposition_result)",
                  "        else:",
                  "            similarity_edges = {}"
                ],
                "line_count": 6
              },
              {
                "start_line": 774,
                "end_line": 788,
                "language": "python",
                "content": [
                  "        # Semantic enrichment with contextual metadata",
                  "        enriched_nodes = self._apply_semantic_enrichment(decomposition_result)",
                  "",
                  "        return {",
                  "            'enriched_nodes': enriched_nodes,",
                  "            'cross_references': cross_references,",
                  "            'similarity_edges': similarity_edges,",
                  "            'augmentation_metadata': {",
                  "                'cross_references_count': len(cross_references),",
                  "                'similarity_edges_count': len(similarity_edges),",
                  "                'enrichment_applied': True",
                  "            }",
                  "        }"
                ],
                "line_count": 13
              },
              {
                "start_line": 792,
                "end_line": 804,
                "language": "python",
                "content": [
                  "    def _enrichment_stage(self, augmentation_result: Dict, config: Dict) -> Dict[str, Any]:",
                  "        \"\"\"Stage 3: Personalized PageRank and reasoning pathway construction.\"\"\"",
                  "",
                  "        print(\"Constructing reasoning pathways...\")",
                  "",
                  "        # Build reasoning pathways for logically coherent contexts",
                  "        reasoning_pathways = {}",
                  "        if config.get('reasoning_integration', True):",
                  "            reasoning_pathways = self._construct_reasoning_pathways_stage3(",
                  "                augmentation_result, config",
                  "            )"
                ],
                "line_count": 11
              },
              {
                "start_line": 808,
                "end_line": 820,
                "language": "python",
                "content": [
                  "        # Apply graph-centric optimization",
                  "        optimized_structure = self._apply_graph_optimization(",
                  "            augmentation_result, reasoning_pathways",
                  "        )",
                  "",
                  "        return {",
                  "            'final_nodes': optimized_structure['nodes'],",
                  "            'reasoning_pathways': reasoning_pathways,",
                  "            'optimization_metadata': optimized_structure['metadata'],",
                  "            'enrichment_complete': True",
                  "        }"
                ],
                "line_count": 11
              },
              {
                "start_line": 822,
                "end_line": 828,
                "language": "",
                "content": [
                  "",
                  "#### Step 2: Personalized PageRank for Semantic Traversal",
                  "",
                  "Graph-centric optimization applies final structural improvements and creates the complete NodeRAG system ready for deployment. The enrichment stage produces reasoning pathways that enable sophisticated multi-hop queries while maintaining computational efficiency.",
                  ""
                ],
                "line_count": 5
              },
              {
                "start_line": 835,
                "end_line": 839,
                "language": "",
                "content": [
                  "",
                  "The PersonalizedPageRankProcessor implements semantic traversal guidance using PageRank algorithm. The damping factor of 0.85 represents the probability of following graph edges versus random jumps, creating more focused traversal patterns. The cache stores computed scores for reuse across queries.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 848,
                "end_line": 852,
                "language": "",
                "content": [
                  "",
                  "PageRank computation begins with creating a personalization vector that emphasizes semantically important nodes based on their types and connections. This guides the algorithm to prioritize entities and concepts over structural nodes during traversal.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 862,
                "end_line": 866,
                "language": "",
                "content": [
                  "",
                  "The PageRank computation uses NetworkX's implementation with carefully tuned parameters. max_iter=100 ensures convergence for most graphs, while tol=1e-6 provides sufficient precision for semantic ranking without over-computation.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 877,
                "end_line": 881,
                "language": "",
                "content": [
                  "",
                  "Score normalization by type ensures fair comparison across different node categories. This prevents highly connected structural nodes from dominating semantically important but lower-degree concept nodes.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 897,
                "end_line": 901,
                "language": "",
                "content": [
                  "",
                  "The personalization vector assigns different base weights to node types based on their semantic importance. Entities get the highest weight (0.3) as they represent concrete knowledge, followed by concepts (0.25) for abstract understanding. This type-based weighting guides PageRank to emphasize semantically rich nodes.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 916,
                "end_line": 920,
                "language": "",
                "content": [
                  "",
                  "Each node's final weight combines its base type weight with dynamic boosts based on confidence scores and connection count. High-confidence nodes and well-connected hubs receive additional weight, creating a personalization vector that reflects both semantic importance and graph centrality.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 928,
                "end_line": 932,
                "language": "",
                "content": [
                  "",
                  "Normalization creates a valid probability distribution where all weights sum to 1.0. This ensures the personalization vector meets PageRank algorithm requirements while maintaining the relative importance relationships between different node types.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 944,
                "end_line": 948,
                "language": "",
                "content": [
                  "",
                  "The semantic pathway method implements intelligent graph traversal using precomputed PageRank scores. It starts from a given node and explores the graph following high-ranking connections toward target concepts. The cache lookup avoids recomputing PageRank scores for the same graph.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 954,
                "end_line": 958,
                "language": "",
                "content": [
                  "",
                  "Traversal initialization sets up the search state with visited node tracking, pathway recording, and depth limiting. This prevents infinite loops and ensures bounded exploration while building the semantic reasoning chain.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 975,
                "end_line": 979,
                "language": "",
                "content": [
                  "",
                  "#### Step 3: HNSW Similarity Edges for High-Performance Retrieval",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 994,
                "end_line": 998,
                "language": "",
                "content": [
                  "",
                  "Entity merging is a crucial deduplication step in GraphRAG systems. It identifies and consolidates different mentions of the same entity (like \"Apple Inc.\", \"Apple\", and \"Apple Corporation\") using semantic similarity. The process uses the all-MiniLM-L6-v2 model for high-quality embeddings with reasonable computational overhead.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1006,
                "end_line": 1010,
                "language": "",
                "content": [
                  "",
                  "The similarity computation creates embeddings for all entity names, then calculates a cosine similarity matrix. This matrix contains similarity scores between every pair of entities, enabling efficient identification of semantic duplicates using the 0.85 threshold.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1022,
                "end_line": 1026,
                "language": "",
                "content": [
                  "",
                  "The clustering algorithm uses a greedy approach to group similar entities. For each unprocessed entity, it finds all other entities exceeding the similarity threshold and groups them into a cluster. This creates entity equivalence groups that will be merged.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1038,
                "end_line": 1042,
                "language": "",
                "content": [
                  "",
                  "When multiple entities are clustered together, the system selects the highest-confidence entity as the canonical form. This preserves the most reliable information while creating a comprehensive merged entity that tracks all variants and source entities.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1060,
                "end_line": 1093,
                "language": "",
                "content": [
                  "",
                  "### Graph Database Integration",
                  "",
                  "### Why Neo4j for Production GraphRAG Systems",
                  "",
                  "While NetworkX is excellent for analysis, production GraphRAG systems require persistent, scalable storage that can handle:",
                  "",
                  "- **Concurrent Access**: Multiple users querying the graph simultaneously",
                  "- **ACID Transactions**: Ensuring data consistency during updates",
                  "- **Optimized Queries**: Cypher query language optimized for graph traversal",
                  "- **Index Performance**: Fast entity lookup and relationship traversal",
                  "- **Scalability**: Handling millions of entities and relationships",
                  "",
                  "The transition from in-memory NetworkX graphs to persistent Neo4j storage marks a crucial evolution in GraphRAG systems. NetworkX serves us well during development and prototyping, but production systems demand the reliability, performance, and concurrent access capabilities that only enterprise graph databases can provide.",
                  "",
                  "### Performance Considerations in Graph Database Design",
                  "",
                  "The key to high-performance GraphRAG lies in thoughtful database design:",
                  "",
                  "1. **Strategic Indexing**: Indices on entity canonical names and types for fast lookup",
                  "2. **Batch Operations**: Bulk inserts minimize transaction overhead",
                  "3. **Query Optimization**: Cypher patterns that leverage graph structure",
                  "4. **Memory Management**: Proper configuration for large graph traversals",
                  "",
                  "These considerations become critical when your knowledge graph grows beyond thousands of entities. A well-indexed Neo4j instance can handle complex graph traversals across millions of nodes in milliseconds, while a poorly designed schema might struggle with simple lookups. The difference lies in understanding how graph databases optimize query execution.",
                  "",
                  "Our Neo4j integration implements production best practices from day one, ensuring your GraphRAG system scales with your knowledge base.",
                  "",
                  "### Production Neo4j Integration",
                  "",
                  "Enterprise graph databases require careful optimization for GraphRAG performance. The following implementation demonstrates how to build a production-ready Neo4j integration that handles the unique requirements of GraphRAG systems - from efficient entity storage to optimized relationship queries.",
                  ""
                ],
                "line_count": 32
              },
              {
                "start_line": 1106,
                "end_line": 1110,
                "language": "",
                "content": [
                  "",
                  "**Production-grade graph database management** addresses the critical challenges of enterprise GraphRAG deployment. Unlike traditional RAG systems that use simple vector stores, graph-based systems require sophisticated database management including connection pooling, strategic indexing, and batch optimization to achieve enterprise-scale performance.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1128,
                "end_line": 1132,
                "language": "",
                "content": [
                  "",
                  "**High-performance database connection configuration** optimizes Neo4j for concurrent GraphRAG workloads. The connection pool of 50 supports multiple simultaneous queries typical in enterprise environments, while the 30-second timeout prevents system hangs during peak usage. These settings enable production GraphRAG systems to handle hundreds of concurrent users.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1143,
                "end_line": 1147,
                "language": "",
                "content": [
                  "",
                  "**Strategic indexing for GraphRAG query patterns** creates the foundation for fast retrieval performance. These indices transform common GraphRAG queries from expensive full-table scans to O(1) lookups, making the difference between 10-second and 100-millisecond response times on enterprise-scale knowledge graphs.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1155,
                "end_line": 1159,
                "language": "",
                "content": [
                  "",
                  "**Entity lookup optimization** creates indices on the most frequently accessed entity properties. The canonical name index enables instant entity retrieval during graph traversal, while type and confidence indices support filtered queries that are common in GraphRAG applications - like finding high-confidence entities of specific types.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1168,
                "end_line": 1172,
                "language": "",
                "content": [
                  "",
                  "**Relationship traversal and provenance optimization** completes the indexing strategy. Relationship type and confidence indices accelerate graph traversal queries that form the core of GraphRAG functionality, while document indices enable fast provenance tracking - critical for enterprise applications requiring audit trails and source attribution.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1188,
                "end_line": 1192,
                "language": "",
                "content": [
                  "",
                  "**Enterprise knowledge graph storage orchestration** coordinates the complex process of storing structured knowledge in Neo4j. This method demonstrates the production patterns required for reliable, high-performance knowledge graph construction - patterns that differentiate enterprise GraphRAG from academic prototypes.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1202,
                "end_line": 1206,
                "language": "",
                "content": [
                  "",
                  "**Sequential storage with dependency management** ensures referential integrity by storing entities before relationships. Graph databases require entities to exist before relationships can reference them, making the storage order critical for avoiding database constraint violations.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1217,
                "end_line": 1221,
                "language": "",
                "content": [
                  "",
                  "**Comprehensive data persistence with provenance** stores all aspects of the knowledge graph including source document metadata. Provenance tracking is essential for enterprise GraphRAG systems where users need to understand the source of information and maintain audit trails for regulatory compliance.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1239,
                "end_line": 1243,
                "language": "",
                "content": [
                  "",
                  "**Performance monitoring and metrics collection** provides essential feedback for production optimization. These metrics enable database administrators to monitor storage performance, identify bottlenecks, and optimize batch sizes for maximum throughput. The entities-per-second metric is particularly valuable for capacity planning in enterprise deployments.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1247,
                "end_line": 1259,
                "language": "python",
                "content": [
                  "    def _store_entities_batch(self, session, entities: Dict[str, Any],",
                  "                             batch_size: int = 1000) -> int:",
                  "        \"\"\"Store entities in optimized batches.",
                  "",
                  "        Batch storage is critical for performance:",
                  "        - Single transactions reduce overhead from ~10ms to ~0.1ms per entity",
                  "        - MERGE operations handle entity updates gracefully",
                  "        - Progress reporting enables monitoring of large ingestions",
                  "",
                  "        Batch size of 1000 balances memory usage with transaction efficiency.",
                  "        \"\"\""
                ],
                "line_count": 11
              },
              {
                "start_line": 1263,
                "end_line": 1275,
                "language": "python",
                "content": [
                  "        entity_list = []",
                  "        for canonical, entity_data in entities.items():",
                  "            entity_list.append({",
                  "                'canonical': canonical,",
                  "                'type': entity_data.get('type', 'UNKNOWN'),",
                  "                'text_variants': entity_data.get('text_variants', []),",
                  "                'confidence': entity_data.get('confidence', 0.5),",
                  "                'extraction_method': entity_data.get('extraction_method', ''),",
                  "                'context': entity_data.get('context', '')[:500],  # Limit context to prevent memory issues",
                  "                'creation_timestamp': time.time()",
                  "            })"
                ],
                "line_count": 11
              },
              {
                "start_line": 1279,
                "end_line": 1287,
                "language": "python",
                "content": [
                  "        # Process in optimized batches",
                  "        total_stored = 0",
                  "        batch_count = (len(entity_list) + batch_size - 1) // batch_size",
                  "",
                  "        for i in range(0, len(entity_list), batch_size):",
                  "            batch = entity_list[i:i + batch_size]",
                  "            batch_num = (i // batch_size) + 1"
                ],
                "line_count": 7
              },
              {
                "start_line": 1291,
                "end_line": 1304,
                "language": "python",
                "content": [
                  "            # Cypher query optimized for batch operations",
                  "            cypher_query = \"\"\"",
                  "            UNWIND $entities AS entity",
                  "            MERGE (e:Entity {canonical: entity.canonical})",
                  "            SET e.type = entity.type,",
                  "                e.text_variants = entity.text_variants,",
                  "                e.confidence = entity.confidence,",
                  "                e.extraction_method = entity.extraction_method,",
                  "                e.context = entity.context,",
                  "                e.creation_timestamp = entity.creation_timestamp,",
                  "                e.updated_at = datetime()",
                  "            \"\"\""
                ],
                "line_count": 12
              },
              {
                "start_line": 1308,
                "end_line": 1317,
                "language": "python",
                "content": [
                  "            session.run(cypher_query, entities=batch)",
                  "            total_stored += len(batch)",
                  "",
                  "            # Progress reporting for large datasets",
                  "            if batch_num % 10 == 0 or batch_num == batch_count:",
                  "                print(f\"Entity batch {batch_num}/{batch_count} complete - {total_stored}/{len(entity_list)} entities stored\")",
                  "",
                  "        return total_stored"
                ],
                "line_count": 8
              },
              {
                "start_line": 1321,
                "end_line": 1331,
                "language": "python",
                "content": [
                  "    def _store_relationships_batch(self, session, relationships: List[Dict],",
                  "                                  batch_size: int = 1000) -> int:",
                  "        \"\"\"Store relationships in optimized batches.",
                  "",
                  "        Relationship storage presents unique challenges:",
                  "        - Must ensure both entities exist before creating relationships",
                  "        - MERGE operations prevent duplicate relationships",
                  "        - Batch processing critical for performance at scale",
                  "        \"\"\""
                ],
                "line_count": 9
              },
              {
                "start_line": 1335,
                "end_line": 1352,
                "language": "python",
                "content": [
                  "        if not relationships:",
                  "            return 0",
                  "",
                  "        # Filter relationships to only include those with existing entities",
                  "        valid_relationships = []",
                  "        for rel in relationships:",
                  "            if all(key in rel for key in ['subject', 'object', 'predicate']):",
                  "                valid_relationships.append({",
                  "                    'subject': rel['subject'],",
                  "                    'object': rel['object'],",
                  "                    'predicate': rel['predicate'],",
                  "                    'confidence': rel.get('confidence', 0.8),",
                  "                    'evidence': rel.get('evidence', ''),",
                  "                    'extraction_method': rel.get('extraction_method', ''),",
                  "                    'creation_timestamp': time.time()",
                  "                })"
                ],
                "line_count": 16
              },
              {
                "start_line": 1356,
                "end_line": 1366,
                "language": "python",
                "content": [
                  "        print(f\"Storing {len(valid_relationships)} valid relationships...\")",
                  "",
                  "        # Process in batches - smaller batch size for relationship complexity",
                  "        total_stored = 0",
                  "        batch_count = (len(valid_relationships) + batch_size - 1) // batch_size",
                  "",
                  "        for i in range(0, len(valid_relationships), batch_size):",
                  "            batch = valid_relationships[i:i + batch_size]",
                  "            batch_num = (i // batch_size) + 1"
                ],
                "line_count": 9
              },
              {
                "start_line": 1370,
                "end_line": 1383,
                "language": "python",
                "content": [
                  "            # Optimized Cypher for batch relationship creation",
                  "            cypher_query = \"\"\"",
                  "            UNWIND $relationships AS rel",
                  "            MATCH (s:Entity {canonical: rel.subject})",
                  "            MATCH (o:Entity {canonical: rel.object})",
                  "            MERGE (s)-[r:RELATED {type: rel.predicate}]->(o)",
                  "            SET r.confidence = rel.confidence,",
                  "                r.evidence = rel.evidence,",
                  "                r.extraction_method = rel.extraction_method,",
                  "                r.creation_timestamp = rel.creation_timestamp,",
                  "                r.updated_at = datetime()",
                  "            \"\"\""
                ],
                "line_count": 12
              },
              {
                "start_line": 1387,
                "end_line": 1402,
                "language": "python",
                "content": [
                  "            try:",
                  "                result = session.run(cypher_query, relationships=batch)",
                  "                total_stored += len(batch)",
                  "",
                  "                # Progress reporting",
                  "                if batch_num % 5 == 0 or batch_num == batch_count:",
                  "                    print(f\"Relationship batch {batch_num}/{batch_count} complete - {total_stored}/{len(valid_relationships)} relationships stored\")",
                  "",
                  "            except Exception as e:",
                  "                print(f\"Error storing relationship batch {batch_num}: {e}\")",
                  "                # Continue with next batch - partial failure handling",
                  "                continue",
                  "",
                  "        return total_stored"
                ],
                "line_count": 14
              },
              {
                "start_line": 1416,
                "end_line": 1424,
                "language": "python",
                "content": [
                  "# Essential imports for code analysis and graph construction",
                  "import ast",
                  "import tree_sitter",
                  "from tree_sitter import Language, Parser",
                  "from typing import Dict, List, Any, Optional",
                  "import os",
                  "from pathlib import Path"
                ],
                "line_count": 7
              },
              {
                "start_line": 1428,
                "end_line": 1437,
                "language": "python",
                "content": [
                  "class CodeGraphRAG:",
                  "    \"\"\"GraphRAG system specialized for software repositories.\"\"\"",
                  "",
                  "    def __init__(self, supported_languages: List[str] = ['python', 'javascript']):",
                  "        self.supported_languages = supported_languages",
                  "        ",
                  "        # Initialize Tree-sitter parsers for multi-language support",
                  "        self.parsers = self._setup_tree_sitter_parsers()"
                ],
                "line_count": 8
              },
              {
                "start_line": 1441,
                "end_line": 1453,
                "language": "python",
                "content": [
                  "        # Define code-specific entity types",
                  "        self.code_entity_types = {",
                  "            'function', 'class', 'method', 'variable', 'module',",
                  "            'interface', 'enum', 'constant', 'type', 'namespace'",
                  "        }",
                  "        ",
                  "        # Define code-specific relationship types",
                  "        self.code_relation_types = {",
                  "            'calls', 'inherits', 'implements', 'imports', 'uses',",
                  "            'defines', 'contains', 'overrides', 'instantiates'",
                  "        }"
                ],
                "line_count": 11
              },
              {
                "start_line": 1457,
                "end_line": 1463,
                "language": "python",
                "content": [
                  "        # Initialize specialized graph structures",
                  "        self.code_entities = {}",
                  "        self.code_relationships = []",
                  "        self.call_graph = nx.DiGraph()  # Function call relationships",
                  "        self.dependency_graph = nx.DiGraph()  # Module import relationships"
                ],
                "line_count": 5
              },
              {
                "start_line": 1480,
                "end_line": 1484,
                "language": "python",
                "content": [
                  "",
                  "The repository analysis configuration balances comprehensiveness with performance. The `max_files` limit prevents runaway analysis on massive codebases, while pattern filters focus on source code and exclude generated files, tests, and build artifacts. This selective approach ensures the knowledge graph captures architectural patterns without noise from temporary or derived files.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1490,
                "end_line": 1509,
                "language": "",
                "content": [
                  "",
                  "File discovery applies the configured patterns to identify relevant source files. This step is crucial for large repositories where indiscriminate parsing would overwhelm the system. The filtering ensures we analyze actual source code while skipping documentation, configuration files, and build outputs.",
                  "",
                  "        # Analyze each discovered source file",
                  "        all_entities = {}",
                  "        all_relationships = []",
                  "        ",
                  "        for file_path in source_files[:config.get('max_files', 1000)]:",
                  "            try:",
                  "                file_analysis = self._analyze_source_file(file_path, config)",
                  "                ",
                  "                if file_analysis:",
                  "                    all_entities.update(file_analysis['entities'])",
                  "                    all_relationships.extend(file_analysis['relationships'])",
                  "                    ",
                  "            except Exception as e:",
                  "                print(f\"Error analyzing {file_path}: {e}\")",
                  "                continue"
                ],
                "line_count": 18
              },
              {
                "start_line": 1519,
                "end_line": 1523,
                "language": "",
                "content": [
                  "",
                  "After collecting raw entities and relationships, the system builds specialized graph views optimized for different types of analysis. The call graph enables impact analysis (\"what functions will break if I change this?\"), while the dependency graph supports refactoring decisions (\"can I safely remove this module?\"). These views transform raw parsing data into actionable architectural insights.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1536,
                "end_line": 1542,
                "language": "python",
                "content": [
                  "",
                  "The comprehensive return structure provides both detailed analysis data and high-level statistics. This dual approach supports both programmatic access to graph data and human-readable progress reporting. The statistics enable monitoring analysis quality and identifying repositories that might benefit from configuration tuning or additional language support.",
                  "",
                  "#### Step 5: Python AST Analysis",
                  ""
                ],
                "line_count": 5
              },
              {
                "start_line": 1553,
                "end_line": 1557,
                "language": "",
                "content": [
                  "",
                  "Python AST analysis begins with safe file reading and syntax tree parsing. The UTF-8 encoding ensures international characters in code comments and strings don't cause failures. The AST parsing converts source code into a structured tree representation that we can traverse systematically to extract programming constructs.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1564,
                "end_line": 1577,
                "language": "python",
                "content": [
                  "",
                  "The `ast.walk()` function performs a depth-first traversal of the entire syntax tree, visiting every node from imports to function definitions to variable assignments. This comprehensive approach ensures we capture all code constructs that might be relevant for graph-based analysis, from top-level functions to nested inner classes.",
                  "                # Process function definitions",
                  "                if isinstance(node, ast.FunctionDef):",
                  "                    func_entity = self._extract_function_entity(node, file_path, source_code)",
                  "                    entities[func_entity['canonical']] = func_entity",
                  "                    ",
                  "                    # Extract function relationships (calls, uses)",
                  "                    func_relationships = self._extract_function_relationships(",
                  "                        node, func_entity['canonical'], source_code",
                  "                    )",
                  "                    relationships.extend(func_relationships)"
                ],
                "line_count": 12
              },
              {
                "start_line": 1591,
                "end_line": 1601,
                "language": "python",
                "content": [
                  "",
                  "Class analysis is more complex than function analysis because classes establish multiple relationship types. Beyond the class entity itself, we extract inheritance relationships (\"ClassA inherits from ClassB\"), method containment (\"ClassA contains method foo\"), and composition relationships (\"ClassA uses ClassB as a field type\"). This rich relationship modeling enables sophisticated architectural queries.",
                  "",
                  "                # Process import statements",
                  "                elif isinstance(node, (ast.Import, ast.ImportFrom)):",
                  "                    import_relationships = self._extract_import_relationships(",
                  "                        node, file_path",
                  "                    )",
                  "                    relationships.extend(import_relationships)"
                ],
                "line_count": 9
              },
              {
                "start_line": 1605,
                "end_line": 1620,
                "language": "python",
                "content": [
                  "            return {",
                  "                'entities': entities,",
                  "                'relationships': relationships,",
                  "                'file_metadata': {",
                  "                    'path': file_path,",
                  "                    'language': 'python', ",
                  "                    'lines_of_code': len(source_code.splitlines()),",
                  "                    'ast_nodes': len(list(ast.walk(tree)))",
                  "                }",
                  "            }",
                  "            ",
                  "        except Exception as e:",
                  "            print(f\"Python AST analysis error for {file_path}: {e}\")",
                  "            return None"
                ],
                "line_count": 14
              },
              {
                "start_line": 1632,
                "end_line": 1636,
                "language": "",
                "content": [
                  "",
                  "Function entity extraction captures the function's interface definition - its signature (name, parameters, return type) and documentation. The signature analysis includes type annotations when present, enabling type-aware graph queries. The source code preservation allows later semantic analysis or code generation tasks.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1641,
                "end_line": 1645,
                "language": "",
                "content": [
                  "",
                  "Parameter extraction focuses on argument names, forming the basis for data flow analysis. The canonical naming scheme prevents conflicts between functions with the same name in different modules. Return type analysis supports interface compatibility checking and refactoring safety verification.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1662,
                "end_line": 1668,
                "language": "",
                "content": [
                  "",
                  "The comprehensive function entity includes location information (file path, line numbers), complexity metrics, and call relationships. Line numbers enable IDE integration for navigation, while complexity metrics help identify refactoring candidates. The high confidence score reflects AST parsing's accuracy compared to text-based extraction methods.",
                  "",
                  "#### Step 6: Call Graph Construction",
                  ""
                ],
                "line_count": 5
              },
              {
                "start_line": 1685,
                "end_line": 1701,
                "language": "",
                "content": [
                  "",
                  "Call graph construction transforms extracted function entities into a NetworkX directed graph optimized for traversal queries. Each function becomes a node enriched with metadata that enables sophisticated analysis - complexity metrics for refactoring decisions, parameter lists for interface analysis, and truncated docstrings for semantic understanding.",
                  "",
                  "        # Add call relationships as directed edges",
                  "        for relationship in relationships:",
                  "            if relationship['predicate'] == 'calls':",
                  "                caller = relationship['subject']",
                  "                callee = relationship['object']",
                  "                ",
                  "                if caller in call_graph.nodes and callee in call_graph.nodes:",
                  "                    call_graph.add_edge(caller, callee, **{",
                  "                        'confidence': relationship.get('confidence', 0.8),",
                  "                        'call_count': relationship.get('call_count', 1),",
                  "                        'evidence': relationship.get('evidence', '')",
                  "                    })"
                ],
                "line_count": 15
              },
              {
                "start_line": 1709,
                "end_line": 1713,
                "language": "",
                "content": [
                  "",
                  "The completed call graph becomes a queryable representation of software architecture. Graph metrics enrichment adds centrality measures that identify architecturally critical functions - those that are frequently called (high in-degree centrality) or serve as communication bridges (high betweenness centrality). This analysis helps prioritize code review and refactoring efforts.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 1726,
                "end_line": 1737,
                "language": "",
                "content": [
                  "",
                  "Centrality calculations identify architecturally significant functions through network analysis. In-degree centrality reveals widely-used utility functions, out-degree centrality highlights functions that coordinate many operations, and betweenness centrality finds critical communication bottlenecks. These metrics inform architectural decisions and technical debt prioritization.",
                  "",
                  "            # Enrich nodes with centrality scores",
                  "            for node in call_graph.nodes():",
                  "                call_graph.nodes[node].update({",
                  "                    'in_degree_centrality': in_degree_centrality.get(node, 0),",
                  "                    'out_degree_centrality': out_degree_centrality.get(node, 0),",
                  "                    'betweenness_centrality': betweenness_centrality.get(node, 0)",
                  "                })"
                ],
                "line_count": 10
              },
              {
                "start_line": 1741,
                "end_line": 1757,
                "language": "python",
                "content": [
                  "            # Identify architecturally critical functions",
                  "            key_functions = sorted(",
                  "                call_graph.nodes(),",
                  "                key=lambda x: (call_graph.nodes[x]['in_degree_centrality'] +",
                  "                              call_graph.nodes[x]['betweenness_centrality']),",
                  "                reverse=True",
                  "            )[:10]",
                  "            ",
                  "            # Store comprehensive graph metadata",
                  "            call_graph.graph.update({",
                  "                'num_functions': num_nodes,",
                  "                'num_calls': num_edges,",
                  "                'key_functions': key_functions,",
                  "                'analysis_timestamp': time.time()",
                  "            })"
                ],
                "line_count": 15
              },
              {
                "start_line": 1777,
                "end_line": 1779,
                "language": "",
                "content": [
                  "Apple \u2192 partners_with \u2192 Company X \u2192 operates_in \u2192 Automotive \u2192 uses_technology \u2192 Technology Y"
                ],
                "line_count": 1
              },
              {
                "start_line": 1871,
                "end_line": 1881,
                "language": "python",
                "content": [
                  "# Advanced graph traversal for GraphRAG",
                  "",
                  "class GraphTraversalEngine:",
                  "    \"\"\"Advanced graph traversal engine for multi-hop reasoning.",
                  "",
                  "    This engine solves the fundamental challenge of graph exploration: how to find",
                  "    meaningful paths through a knowledge graph without being overwhelmed by the",
                  "    exponential growth of possible paths.",
                  "    \"\"\""
                ],
                "line_count": 9
              },
              {
                "start_line": 1885,
                "end_line": 1900,
                "language": "python",
                "content": [
                  "    # Key innovations:",
                  "    # - Adaptive Strategy Selection: Chooses optimal traversal based on query type",
                  "    # - Semantic Guidance: Uses embedding similarity to prune irrelevant paths",
                  "    # - Multi-Criteria Ranking: Evaluates paths on multiple quality dimensions",
                  "    # - Early Termination: Stops exploration when sufficient quality paths found",
                  "",
                  "    # Performance characteristics:",
                  "    # - 3-hop traversals: <200ms on graphs with 100K entities",
                  "    # - Semantic filtering reduces path space by 80-95%",
                  "    # - Quality-based ranking improves answer relevance by 40-60%",
                  "",
                  "    def __init__(self, neo4j_manager: Neo4jGraphManager, embedding_model):",
                  "        self.neo4j = neo4j_manager",
                  "        self.embedding_model = embedding_model"
                ],
                "line_count": 14
              },
              {
                "start_line": 1904,
                "end_line": 1913,
                "language": "python",
                "content": [
                  "        # Traversal strategies - each optimized for different exploration patterns",
                  "        self.traversal_strategies = {",
                  "            'breadth_first': self._breadth_first_traversal,        # Nearby relationships",
                  "            'depth_first': self._depth_first_traversal,           # Deep chains",
                  "            'semantic_guided': self._semantic_guided_traversal,   # Query-relevant paths",
                  "            'relevance_ranked': self._relevance_ranked_traversal, # High-quality relationships",
                  "            'community_focused': self._community_focused_traversal # Dense clusters",
                  "        }"
                ],
                "line_count": 8
              },
              {
                "start_line": 1917,
                "end_line": 1925,
                "language": "python",
                "content": [
                  "        # Path ranking functions - multi-criteria evaluation",
                  "        self.path_rankers = {",
                  "            'shortest_path': self._rank_by_path_length,              # Minimize hops",
                  "            'semantic_coherence': self._rank_by_semantic_coherence,   # Query relevance",
                  "            'entity_importance': self._rank_by_entity_importance,     # Entity significance",
                  "            'relationship_confidence': self._rank_by_relationship_confidence  # Extraction quality",
                  "        }"
                ],
                "line_count": 7
              },
              {
                "start_line": 1929,
                "end_line": 1942,
                "language": "python",
                "content": [
                  "    def multi_hop_retrieval(self, query: str, starting_entities: List[str],",
                  "                           traversal_config: Dict = None) -> Dict[str, Any]:",
                  "        \"\"\"Perform multi-hop retrieval using graph traversal.",
                  "",
                  "        This is the core method that enables GraphRAG's multi-hop reasoning:",
                  "",
                  "        1. Path Discovery: Find all relevant paths from seed entities",
                  "        2. Intelligent Filtering: Apply semantic and confidence-based pruning",
                  "        3. Path Ranking: Score paths by multiple quality criteria",
                  "        4. Context Extraction: Convert graph paths into natural language",
                  "        5. Synthesis: Combine path information into comprehensive answers",
                  "        \"\"\""
                ],
                "line_count": 12
              },
              {
                "start_line": 1946,
                "end_line": 1955,
                "language": "python",
                "content": [
                  "        config = traversal_config or {",
                  "            'max_hops': 3,                          # Reasonable depth limit",
                  "            'max_paths': 50,                        # Top-k path selection",
                  "            'strategy': 'semantic_guided',          # Query-relevant traversal",
                  "            'path_ranking': 'semantic_coherence',   # Primary ranking criterion",
                  "            'include_path_context': True,           # Rich context extraction",
                  "            'semantic_threshold': 0.7               # Quality gate",
                  "        }"
                ],
                "line_count": 8
              },
              {
                "start_line": 1959,
                "end_line": 1974,
                "language": "python",
                "content": [
                  "        print(f\"Multi-hop retrieval for query: {query[:100]}...\")",
                  "        print(f\"Starting from entities: {starting_entities}\")",
                  "        print(f\"Configuration - Max hops: {config['max_hops']}, Strategy: {config['strategy']}\")",
                  "",
                  "        # Step 1: Find relevant paths from starting entities",
                  "        # Each starting entity serves as a seed for exploration",
                  "        all_paths = []",
                  "        path_contexts = []",
                  "",
                  "        for start_entity in starting_entities:",
                  "            print(f\"Exploring paths from: {start_entity}\")",
                  "            entity_paths = self._find_entity_paths(start_entity, query, config)",
                  "            all_paths.extend(entity_paths)",
                  "            print(f\"Found {len(entity_paths)} paths from {start_entity}\")"
                ],
                "line_count": 14
              },
              {
                "start_line": 1978,
                "end_line": 1987,
                "language": "python",
                "content": [
                  "        print(f\"Total paths discovered: {len(all_paths)}\")",
                  "",
                  "        # Step 2: Rank and filter paths using configured ranking strategy",
                  "        # Multi-criteria ranking ensures high-quality path selection",
                  "        ranked_paths = self._rank_paths(all_paths, query, config)",
                  "        print(f\"Path ranking complete - using {config['path_ranking']} strategy\")",
                  "",
                  "        # Step 3: Extract context from top paths"
                ],
                "line_count": 8
              },
              {
                "start_line": 1991,
                "end_line": 2000,
                "language": "python",
                "content": [
                  "        # Convert graph structures into natural language narratives",
                  "        top_paths = ranked_paths[:config.get('max_paths', 50)]",
                  "        path_contexts = self._extract_path_contexts(top_paths, query)",
                  "        print(f\"Context extracted from {len(path_contexts)} top paths\")",
                  "",
                  "        # Step 4: Generate comprehensive answer using path information",
                  "        # Synthesize individual path contexts into coherent response",
                  "        comprehensive_context = self._synthesize_path_contexts(path_contexts, query)"
                ],
                "line_count": 8
              },
              {
                "start_line": 2004,
                "end_line": 2020,
                "language": "python",
                "content": [
                  "        return {",
                  "            'query': query,",
                  "            'starting_entities': starting_entities,",
                  "            'discovered_paths': len(all_paths),",
                  "            'top_paths': top_paths,",
                  "            'path_contexts': path_contexts,",
                  "            'comprehensive_context': comprehensive_context,",
                  "            'traversal_metadata': {",
                  "                'max_hops': config['max_hops'],",
                  "                'strategy_used': config['strategy'],",
                  "                'paths_explored': len(all_paths),",
                  "                'semantic_threshold': config['semantic_threshold'],",
                  "                'avg_path_length': sum(len(p.get('entity_path', [])) for p in top_paths) / len(top_paths) if top_paths else 0",
                  "            }",
                  "        }"
                ],
                "line_count": 15
              },
              {
                "start_line": 2026,
                "end_line": 2044,
                "language": "python",
                "content": [
                  "    def _semantic_guided_traversal(self, start_entity: str, query: str,",
                  "                                  config: Dict) -> List[List[str]]:",
                  "        \"\"\"Traverse graph guided by semantic similarity to query.",
                  "        ",
                  "        This is the most sophisticated traversal strategy, implementing semantic",
                  "        filtering at the path level rather than just relationship level.",
                  "        ",
                  "        The approach solves a key GraphRAG challenge: how to explore the graph",
                  "        systematically without being overwhelmed by irrelevant paths.",
                  "        \"\"\"",
                  "        ",
                  "        import numpy as np",
                  "        ",
                  "        # Initialize semantic comparison infrastructure",
                  "        query_embedding = self.embedding_model.encode([query])[0]",
                  "        semantic_threshold = config.get('semantic_threshold', 0.7)",
                  "        max_hops = config.get('max_hops', 3)"
                ],
                "line_count": 17
              },
              {
                "start_line": 2064,
                "end_line": 2087,
                "language": "",
                "content": [
                  "",
                  "The Cypher query implements sophisticated path discovery with built-in quality filtering. The confidence threshold (0.6) ensures we only traverse high-quality relationships, while the path length constraint prevents exponential explosion. The LIMIT clause caps exploration at 1000 paths to maintain performance, relying on semantic filtering to identify the most relevant subset.",
                  "",
                  "            result = session.run(cypher_query,",
                  "                               start_entity=start_entity,",
                  "                               max_hops=max_hops)",
                  "            ",
                  "            semantic_paths = []",
                  "            processed_paths = 0",
                  "            ",
                  "            for record in result:",
                  "                processed_paths += 1",
                  "                ",
                  "                # Extract path components from Neo4j result",
                  "                entity_path = record['entity_path']",
                  "                relation_path = record['relation_path']",
                  "                confidence_path = record['confidence_path']",
                  "                path_length = record['path_length']",
                  "                entity_types = record['entity_types']",
                  "                ",
                  "                # Convert graph path to natural language representation",
                  "                path_text = self._construct_path_text(entity_path, relation_path)"
                ],
                "line_count": 22
              },
              {
                "start_line": 2097,
                "end_line": 2117,
                "language": "",
                "content": [
                  "",
                  "Semantic similarity calculation uses cosine similarity between query and path embeddings to quantify relevance. This mathematical approach enables objective comparison between the user's information need and each discovered path's content. The cosine similarity metric ranges from 0 to 1, providing a consistent scale for path ranking.",
                  "",
                  "                # Apply semantic threshold and calculate quality metrics",
                  "                if semantic_similarity > semantic_threshold:",
                  "                    avg_confidence = sum(confidence_path) / len(confidence_path) if confidence_path else 0.5",
                  "                    path_diversity = len(set(entity_types)) / len(entity_types) if entity_types else 0",
                  "                    ",
                  "                    semantic_paths.append({",
                  "                        'entity_path': entity_path,",
                  "                        'relation_path': relation_path,",
                  "                        'confidence_path': confidence_path,",
                  "                        'path_length': path_length,",
                  "                        'semantic_similarity': float(semantic_similarity),",
                  "                        'avg_confidence': avg_confidence,",
                  "                        'path_diversity': path_diversity,",
                  "                        'path_text': path_text,",
                  "                        'entity_types': entity_types",
                  "                    })"
                ],
                "line_count": 19
              },
              {
                "start_line": 2131,
                "end_line": 2137,
                "language": "",
                "content": [
                  "",
                  "Final path ranking and efficiency reporting complete the semantic-guided traversal process. The dual-criteria sorting prioritizes semantic relevance while using confidence as a tie-breaker, ensuring the highest-quality paths rise to the top. The efficiency metrics demonstrate the dramatic reduction in information overload - typically 80-90% of paths are pruned while maintaining comprehensive coverage of relevant insights.",
                  "",
                  "#### Step 8: Path Context Synthesis",
                  ""
                ],
                "line_count": 5
              },
              {
                "start_line": 2150,
                "end_line": 2154,
                "language": "",
                "content": [
                  "",
                  "Path context extraction transforms raw graph paths into rich, narrative descriptions suitable for language model processing. This process enriches each path by fetching detailed entity information from the database, including descriptions, properties, and contextual metadata that wasn't included in the original path traversal results.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2164,
                "end_line": 2179,
                "language": "",
                "content": [
                  "",
                  "Narrative construction converts the structured graph path into flowing natural language that explains the logical connections between entities. This transformation is crucial for enabling LLMs to understand and reason about the discovered relationships. The relevance scoring ensures that even semantically similar paths are ranked by their actual utility for answering the specific query.",
                  "",
                  "                    path_contexts.append({",
                  "                        'path': path,",
                  "                        'entity_details': entity_details,",
                  "                        'narrative': narrative,",
                  "                        'relevance_score': relevance_score,",
                  "                        'context_length': len(narrative.split())",
                  "                    })",
                  "                    ",
                  "                except Exception as e:",
                  "                    print(f\"Error extracting path context: {e}\")",
                  "                    continue"
                ],
                "line_count": 14
              },
              {
                "start_line": 2183,
                "end_line": 2188,
                "language": "python",
                "content": [
                  "        # Rank contexts by query relevance",
                  "        path_contexts.sort(key=lambda x: x['relevance_score'], reverse=True)",
                  "        ",
                  "        return path_contexts"
                ],
                "line_count": 4
              },
              {
                "start_line": 2202,
                "end_line": 2206,
                "language": "",
                "content": [
                  "",
                  "Narrative construction begins with basic path validation and initialization. The process requires at least two entities to form a meaningful relationship statement. The narrative parts list will accumulate individual relationship statements that will be connected into a flowing narrative.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2218,
                "end_line": 2222,
                "language": "",
                "content": [
                  "",
                  "Each step in the path becomes a natural language statement connecting two entities through their relationship. Entity descriptions provide human-readable names and context, while relation humanization converts technical relationship types into readable phrases. This piecewise construction ensures each logical connection is clearly articulated.",
                  ""
                ],
                "line_count": 3
              },
              {
                "start_line": 2227,
                "end_line": 2249,
                "language": "",
                "content": [
                  "",
                  "The final narrative assembly uses contextual connectors to create flowing, readable text from individual relationship statements. This step transforms a sequence of discrete facts into a coherent explanation that language models can effectively process and reason about.",
                  "",
                  "    def _humanize_relation(self, relation_type: str) -> str:",
                  "        \"\"\"Convert technical relation types to human-readable form.\"\"\"",
                  "        ",
                  "        relation_map = {",
                  "            'calls': 'calls',",
                  "            'inherits': 'inherits from', ",
                  "            'uses': 'uses',",
                  "            'contains': 'contains',",
                  "            'implements': 'implements',",
                  "            'founded': 'founded',",
                  "            'works_for': 'works for',",
                  "            'located_in': 'is located in',",
                  "            'part_of': 'is part of',",
                  "            'causes': 'causes',",
                  "            'leads_to': 'leads to'",
                  "        }",
                  "        ",
                  "        return relation_map.get(relation_type, f'is related to via {relation_type}')"
                ],
                "line_count": 21
              },
              {
                "start_line": 2333,
                "end_line": 2343,
                "language": "python",
                "content": [
                  "# Hybrid graph-vector search system",
                  "",
                  "class HybridGraphVectorRAG:",
                  "    \"\"\"Hybrid system combining graph traversal with vector search.",
                  "",
                  "    This system represents the state-of-the-art in RAG architecture, addressing",
                  "    the fundamental limitation that neither graph nor vector search alone can",
                  "    handle the full spectrum of information retrieval challenges.",
                  "    \"\"\""
                ],
                "line_count": 9
              },
              {
                "start_line": 2347,
                "end_line": 2363,
                "language": "python",
                "content": [
                  "    # Key architectural principles:",
                  "",
                  "    # 1. **Complementary Strengths**: Leverages vector search for semantic similarity",
                  "    #    and graph search for relational reasoning",
                  "",
                  "    # 2. **Adaptive Fusion**: Dynamically weights approaches based on query analysis",
                  "    #    - Factual queries: Higher vector weight",
                  "    #    - Analytical queries: Higher graph weight",
                  "    #    - Complex queries: Balanced combination",
                  "",
                  "    # 3. **Intelligent Integration**: Ensures graph and vector results enhance",
                  "    #    rather than compete with each other",
                  "",
                  "    # 4. **Performance Optimization**: Parallel execution and result caching",
                  "    #    minimize latency while maximizing coverage"
                ],
                "line_count": 15
              },
              {
                "start_line": 2367,
                "end_line": 2379,
                "language": "python",
                "content": [
                  "    # Performance characteristics:",
                  "    # - Response time: 200-800ms for complex hybrid queries",
                  "    # - Coverage improvement: 30-40% over single-method approaches",
                  "    # - Accuracy improvement: 25-35% for multi-hop reasoning queries",
                  "",
                  "    def __init__(self, neo4j_manager: Neo4jGraphManager,",
                  "                 vector_store, embedding_model, llm_model):",
                  "        self.neo4j = neo4j_manager",
                  "        self.vector_store = vector_store",
                  "        self.embedding_model = embedding_model",
                  "        self.llm_model = llm_model"
                ],
                "line_count": 11
              },
              {
                "start_line": 2383,
                "end_line": 2394,
                "language": "python",
                "content": [
                  "        # Initialize graph traversal engine",
                  "        self.graph_traversal = GraphTraversalEngine(neo4j_manager, embedding_model)",
                  "",
                  "        # Fusion strategies - each optimized for different query patterns",
                  "        self.fusion_strategies = {",
                  "            'weighted_combination': self._weighted_fusion,        # Linear combination with learned weights",
                  "            'rank_fusion': self._rank_fusion,                   # Reciprocal rank fusion",
                  "            'cascade_retrieval': self._cascade_retrieval,       # Sequential refinement",
                  "            'adaptive_selection': self._adaptive_selection      # Query-aware strategy selection",
                  "        }"
                ],
                "line_count": 10
              },
              {
                "start_line": 2398,
                "end_line": 2406,
                "language": "python",
                "content": [
                  "        # Performance tracking",
                  "        self.performance_metrics = {",
                  "            'vector_retrieval_time': [],",
                  "            'graph_retrieval_time': [],",
                  "            'fusion_time': [],",
                  "            'total_query_time': []",
                  "        }"
                ],
                "line_count": 7
              },
              {
                "start_line": 2412,
                "end_line": 2424,
                "language": "python",
                "content": [
                  "    def hybrid_search(self, query: str, search_config: Dict = None) -> Dict[str, Any]:",
                  "        \"\"\"Perform hybrid graph-vector search.",
                  "",
                  "        This method orchestrates the complete hybrid search pipeline:",
                  "",
                  "        1. **Parallel Retrieval**: Simultaneously performs vector and graph search",
                  "        2. **Entity Bridging**: Uses vector results to seed graph exploration",
                  "        3. **Intelligent Fusion**: Combines results based on query analysis",
                  "        4. **Quality Assurance**: Validates and ranks final context",
                  "        5. **Response Generation**: Synthesizes comprehensive answers",
                  "        \"\"\""
                ],
                "line_count": 11
              },
              {
                "start_line": 2428,
                "end_line": 2440,
                "language": "python",
                "content": [
                  "        # The hybrid approach is particularly powerful for queries that benefit from both:",
                  "        # - Semantic similarity (vector strength)",
                  "        # - Relational reasoning (graph strength)",
                  "",
                  "        # Example scenarios where hybrid excels:",
                  "        # - \"What are the environmental impacts of technologies used by Tesla's suppliers?\"",
                  "        #   (requires both semantic understanding of 'environmental impacts' and",
                  "        #    graph traversal: Tesla \u2192 suppliers \u2192 technologies \u2192 impacts)",
                  "",
                  "        import time",
                  "        start_time = time.time()"
                ],
                "line_count": 11
              },
              {
                "start_line": 2444,
                "end_line": 2455,
                "language": "python",
                "content": [
                  "        config = search_config or {",
                  "            'vector_weight': 0.4,                    # Base weight for vector results",
                  "            'graph_weight': 0.6,                     # Base weight for graph results",
                  "            'fusion_strategy': 'adaptive_selection', # Dynamic strategy selection",
                  "            'max_vector_results': 20,                # Top-k vector retrieval",
                  "            'max_graph_paths': 15,                   # Top-k graph paths",
                  "            'similarity_threshold': 0.7,             # Quality gate",
                  "            'use_query_expansion': True,             # Enhance query coverage",
                  "            'parallel_execution': True               # Performance optimization",
                  "        }"
                ],
                "line_count": 10
              },
              {
                "start_line": 2459,
                "end_line": 2462,
                "language": "python",
                "content": [
                  "        print(f\"Hybrid search for: {query[:100]}...\")",
                  "        print(f\"Strategy: {config['fusion_strategy']}, Weights: V={config['vector_weight']}, G={config['graph_weight']}\")"
                ],
                "line_count": 2
              },
              {
                "start_line": 2466,
                "end_line": 2473,
                "language": "python",
                "content": [
                  "        # Step 1: Vector-based retrieval (semantic similarity)",
                  "        vector_start = time.time()",
                  "        print(\"Performing vector retrieval...\")",
                  "        vector_results = self._perform_vector_retrieval(query, config)",
                  "        vector_time = time.time() - vector_start",
                  "        print(f\"Vector retrieval complete: {len(vector_results.get('results', []))} results in {vector_time:.2f}s\")"
                ],
                "line_count": 6
              },
              {
                "start_line": 2477,
                "end_line": 2483,
                "language": "python",
                "content": [
                  "        # Step 2: Identify seed entities for graph traversal",
                  "        # This bridges vector and graph search by using vector results to identify",
                  "        # relevant entities in the knowledge graph",
                  "        seed_entities = self._identify_seed_entities(query, vector_results)",
                  "        print(f\"Identified {len(seed_entities)} seed entities for graph traversal\")"
                ],
                "line_count": 5
              },
              {
                "start_line": 2487,
                "end_line": 2494,
                "language": "python",
                "content": [
                  "        # Step 3: Graph-based multi-hop retrieval (relationship reasoning)",
                  "        graph_start = time.time()",
                  "        print(\"Performing graph traversal...\")",
                  "        graph_results = self._perform_graph_retrieval(query, seed_entities, config)",
                  "        graph_time = time.time() - graph_start",
                  "        print(f\"Graph traversal complete: {len(graph_results.get('top_paths', []))} paths in {graph_time:.2f}s\")"
                ],
                "line_count": 6
              },
              {
                "start_line": 2498,
                "end_line": 2508,
                "language": "python",
                "content": [
                  "        # Step 4: Intelligent fusion using configured strategy",
                  "        # This is where the magic happens - combining complementary strengths",
                  "        fusion_start = time.time()",
                  "        fusion_strategy = config.get('fusion_strategy', 'adaptive_selection')",
                  "        print(f\"Applying fusion strategy: {fusion_strategy}\")",
                  "        fused_results = self.fusion_strategies[fusion_strategy](",
                  "            query, vector_results, graph_results, config",
                  "        )",
                  "        fusion_time = time.time() - fusion_start"
                ],
                "line_count": 9
              },
              {
                "start_line": 2512,
                "end_line": 2521,
                "language": "python",
                "content": [
                  "        # Step 5: Generate comprehensive response",
                  "        response_start = time.time()",
                  "        comprehensive_response = self._generate_hybrid_response(",
                  "            query, fused_results, config",
                  "        )",
                  "        response_time = time.time() - response_start",
                  "",
                  "        total_time = time.time() - start_time"
                ],
                "line_count": 8
              },
              {
                "start_line": 2525,
                "end_line": 2533,
                "language": "python",
                "content": [
                  "        # Track performance metrics",
                  "        self.performance_metrics['vector_retrieval_time'].append(vector_time)",
                  "        self.performance_metrics['graph_retrieval_time'].append(graph_time)",
                  "        self.performance_metrics['fusion_time'].append(fusion_time)",
                  "        self.performance_metrics['total_query_time'].append(total_time)",
                  "",
                  "        print(f\"Hybrid search complete in {total_time:.2f}s - {len(fused_results.get('contexts', []))} final contexts\")"
                ],
                "line_count": 7
              },
              {
                "start_line": 2537,
                "end_line": 2557,
                "language": "python",
                "content": [
                  "        return {",
                  "            'query': query,",
                  "            'vector_results': vector_results,",
                  "            'graph_results': graph_results,",
                  "            'fused_results': fused_results,",
                  "            'comprehensive_response': comprehensive_response,",
                  "            'search_metadata': {",
                  "                'fusion_strategy': fusion_strategy,",
                  "                'vector_count': len(vector_results.get('results', [])),",
                  "                'graph_paths': len(graph_results.get('top_paths', [])),",
                  "                'final_context_sources': len(fused_results.get('contexts', [])),",
                  "                'performance': {",
                  "                    'vector_time_ms': vector_time * 1000,",
                  "                    'graph_time_ms': graph_time * 1000,",
                  "                    'fusion_time_ms': fusion_time * 1000,",
                  "                    'total_time_ms': total_time * 1000",
                  "                }",
                  "            }",
                  "        }"
                ],
                "line_count": 19
              },
              {
                "start_line": 2605,
                "end_line": 2624,
                "language": "python",
                "content": [
                  "    def _adaptive_selection(self, query: str, vector_results: Dict,",
                  "                          graph_results: Dict, config: Dict) -> Dict[str, Any]:",
                  "        \"\"\"Adaptively select and combine results based on query characteristics.",
                  "        ",
                  "        This approach typically improves answer quality by 25-40% over static weighting",
                  "        by intelligently weighing vector vs graph results based on query type.",
                  "        \"\"\"",
                  "",
                  "        print(\"Applying adaptive selection fusion strategy...\")",
                  "",
                  "        # Phase 1: Analyze query to understand characteristics and intent",
                  "        query_analysis = self._analyze_query_characteristics(query)",
                  "        print(f\"Query analysis: {query_analysis}\")",
                  "",
                  "        # Phase 2: Determine optimal fusion weights based on query type",
                  "        fusion_weights = self._determine_adaptive_weights(query_analysis)",
                  "        print(f\"Adaptive weights - Vector: {fusion_weights['vector_weight']:.2f}, \"",
                  "              f\"Graph: {fusion_weights['graph_weight']:.2f}\")"
                ],
                "line_count": 18
              },
              {
                "start_line": 2630,
                "end_line": 2651,
                "language": "python",
                "content": [
                  "",
                  "        # Phase 3: Extract vector contexts with enriched metadata",
                  "        vector_contexts = vector_results.get('results', [])",
                  "        print(f\"Processing {len(vector_contexts)} vector contexts\")",
                  "",
                  "        # Phase 4: Extract graph contexts with path information",
                  "        graph_contexts = []",
                  "        if 'path_contexts' in graph_results:",
                  "            graph_contexts = [",
                  "                {",
                  "                    'content': ctx['narrative'],",
                  "                    'score': ctx['relevance_score'],",
                  "                    'type': 'graph_path',",
                  "                    'metadata': ctx['path'],",
                  "                    'path_length': len(ctx['path'].get('entity_path', [])),",
                  "                    'confidence': ctx['path'].get('avg_confidence', 0.5)",
                  "                }",
                  "                for ctx in graph_results['path_contexts']",
                  "            ]",
                  "        print(f\"Processing {len(graph_contexts)} graph contexts\")"
                ],
                "line_count": 20
              },
              {
                "start_line": 2657,
                "end_line": 2680,
                "language": "python",
                "content": [
                  "        # Phase 5: Initialize context collection with adaptive scoring",
                  "        all_contexts = []",
                  "",
                  "        # Process vector contexts with adaptive weights and boosts",
                  "        for ctx in vector_contexts:",
                  "            vector_score = ctx.get('similarity_score', 0.5)",
                  "            ",
                  "            # Apply adaptive weight based on query analysis",
                  "            adapted_score = vector_score * fusion_weights['vector_weight']",
                  "",
                  "            # Apply query-specific boosts for better relevance",
                  "            if query_analysis.get('type') == 'factual' and vector_score > 0.8:",
                  "                adapted_score *= 1.2  # Boost high-confidence factual matches",
                  "",
                  "            all_contexts.append({",
                  "                'content': ctx['content'],",
                  "                'score': adapted_score,",
                  "                'type': 'vector_similarity',",
                  "                'source': ctx.get('metadata', {}).get('source', 'unknown'),",
                  "                'original_score': vector_score,",
                  "                'boost_applied': adapted_score / (vector_score * fusion_weights['vector_weight']) if vector_score > 0 else 1.0",
                  "            })"
                ],
                "line_count": 22
              },
              {
                "start_line": 2686,
                "end_line": 2693,
                "language": "python",
                "content": [
                  "        # Process graph contexts with relationship-aware adaptive scoring",
                  "        for ctx in graph_contexts:",
                  "            graph_score = ctx['score']",
                  "            ",
                  "            # Apply adaptive weight based on query analysis",
                  "            adapted_score = graph_score * fusion_weights['graph_weight']"
                ],
                "line_count": 6
              },
              {
                "start_line": 2697,
                "end_line": 2704,
                "language": "python",
                "content": [
                  "            # Apply query-specific boosts for relationship understanding",
                  "            if query_analysis.get('complexity') == 'complex' and ctx['path_length'] > 2:",
                  "                adapted_score *= 1.3  # Boost multi-hop reasoning for complex queries",
                  "",
                  "            if ctx['confidence'] > 0.8:",
                  "                adapted_score *= 1.1  # Boost high-confidence relationships"
                ],
                "line_count": 6
              },
              {
                "start_line": 2708,
                "end_line": 2719,
                "language": "python",
                "content": [
                  "            all_contexts.append({",
                  "                'content': ctx['content'],",
                  "                'score': adapted_score,",
                  "                'type': 'graph_path',",
                  "                'source': f\"path_length_{ctx['path_length']}\",",
                  "                'original_score': graph_score,",
                  "                'path_metadata': ctx['metadata'],",
                  "                'path_length': ctx['path_length'],",
                  "                'confidence': ctx['confidence']",
                  "            })"
                ],
                "line_count": 10
              },
              {
                "start_line": 2723,
                "end_line": 2733,
                "language": "python",
                "content": [
                  "        # Rank by adapted scores",
                  "        all_contexts.sort(key=lambda x: x['score'], reverse=True)",
                  "        print(f\"Ranked {len(all_contexts)} total contexts\")",
                  "",
                  "        # Select top contexts with diversity to ensure comprehensive coverage",
                  "        selected_contexts = self._select_diverse_contexts(",
                  "            all_contexts, max_contexts=config.get('max_final_contexts', 10)",
                  "        )",
                  "        print(f\"Selected {len(selected_contexts)} diverse contexts for final answer\")"
                ],
                "line_count": 9
              },
              {
                "start_line": 2737,
                "end_line": 2794,
                "language": "python",
                "content": [
                  "        # Calculate fusion statistics",
                  "        vector_selected = sum(1 for ctx in selected_contexts if ctx['type'] == 'vector_similarity')",
                  "        graph_selected = sum(1 for ctx in selected_contexts if ctx['type'] == 'graph_path')",
                  "",
                  "        return {",
                  "            'contexts': selected_contexts,",
                  "            'fusion_weights': fusion_weights,",
                  "            'query_analysis': query_analysis,",
                  "            'total_candidates': len(all_contexts),",
                  "            'selection_stats': {",
                  "                'vector_contexts_selected': vector_selected,",
                  "                'graph_contexts_selected': graph_selected,",
                  "                'selection_ratio': f\"{vector_selected}/{graph_selected}\" if graph_selected > 0 else f\"{vector_selected}/0\"",
                  "            }",
                  "        }",
                  "",
                  "    def _analyze_query_characteristics(self, query: str) -> Dict[str, Any]:",
                  "        \"\"\"Analyze query to determine optimal retrieval strategy.",
                  "",
                  "        This analysis is crucial for adaptive fusion - different query types",
                  "        benefit from different combinations of vector and graph search:",
                  "",
                  "        **Query Type Analysis:**",
                  "        - **Factual**: Direct lookup queries (\"What is X?\") \u2192 Vector-heavy",
                  "        - **Analytical**: Cause-effect relationships (\"How does X impact Y?\") \u2192 Balanced",
                  "        - **Relational**: Connection queries (\"How is X related to Y?\") \u2192 Graph-heavy",
                  "        - **Comparative**: Multi-entity analysis (\"Compare X and Y\") \u2192 Balanced",
                  "",
                  "        **Complexity Assessment:**",
                  "        - **Simple**: Single-hop, direct answer",
                  "        - **Complex**: Multi-step reasoning, synthesis required",
                  "",
                  "        **Scope Evaluation:**",
                  "        - **Narrow**: Specific entities or facts",
                  "        - **Broad**: General topics or concepts",
                  "",
                  "        The LLM analysis enables dynamic strategy selection rather than static rules.",
                  "        \"\"\"",
                  "",
                  "        analysis_prompt = f\"\"\"",
                  "        As an expert query analyst, analyze this search query to determine the optimal retrieval strategy.",
                  "",
                  "        Query: \"{query}\"",
                  "",
                  "        Analyze the query on these dimensions and return ONLY a JSON response:",
                  "",
                  "        {{",
                  "            \"complexity\": \"simple\" or \"complex\",",
                  "            \"scope\": \"narrow\" or \"broad\",",
                  "            \"type\": \"factual\" or \"analytical\" or \"procedural\" or \"comparative\" or \"relational\",",
                  "            \"graph_benefit\": 0.0-1.0,",
                  "            \"vector_benefit\": 0.0-1.0,",
                  "            \"reasoning_required\": true/false,",
                  "            \"multi_entity\": true/false,",
                  "            \"explanation\": \"Brief explanation of the classification\"",
                  "        }}"
                ],
                "line_count": 56
              },
              {
                "start_line": 2798,
                "end_line": 2807,
                "language": "python",
                "content": [
                  "        Guidelines:",
                  "        - graph_benefit: High for queries requiring relationship traversal or multi-hop reasoning",
                  "        - vector_benefit: High for semantic similarity and factual lookup queries",
                  "        - reasoning_required: True if query needs synthesis or inference",
                  "        - multi_entity: True if query involves multiple entities or comparisons",
                  "",
                  "        Return only the JSON object:",
                  "        \"\"\""
                ],
                "line_count": 8
              },
              {
                "start_line": 2811,
                "end_line": 2823,
                "language": "python",
                "content": [
                  "        try:",
                  "            response = self.llm_model.predict(analysis_prompt, temperature=0.1)",
                  "            analysis = json.loads(self._extract_json_from_response(response))",
                  "",
                  "            # Validate required fields and add defaults if missing",
                  "            required_fields = ['complexity', 'scope', 'type', 'graph_benefit', 'vector_benefit']",
                  "            for field in required_fields:",
                  "                if field not in analysis:",
                  "                    # Provide sensible defaults based on query length and content",
                  "                    if field == 'complexity':",
                  "                        analysis[field] = 'complex' if len(query.split()) > 10 or '?' in query else 'simple'"
                ],
                "line_count": 11
              },
              {
                "start_line": 2827,
                "end_line": 2836,
                "language": "python",
                "content": [
                  "                    elif field == 'scope':",
                  "                        analysis[field] = 'broad' if len(query.split()) > 8 else 'narrow'",
                  "                    elif field == 'type':",
                  "                        analysis[field] = 'factual'",
                  "                    elif field == 'graph_benefit':",
                  "                        analysis[field] = 0.6 if 'how' in query.lower() or 'why' in query.lower() else 0.4",
                  "                    elif field == 'vector_benefit':",
                  "                        analysis[field] = 0.7"
                ],
                "line_count": 8
              },
              {
                "start_line": 2840,
                "end_line": 2846,
                "language": "python",
                "content": [
                  "            # Ensure numeric values are in valid range",
                  "            analysis['graph_benefit'] = max(0.0, min(1.0, float(analysis.get('graph_benefit', 0.5))))",
                  "            analysis['vector_benefit'] = max(0.0, min(1.0, float(analysis.get('vector_benefit', 0.7))))",
                  "",
                  "            return analysis"
                ],
                "line_count": 5
              },
              {
                "start_line": 2850,
                "end_line": 2861,
                "language": "python",
                "content": [
                  "        except Exception as e:",
                  "            print(f\"Query analysis error: {e}\")",
                  "            print(\"Using fallback query analysis\")",
                  "",
                  "            # Enhanced fallback analysis based on query patterns",
                  "            query_lower = query.lower()",
                  "",
                  "            # Determine complexity based on query patterns",
                  "            complexity_indicators = ['how', 'why', 'explain', 'analyze', 'compare', 'relationship', 'impact', 'effect']",
                  "            is_complex = any(indicator in query_lower for indicator in complexity_indicators)"
                ],
                "line_count": 10
              },
              {
                "start_line": 2865,
                "end_line": 2879,
                "language": "python",
                "content": [
                  "            # Determine scope based on query specificity",
                  "            specific_patterns = ['what is', 'who is', 'when did', 'where is']",
                  "            is_narrow = any(pattern in query_lower for pattern in specific_patterns)",
                  "",
                  "            # Determine type based on query structure",
                  "            if any(word in query_lower for word in ['compare', 'versus', 'vs', 'difference']):",
                  "                query_type = 'comparative'",
                  "            elif any(word in query_lower for word in ['how', 'why', 'analyze', 'impact', 'effect']):",
                  "                query_type = 'analytical'",
                  "            elif any(word in query_lower for word in ['relate', 'connect', 'link', 'between']):",
                  "                query_type = 'relational'",
                  "            else:",
                  "                query_type = 'factual'"
                ],
                "line_count": 13
              },
              {
                "start_line": 2883,
                "end_line": 2894,
                "language": "python",
                "content": [
                  "            return {",
                  "                'complexity': 'complex' if is_complex else 'simple',",
                  "                'scope': 'narrow' if is_narrow else 'broad',",
                  "                'type': query_type,",
                  "                'graph_benefit': 0.7 if query_type in ['analytical', 'relational'] else 0.4,",
                  "                'vector_benefit': 0.8 if query_type == 'factual' else 0.6,",
                  "                'reasoning_required': is_complex,",
                  "                'multi_entity': 'and' in query_lower or ',' in query,",
                  "                'explanation': f'Fallback analysis: {query_type} query with {\"complex\" if is_complex else \"simple\"} reasoning'",
                  "            }"
                ],
                "line_count": 10
              },
              {
                "start_line": 2900,
                "end_line": 2913,
                "language": "python",
                "content": [
                  "    def _generate_hybrid_response(self, query: str, fused_results: Dict,",
                  "                                 config: Dict) -> Dict[str, Any]:",
                  "        \"\"\"Generate comprehensive response using hybrid context.\"\"\"",
                  "        ",
                  "        contexts = fused_results.get('contexts', [])",
                  "        ",
                  "        if not contexts:",
                  "            return {'response': \"I couldn't find relevant information to answer your question.\"}",
                  "        ",
                  "        # Separate contexts by retrieval type for specialized processing",
                  "        vector_contexts = [ctx for ctx in contexts if ctx['type'] == 'vector_similarity']",
                  "        graph_contexts = [ctx for ctx in contexts if ctx['type'] == 'graph_path']"
                ],
                "line_count": 12
              },
              {
                "start_line": 2938,
                "end_line": 2951,
                "language": "",
                "content": [
                  "",
                  "The hybrid prompt template explicitly distinguishes between factual information (from vector search) and relationship knowledge (from graph traversal). This distinction enables the LLM to appropriately handle each information type - treating direct facts as authoritative while explaining relationship-based insights as inferences. The instruction set guides the model to synthesize both types of information while maintaining proper attribution and confidence levels.",
                  "",
                  "        try:",
                  "            # Generate response with controlled temperature for consistency",
                  "            response = self.llm_model.predict(response_prompt, temperature=0.3)",
                  "            ",
                  "            # Extract source attributions for transparency",
                  "            source_attributions = self._extract_source_attributions(contexts)",
                  "            ",
                  "            # Calculate response confidence based on context quality",
                  "            response_confidence = self._calculate_response_confidence(contexts)"
                ],
                "line_count": 12
              },
              {
                "start_line": 2955,
                "end_line": 2971,
                "language": "python",
                "content": [
                  "            return {",
                  "                'response': response,",
                  "                'source_attributions': source_attributions,",
                  "                'confidence_score': response_confidence,",
                  "                'context_breakdown': {",
                  "                    'vector_sources': len(vector_contexts),",
                  "                    'graph_paths': len(graph_contexts),",
                  "                    'total_contexts': len(contexts)",
                  "                },",
                  "                'reasoning_type': 'hybrid_graph_vector'",
                  "            }",
                  "            ",
                  "        except Exception as e:",
                  "            print(f\"Response generation error: {e}\")",
                  "            return {'response': \"I encountered an error generating the response.\"}"
                ],
                "line_count": 15
              },
              {
                "start_line": 2993,
                "end_line": 3006,
                "language": "python",
                "content": [
                  "# Production-ready GraphRAG system integration",
                  "class ProductionGraphRAG:",
                  "    \"\"\"Production-ready GraphRAG system.\"\"\"",
                  "    ",
                  "    def __init__(self, config: Dict):",
                  "        # Initialize core knowledge extraction components",
                  "        self.kg_extractor = KnowledgeGraphExtractor(",
                  "            llm_model=config['llm_model']",
                  "        )",
                  "        self.code_analyzer = CodeGraphRAG(",
                  "            supported_languages=config.get('languages', ['python', 'javascript'])",
                  "        )"
                ],
                "line_count": 12
              },
              {
                "start_line": 3010,
                "end_line": 3026,
                "language": "python",
                "content": [
                  "        # Initialize graph storage and hybrid search infrastructure",
                  "        self.neo4j_manager = Neo4jGraphManager(",
                  "            uri=config['neo4j_uri'],",
                  "            username=config['neo4j_user'],",
                  "            password=config['neo4j_password']",
                  "        )",
                  "        self.hybrid_rag = HybridGraphVectorRAG(",
                  "            neo4j_manager=self.neo4j_manager,",
                  "            vector_store=config['vector_store'],",
                  "            embedding_model=config['embedding_model'],",
                  "            llm_model=config['llm_model']",
                  "        )",
                  "        ",
                  "        # Performance monitoring infrastructure",
                  "        self.performance_metrics = {}"
                ],
                "line_count": 15
              },
              {
                "start_line": 3046,
                "end_line": 3066,
                "language": "",
                "content": [
                  "",
                  "Document ingestion implements the complete pipeline from raw text to queryable knowledge graph. The extraction phase uses LLM-powered analysis to identify entities and relationships, while the storage phase persists this structured knowledge to Neo4j with proper indexing and optimization. The return structure provides complete visibility into both extraction quality and storage success.",
                  "",
                  "    def analyze_repository(self, repo_path: str) -> Dict[str, Any]:",
                  "        \"\"\"Analyze code repository and build code graph.\"\"\"",
                  "        ",
                  "        # Extract architectural knowledge from source code",
                  "        code_analysis = self.code_analyzer.analyze_repository(repo_path)",
                  "        ",
                  "        # Persist code entities and relationships to graph database",
                  "        code_storage = self.neo4j_manager.store_knowledge_graph(",
                  "            code_analysis['entities'],",
                  "            code_analysis['relationships']",
                  "        )",
                  "        ",
                  "        return {",
                  "            'code_analysis': code_analysis,",
                  "            'storage_result': code_storage",
                  "        }"
                ],
                "line_count": 19
              },
              {
                "start_line": 3077,
                "end_line": 3444,
                "language": "",
                "content": [
                  "",
                  "The unified search interface provides access to the full power of GraphRAG through a simple API. The hybrid search combines graph traversal's multi-hop reasoning with vector search's semantic similarity, automatically selecting the optimal approach based on query characteristics. This abstraction allows applications to benefit from sophisticated retrieval without complexity.",
                  "",
                  "---",
                  "",
                  "## Chapter Summary",
                  "",
                  "### What You've Built: Complete GraphRAG System",
                  "",
                  "#### Advanced Graph Architectures",
                  "",
                  "**Modern Graph Systems You've Implemented:**",
                  "",
                  "- \u2705 **NodeRAG Architecture**",
                  "  - Heterogeneous graph system with specialized node types",
                  "  - Three-stage processing pipeline",
                  "  - Optimized for different knowledge structures",
                  "",
                  "- \u2705 **Structured Brain Architecture**",
                  "  - Six specialized node types mimicking human knowledge organization",
                  "  - Concept nodes, entity nodes, relationship nodes, and more",
                  "  - Natural knowledge representation patterns",
                  "",
                  "- \u2705 **Advanced Graph Algorithms**",
                  "  - Personalized PageRank implementation",
                  "  - HNSW similarity integration",
                  "  - Semantic pathway construction",
                  "",
                  "#### Graph Construction Systems",
                  "",
                  "**Knowledge Construction Methods You've Mastered:**",
                  "",
                  "- \u2705 **Traditional GraphRAG**",
                  "  - Knowledge graph construction from unstructured documents",
                  "  - LLM-enhanced entity and relationship extraction",
                  "  - Automated graph building workflows",
                  "",
                  "- \u2705 **Code GraphRAG**",
                  "  - AST parsing for code structure analysis",
                  "  - Call graph analysis for software repositories",
                  "  - Dependency tracking and relationship mapping",
                  "",
                  "- \u2705 **Production Neo4j Integration**",
                  "  - Optimized batch operations for large-scale data",
                  "  - Performance-critical indexing strategies",
                  "  - Enterprise-grade graph database deployment",
                  "",
                  "#### Intelligent Retrieval Systems",
                  "",
                  "**Retrieval Technologies You've Built:**",
                  "",
                  "- \u2705 **Multi-hop Graph Traversal**",
                  "  - Semantic guidance for intelligent path selection",
                  "  - Path ranking and quality assessment",
                  "  - Coherent reasoning pathway construction",
                  "",
                  "- \u2705 **Hybrid Graph-Vector Search**",
                  "  - Adaptive fusion strategies",
                  "  - Graph reasoning combined with vector similarity",
                  "  - Query-aware result combination",
                  "",
                  "### Key Technical Skills Learned",
                  "",
                  "#### Graph Architecture & Algorithms",
                  "",
                  "**Core Architecture Skills:**",
                  "",
                  "1. **NodeRAG Architecture Design**",
                  "   - Heterogeneous graph design principles",
                  "   - Specialized node processing strategies",
                  "   - Three-stage pipeline implementation",
                  "",
                  "2. **Advanced Graph Algorithm Implementation**",
                  "   - Personalized PageRank for semantic ranking",
                  "   - HNSW integration for similarity search",
                  "   - Semantic pathway construction algorithms",
                  "",
                  "3. **Graph Traversal Mastery**",
                  "   - Multi-hop reasoning implementation",
                  "   - Semantic-guided exploration strategies",
                  "   - Coherent path synthesis techniques",
                  "",
                  "#### Knowledge Engineering & Analysis",
                  "",
                  "**Knowledge Processing Skills:**",
                  "",
                  "4. **Knowledge Graph Engineering**",
                  "   - Traditional entity extraction methods",
                  "   - Relationship mapping and validation",
                  "   - Automated graph construction workflows",
                  "",
                  "5. **Code Analysis Expertise**",
                  "   - AST parsing for structural analysis",
                  "   - Dependency analysis and tracking",
                  "   - Call graph construction for software systems",
                  "",
                  "6. **Graph Database Mastery**",
                  "   - Neo4j schema design and optimization",
                  "   - Performance optimization strategies",
                  "   - Batch operation implementation for scale",
                  "",
                  "#### Hybrid Retrieval Systems",
                  "",
                  "**Advanced Retrieval Skills:**",
                  "",
                  "7. **Hybrid System Design**",
                  "   - Graph-vector fusion architecture",
                  "   - Adaptive weighting algorithms",
                  "   - Comprehensive response generation",
                  "",
                  "### Performance Characteristics",
                  "",
                  "#### System Performance Metrics",
                  "",
                  "**Processing Performance:**",
                  "",
                  "- **NodeRAG Processing Pipeline**",
                  "  - Handles 10K+ documents efficiently",
                  "  - Achieves 85-95% pathway coherence",
                  "  - Three-stage processing maintains quality at scale",
                  "",
                  "- **Personalized PageRank Computation**",
                  "  - Sub-100ms semantic pathway computation",
                  "  - Scales to 100K+ heterogeneous graph nodes",
                  "  - Real-time ranking for production systems",
                  "",
                  "- **HNSW Graph Integration**",
                  "  - 200-500ms similarity edge construction",
                  "  - 80-90% type compatibility achieved",
                  "  - Efficient nearest neighbor search integration",
                  "",
                  "#### Retrieval Performance",
                  "",
                  "**Query and Extraction Performance:**",
                  "",
                  "- **Traditional Entity Extraction**",
                  "  - 80-90% precision with LLM-enhanced methods",
                  "  - Automated relationship discovery",
                  "  - High-quality knowledge graph construction",
                  "",
                  "- **Graph Traversal Speed**",
                  "  - Sub-second multi-hop queries",
                  "  - Scales to graphs with 100K+ entities",
                  "  - Efficient path exploration algorithms",
                  "",
                  "- **Hybrid Search Advantage**",
                  "  - **30-40% improvement** in complex query answering",
                  "  - Outperforms pure vector search significantly",
                  "  - Best-in-class results for analytical queries",
                  "",
                  "- **Code Analysis Capability**",
                  "  - Comprehensive repository analysis",
                  "  - Complete relationship extraction",
                  "  - Scalable to large codebases",
                  "",
                  "### When to Choose NodeRAG, GraphRAG, or Vector RAG",
                  "",
                  "#### Use NodeRAG When:",
                  "",
                  "**Ideal for Complex Knowledge Processing:**",
                  "",
                  "- **Complex reasoning** requires understanding different knowledge types",
                  "  - Concepts, entities, and relationships need specialized handling",
                  "  - Multi-faceted knowledge domains",
                  "",
                  "- **Coherent narratives** needed from fragmented information sources",
                  "  - Educational content generation",
                  "  - Research synthesis from multiple documents",
                  "",
                  "- **Educational applications** where knowledge structure understanding is critical",
                  "  - Learning pathway construction",
                  "  - Concept dependency mapping",
                  "",
                  "- **Multi-domain knowledge** needs specialized processing",
                  "  - Technical + business + regulatory information",
                  "  - Cross-domain expertise synthesis",
                  "",
                  "- **Advanced query types** requiring synthesis across knowledge structures",
                  "  - Complex analytical questions",
                  "  - Multi-perspective analysis requirements",
                  "",
                  "#### Use Traditional GraphRAG When:",
                  "",
                  "**Perfect for Relationship-Driven Queries:**",
                  "",
                  "- **Multi-hop reasoning** is required",
                  "  - Example: \"What technologies do Apple's partners' suppliers use?\"",
                  "  - Deep relationship chain exploration",
                  "",
                  "- **Relationship discovery** is key",
                  "  - Example: \"How are these companies connected?\"",
                  "  - Network analysis and connection mapping",
                  "",
                  "- **Comprehensive exploration** needed",
                  "  - Example: \"Find all related information\"",
                  "  - Exhaustive relationship traversal",
                  "",
                  "- **Complex analytical queries**",
                  "  - Example: \"Analyze the supply chain impact of X on Y\"",
                  "  - Multi-step reasoning through relationships",
                  "",
                  "- **Domain has rich entity relationships**",
                  "  - Business networks and partnerships",
                  "  - Scientific literature with citation networks",
                  "  - Code repositories with dependency graphs",
                  "",
                  "#### Use Vector RAG When:",
                  "",
                  "**Optimal for Semantic Similarity Tasks:**",
                  "",
                  "- **Direct factual lookup** queries",
                  "  - Example: \"What is quantum computing?\"",
                  "  - Straightforward information retrieval",
                  "",
                  "- **Semantic similarity** is primary concern",
                  "  - Example: \"Find similar concepts\"",
                  "  - Content recommendation systems",
                  "",
                  "- **Simple Q&A** scenarios",
                  "  - Example: \"When was X founded?\"",
                  "  - Direct fact-based questions",
                  "",
                  "- **Limited relationship structure** in domain",
                  "  - Document collections without explicit connections",
                  "  - Knowledge domains with weak entity relationships",
                  "",
                  "- **Fast response time** is critical",
                  "  - Real-time applications",
                  "  - High-throughput query scenarios",
                  "",
                  "#### Use Hybrid GraphRAG When:",
                  "",
                  "**Best Choice for Production Systems:**",
                  "",
                  "- **Query types vary** significantly",
                  "  - Production systems with diverse users",
                  "  - Multiple use cases in single application",
                  "",
                  "- **Maximum coverage** is needed",
                  "  - Research and analysis scenarios",
                  "  - Comprehensive knowledge exploration",
                  "",
                  "- **Both factual accuracy and insight discovery** are important",
                  "  - Business intelligence applications",
                  "  - Research and development environments",
                  "",
                  "- **You want the best of both worlds**",
                  "  - Most real-world applications",
                  "  - When unsure which approach is optimal",
                  "",
                  "### GraphRAG vs Vector RAG: Concrete Examples",
                  "",
                  "#### Example Query Analysis",
                  "",
                  "**Complex Query Example:**",
                  "\"What are the environmental impacts of technologies used by Apple's automotive partners?\"",
                  "",
                  "#### Vector RAG Approach",
                  "",
                  "**Process:**",
                  "1. Search for \"environmental impacts technologies\"",
                  "2. Search for \"Apple automotive partners\"",
                  "3. Try to connect results manually",
                  "4. Struggle with multi-step reasoning",
                  "",
                  "**Result:**",
                  "- Finds documents about each topic separately",
                  "- **Limitation:** Struggles to connect disparate information",
                  "- **Issue:** Missing relationship-based insights",
                  "",
                  "#### GraphRAG Approach",
                  "",
                  "**Process:**",
                  "1. Find Apple entity in knowledge graph",
                  "2. **First Hop:** Apple \u2192 partners_with \u2192 [Automotive Companies]",
                  "3. **Second Hop:** [Automotive Companies] \u2192 uses_technology \u2192 [Technologies]",
                  "4. **Third Hop:** [Technologies] \u2192 has_environmental_impact \u2192 [Impacts]",
                  "",
                  "**Result:**",
                  "- Discovers specific impact chains through relationships",
                  "- **Advantage:** Finds information no single document contains",
                  "- **Strength:** Multi-hop reasoning reveals hidden connections",
                  "",
                  "#### Hybrid Approach: Best of Both Worlds",
                  "",
                  "**Process:**",
                  "1. **Vector Component:** Understands \"environmental impacts\" semantically",
                  "2. **Graph Component:** Follows the relationship chain systematically",
                  "3. **Fusion:** Combines semantic understanding with relationship reasoning",
                  "4. **Validation:** Cross-references findings from both methods",
                  "",
                  "**Result:**",
                  "- **Coverage:** Best comprehensive coverage of the topic",
                  "- **Accuracy:** Highest accuracy through dual validation",
                  "- **Insight:** Both factual information and relationship insights",
                  "- **Quality:** Superior answer quality for complex queries",
                  "",
                  "---",
                  "",
                  "## \ud83d\udcdd Multiple Choice Test - Session 6",
                  "",
                  "Test your understanding of graph-based RAG systems and GraphRAG implementations.",
                  "",
                  "**Question 1:** What is the primary advantage of GraphRAG over traditional vector-based RAG?  ",
                  "A) Faster query processing  ",
                  "B) Lower computational requirements  ",
                  "C) Multi-hop reasoning through explicit relationship modeling  ",
                  "D) Simpler system architecture  ",
                  "",
                  "**Question 2:** In knowledge graph construction, what is the purpose of entity standardization?  ",
                  "A) To reduce memory usage  ",
                  "B) To merge different mentions of the same entity (e.g., \"Apple Inc.\" and \"Apple\")  ",
                  "C) To improve query speed  ",
                  "D) To compress graph storage  ",
                  "",
                  "**Question 3:** Which graph traversal algorithm is most suitable for finding related entities within a limited number of hops?  ",
                  "A) Depth-First Search (DFS)  ",
                  "B) Breadth-First Search (BFS)  ",
                  "C) Dijkstra's algorithm  ",
                  "D) A* search  ",
                  "",
                  "**Question 4:** In Code GraphRAG, what information is typically extracted from Abstract Syntax Trees (ASTs)?  ",
                  "A) Only function definitions  ",
                  "B) Function calls, imports, class hierarchies, and variable dependencies  ",
                  "C) Only variable names  ",
                  "D) Just file names and sizes  ",
                  "",
                  "**Question 5:** What is the key benefit of hybrid graph-vector search?  ",
                  "A) Reduced computational cost  ",
                  "B) Combining structural relationships with semantic similarity  ",
                  "C) Simpler implementation  ",
                  "D) Faster indexing  ",
                  "",
                  "**Question 6:** When should you choose Neo4j over a simple graph data structure for GraphRAG?  ",
                  "A) Always, regardless of scale  ",
                  "B) When you need persistent storage and complex queries at scale  ",
                  "C) Only for small datasets  ",
                  "D) Never, simple structures are always better  ",
                  "",
                  "**Question 7:** What is the primary challenge in multi-hop graph traversal for RAG?  ",
                  "A) Memory limitations  ",
                  "B) Balancing comprehensiveness with relevance and avoiding information explosion  ",
                  "C) Slow database queries  ",
                  "D) Complex code implementation  ",
                  "",
                  "**Question 8:** In production GraphRAG systems, what is the most important consideration for incremental updates?  ",
                  "A) Minimizing downtime while maintaining graph consistency  ",
                  "B) Reducing storage costs  ",
                  "C) Maximizing query speed  ",
                  "D) Simplifying the codebase  ",
                  "",
                  "[**\ud83d\uddc2\ufe0f View Test Solutions \u2192**](Session6_Test_Solutions.md)",
                  "",
                  "---",
                  "",
                  "## \ud83e\udded Navigation",
                  "",
                  "**Previous:** [Session 5 - RAG Evaluation & Quality Assessment](Session5_RAG_Evaluation_Quality_Assessment.md)",
                  "",
                  "## Optional Deep Dive Modules",
                  "",
                  "- \ud83d\udd2c **[Module A: Advanced Graph Algorithms](Session6_ModuleA_Advanced_Algorithms.md)** - Complex graph traversal and reasoning patterns",
                  "- \ud83c\udfed **[Module B: Production GraphRAG](Session6_ModuleB_Production_Systems.md)** - Enterprise graph database deployment",
                  "",
                  "**Next:** [Session 7 - Agentic RAG Systems \u2192](Session7_Agentic_RAG_Systems.md)",
                  ""
                ],
                "line_count": 366
              }
            ],
            "large_blocks": [
              {
                "start_line": 1060,
                "end_line": 1093,
                "language": "",
                "content": [
                  "",
                  "### Graph Database Integration",
                  "",
                  "### Why Neo4j for Production GraphRAG Systems",
                  "",
                  "While NetworkX is excellent for analysis, production GraphRAG systems require persistent, scalable storage that can handle:",
                  "",
                  "- **Concurrent Access**: Multiple users querying the graph simultaneously",
                  "- **ACID Transactions**: Ensuring data consistency during updates",
                  "- **Optimized Queries**: Cypher query language optimized for graph traversal",
                  "- **Index Performance**: Fast entity lookup and relationship traversal",
                  "- **Scalability**: Handling millions of entities and relationships",
                  "",
                  "The transition from in-memory NetworkX graphs to persistent Neo4j storage marks a crucial evolution in GraphRAG systems. NetworkX serves us well during development and prototyping, but production systems demand the reliability, performance, and concurrent access capabilities that only enterprise graph databases can provide.",
                  "",
                  "### Performance Considerations in Graph Database Design",
                  "",
                  "The key to high-performance GraphRAG lies in thoughtful database design:",
                  "",
                  "1. **Strategic Indexing**: Indices on entity canonical names and types for fast lookup",
                  "2. **Batch Operations**: Bulk inserts minimize transaction overhead",
                  "3. **Query Optimization**: Cypher patterns that leverage graph structure",
                  "4. **Memory Management**: Proper configuration for large graph traversals",
                  "",
                  "These considerations become critical when your knowledge graph grows beyond thousands of entities. A well-indexed Neo4j instance can handle complex graph traversals across millions of nodes in milliseconds, while a poorly designed schema might struggle with simple lookups. The difference lies in understanding how graph databases optimize query execution.",
                  "",
                  "Our Neo4j integration implements production best practices from day one, ensuring your GraphRAG system scales with your knowledge base.",
                  "",
                  "### Production Neo4j Integration",
                  "",
                  "Enterprise graph databases require careful optimization for GraphRAG performance. The following implementation demonstrates how to build a production-ready Neo4j integration that handles the unique requirements of GraphRAG systems - from efficient entity storage to optimized relationship queries.",
                  ""
                ],
                "line_count": 32
              },
              {
                "start_line": 2064,
                "end_line": 2087,
                "language": "",
                "content": [
                  "",
                  "The Cypher query implements sophisticated path discovery with built-in quality filtering. The confidence threshold (0.6) ensures we only traverse high-quality relationships, while the path length constraint prevents exponential explosion. The LIMIT clause caps exploration at 1000 paths to maintain performance, relying on semantic filtering to identify the most relevant subset.",
                  "",
                  "            result = session.run(cypher_query,",
                  "                               start_entity=start_entity,",
                  "                               max_hops=max_hops)",
                  "            ",
                  "            semantic_paths = []",
                  "            processed_paths = 0",
                  "            ",
                  "            for record in result:",
                  "                processed_paths += 1",
                  "                ",
                  "                # Extract path components from Neo4j result",
                  "                entity_path = record['entity_path']",
                  "                relation_path = record['relation_path']",
                  "                confidence_path = record['confidence_path']",
                  "                path_length = record['path_length']",
                  "                entity_types = record['entity_types']",
                  "                ",
                  "                # Convert graph path to natural language representation",
                  "                path_text = self._construct_path_text(entity_path, relation_path)"
                ],
                "line_count": 22
              },
              {
                "start_line": 2227,
                "end_line": 2249,
                "language": "",
                "content": [
                  "",
                  "The final narrative assembly uses contextual connectors to create flowing, readable text from individual relationship statements. This step transforms a sequence of discrete facts into a coherent explanation that language models can effectively process and reason about.",
                  "",
                  "    def _humanize_relation(self, relation_type: str) -> str:",
                  "        \"\"\"Convert technical relation types to human-readable form.\"\"\"",
                  "        ",
                  "        relation_map = {",
                  "            'calls': 'calls',",
                  "            'inherits': 'inherits from', ",
                  "            'uses': 'uses',",
                  "            'contains': 'contains',",
                  "            'implements': 'implements',",
                  "            'founded': 'founded',",
                  "            'works_for': 'works for',",
                  "            'located_in': 'is located in',",
                  "            'part_of': 'is part of',",
                  "            'causes': 'causes',",
                  "            'leads_to': 'leads to'",
                  "        }",
                  "        ",
                  "        return relation_map.get(relation_type, f'is related to via {relation_type}')"
                ],
                "line_count": 21
              },
              {
                "start_line": 2657,
                "end_line": 2680,
                "language": "python",
                "content": [
                  "        # Phase 5: Initialize context collection with adaptive scoring",
                  "        all_contexts = []",
                  "",
                  "        # Process vector contexts with adaptive weights and boosts",
                  "        for ctx in vector_contexts:",
                  "            vector_score = ctx.get('similarity_score', 0.5)",
                  "            ",
                  "            # Apply adaptive weight based on query analysis",
                  "            adapted_score = vector_score * fusion_weights['vector_weight']",
                  "",
                  "            # Apply query-specific boosts for better relevance",
                  "            if query_analysis.get('type') == 'factual' and vector_score > 0.8:",
                  "                adapted_score *= 1.2  # Boost high-confidence factual matches",
                  "",
                  "            all_contexts.append({",
                  "                'content': ctx['content'],",
                  "                'score': adapted_score,",
                  "                'type': 'vector_similarity',",
                  "                'source': ctx.get('metadata', {}).get('source', 'unknown'),",
                  "                'original_score': vector_score,",
                  "                'boost_applied': adapted_score / (vector_score * fusion_weights['vector_weight']) if vector_score > 0 else 1.0",
                  "            })"
                ],
                "line_count": 22
              },
              {
                "start_line": 2737,
                "end_line": 2794,
                "language": "python",
                "content": [
                  "        # Calculate fusion statistics",
                  "        vector_selected = sum(1 for ctx in selected_contexts if ctx['type'] == 'vector_similarity')",
                  "        graph_selected = sum(1 for ctx in selected_contexts if ctx['type'] == 'graph_path')",
                  "",
                  "        return {",
                  "            'contexts': selected_contexts,",
                  "            'fusion_weights': fusion_weights,",
                  "            'query_analysis': query_analysis,",
                  "            'total_candidates': len(all_contexts),",
                  "            'selection_stats': {",
                  "                'vector_contexts_selected': vector_selected,",
                  "                'graph_contexts_selected': graph_selected,",
                  "                'selection_ratio': f\"{vector_selected}/{graph_selected}\" if graph_selected > 0 else f\"{vector_selected}/0\"",
                  "            }",
                  "        }",
                  "",
                  "    def _analyze_query_characteristics(self, query: str) -> Dict[str, Any]:",
                  "        \"\"\"Analyze query to determine optimal retrieval strategy.",
                  "",
                  "        This analysis is crucial for adaptive fusion - different query types",
                  "        benefit from different combinations of vector and graph search:",
                  "",
                  "        **Query Type Analysis:**",
                  "        - **Factual**: Direct lookup queries (\"What is X?\") \u2192 Vector-heavy",
                  "        - **Analytical**: Cause-effect relationships (\"How does X impact Y?\") \u2192 Balanced",
                  "        - **Relational**: Connection queries (\"How is X related to Y?\") \u2192 Graph-heavy",
                  "        - **Comparative**: Multi-entity analysis (\"Compare X and Y\") \u2192 Balanced",
                  "",
                  "        **Complexity Assessment:**",
                  "        - **Simple**: Single-hop, direct answer",
                  "        - **Complex**: Multi-step reasoning, synthesis required",
                  "",
                  "        **Scope Evaluation:**",
                  "        - **Narrow**: Specific entities or facts",
                  "        - **Broad**: General topics or concepts",
                  "",
                  "        The LLM analysis enables dynamic strategy selection rather than static rules.",
                  "        \"\"\"",
                  "",
                  "        analysis_prompt = f\"\"\"",
                  "        As an expert query analyst, analyze this search query to determine the optimal retrieval strategy.",
                  "",
                  "        Query: \"{query}\"",
                  "",
                  "        Analyze the query on these dimensions and return ONLY a JSON response:",
                  "",
                  "        {{",
                  "            \"complexity\": \"simple\" or \"complex\",",
                  "            \"scope\": \"narrow\" or \"broad\",",
                  "            \"type\": \"factual\" or \"analytical\" or \"procedural\" or \"comparative\" or \"relational\",",
                  "            \"graph_benefit\": 0.0-1.0,",
                  "            \"vector_benefit\": 0.0-1.0,",
                  "            \"reasoning_required\": true/false,",
                  "            \"multi_entity\": true/false,",
                  "            \"explanation\": \"Brief explanation of the classification\"",
                  "        }}"
                ],
                "line_count": 56
              },
              {
                "start_line": 3077,
                "end_line": 3444,
                "language": "",
                "content": [
                  "",
                  "The unified search interface provides access to the full power of GraphRAG through a simple API. The hybrid search combines graph traversal's multi-hop reasoning with vector search's semantic similarity, automatically selecting the optimal approach based on query characteristics. This abstraction allows applications to benefit from sophisticated retrieval without complexity.",
                  "",
                  "---",
                  "",
                  "## Chapter Summary",
                  "",
                  "### What You've Built: Complete GraphRAG System",
                  "",
                  "#### Advanced Graph Architectures",
                  "",
                  "**Modern Graph Systems You've Implemented:**",
                  "",
                  "- \u2705 **NodeRAG Architecture**",
                  "  - Heterogeneous graph system with specialized node types",
                  "  - Three-stage processing pipeline",
                  "  - Optimized for different knowledge structures",
                  "",
                  "- \u2705 **Structured Brain Architecture**",
                  "  - Six specialized node types mimicking human knowledge organization",
                  "  - Concept nodes, entity nodes, relationship nodes, and more",
                  "  - Natural knowledge representation patterns",
                  "",
                  "- \u2705 **Advanced Graph Algorithms**",
                  "  - Personalized PageRank implementation",
                  "  - HNSW similarity integration",
                  "  - Semantic pathway construction",
                  "",
                  "#### Graph Construction Systems",
                  "",
                  "**Knowledge Construction Methods You've Mastered:**",
                  "",
                  "- \u2705 **Traditional GraphRAG**",
                  "  - Knowledge graph construction from unstructured documents",
                  "  - LLM-enhanced entity and relationship extraction",
                  "  - Automated graph building workflows",
                  "",
                  "- \u2705 **Code GraphRAG**",
                  "  - AST parsing for code structure analysis",
                  "  - Call graph analysis for software repositories",
                  "  - Dependency tracking and relationship mapping",
                  "",
                  "- \u2705 **Production Neo4j Integration**",
                  "  - Optimized batch operations for large-scale data",
                  "  - Performance-critical indexing strategies",
                  "  - Enterprise-grade graph database deployment",
                  "",
                  "#### Intelligent Retrieval Systems",
                  "",
                  "**Retrieval Technologies You've Built:**",
                  "",
                  "- \u2705 **Multi-hop Graph Traversal**",
                  "  - Semantic guidance for intelligent path selection",
                  "  - Path ranking and quality assessment",
                  "  - Coherent reasoning pathway construction",
                  "",
                  "- \u2705 **Hybrid Graph-Vector Search**",
                  "  - Adaptive fusion strategies",
                  "  - Graph reasoning combined with vector similarity",
                  "  - Query-aware result combination",
                  "",
                  "### Key Technical Skills Learned",
                  "",
                  "#### Graph Architecture & Algorithms",
                  "",
                  "**Core Architecture Skills:**",
                  "",
                  "1. **NodeRAG Architecture Design**",
                  "   - Heterogeneous graph design principles",
                  "   - Specialized node processing strategies",
                  "   - Three-stage pipeline implementation",
                  "",
                  "2. **Advanced Graph Algorithm Implementation**",
                  "   - Personalized PageRank for semantic ranking",
                  "   - HNSW integration for similarity search",
                  "   - Semantic pathway construction algorithms",
                  "",
                  "3. **Graph Traversal Mastery**",
                  "   - Multi-hop reasoning implementation",
                  "   - Semantic-guided exploration strategies",
                  "   - Coherent path synthesis techniques",
                  "",
                  "#### Knowledge Engineering & Analysis",
                  "",
                  "**Knowledge Processing Skills:**",
                  "",
                  "4. **Knowledge Graph Engineering**",
                  "   - Traditional entity extraction methods",
                  "   - Relationship mapping and validation",
                  "   - Automated graph construction workflows",
                  "",
                  "5. **Code Analysis Expertise**",
                  "   - AST parsing for structural analysis",
                  "   - Dependency analysis and tracking",
                  "   - Call graph construction for software systems",
                  "",
                  "6. **Graph Database Mastery**",
                  "   - Neo4j schema design and optimization",
                  "   - Performance optimization strategies",
                  "   - Batch operation implementation for scale",
                  "",
                  "#### Hybrid Retrieval Systems",
                  "",
                  "**Advanced Retrieval Skills:**",
                  "",
                  "7. **Hybrid System Design**",
                  "   - Graph-vector fusion architecture",
                  "   - Adaptive weighting algorithms",
                  "   - Comprehensive response generation",
                  "",
                  "### Performance Characteristics",
                  "",
                  "#### System Performance Metrics",
                  "",
                  "**Processing Performance:**",
                  "",
                  "- **NodeRAG Processing Pipeline**",
                  "  - Handles 10K+ documents efficiently",
                  "  - Achieves 85-95% pathway coherence",
                  "  - Three-stage processing maintains quality at scale",
                  "",
                  "- **Personalized PageRank Computation**",
                  "  - Sub-100ms semantic pathway computation",
                  "  - Scales to 100K+ heterogeneous graph nodes",
                  "  - Real-time ranking for production systems",
                  "",
                  "- **HNSW Graph Integration**",
                  "  - 200-500ms similarity edge construction",
                  "  - 80-90% type compatibility achieved",
                  "  - Efficient nearest neighbor search integration",
                  "",
                  "#### Retrieval Performance",
                  "",
                  "**Query and Extraction Performance:**",
                  "",
                  "- **Traditional Entity Extraction**",
                  "  - 80-90% precision with LLM-enhanced methods",
                  "  - Automated relationship discovery",
                  "  - High-quality knowledge graph construction",
                  "",
                  "- **Graph Traversal Speed**",
                  "  - Sub-second multi-hop queries",
                  "  - Scales to graphs with 100K+ entities",
                  "  - Efficient path exploration algorithms",
                  "",
                  "- **Hybrid Search Advantage**",
                  "  - **30-40% improvement** in complex query answering",
                  "  - Outperforms pure vector search significantly",
                  "  - Best-in-class results for analytical queries",
                  "",
                  "- **Code Analysis Capability**",
                  "  - Comprehensive repository analysis",
                  "  - Complete relationship extraction",
                  "  - Scalable to large codebases",
                  "",
                  "### When to Choose NodeRAG, GraphRAG, or Vector RAG",
                  "",
                  "#### Use NodeRAG When:",
                  "",
                  "**Ideal for Complex Knowledge Processing:**",
                  "",
                  "- **Complex reasoning** requires understanding different knowledge types",
                  "  - Concepts, entities, and relationships need specialized handling",
                  "  - Multi-faceted knowledge domains",
                  "",
                  "- **Coherent narratives** needed from fragmented information sources",
                  "  - Educational content generation",
                  "  - Research synthesis from multiple documents",
                  "",
                  "- **Educational applications** where knowledge structure understanding is critical",
                  "  - Learning pathway construction",
                  "  - Concept dependency mapping",
                  "",
                  "- **Multi-domain knowledge** needs specialized processing",
                  "  - Technical + business + regulatory information",
                  "  - Cross-domain expertise synthesis",
                  "",
                  "- **Advanced query types** requiring synthesis across knowledge structures",
                  "  - Complex analytical questions",
                  "  - Multi-perspective analysis requirements",
                  "",
                  "#### Use Traditional GraphRAG When:",
                  "",
                  "**Perfect for Relationship-Driven Queries:**",
                  "",
                  "- **Multi-hop reasoning** is required",
                  "  - Example: \"What technologies do Apple's partners' suppliers use?\"",
                  "  - Deep relationship chain exploration",
                  "",
                  "- **Relationship discovery** is key",
                  "  - Example: \"How are these companies connected?\"",
                  "  - Network analysis and connection mapping",
                  "",
                  "- **Comprehensive exploration** needed",
                  "  - Example: \"Find all related information\"",
                  "  - Exhaustive relationship traversal",
                  "",
                  "- **Complex analytical queries**",
                  "  - Example: \"Analyze the supply chain impact of X on Y\"",
                  "  - Multi-step reasoning through relationships",
                  "",
                  "- **Domain has rich entity relationships**",
                  "  - Business networks and partnerships",
                  "  - Scientific literature with citation networks",
                  "  - Code repositories with dependency graphs",
                  "",
                  "#### Use Vector RAG When:",
                  "",
                  "**Optimal for Semantic Similarity Tasks:**",
                  "",
                  "- **Direct factual lookup** queries",
                  "  - Example: \"What is quantum computing?\"",
                  "  - Straightforward information retrieval",
                  "",
                  "- **Semantic similarity** is primary concern",
                  "  - Example: \"Find similar concepts\"",
                  "  - Content recommendation systems",
                  "",
                  "- **Simple Q&A** scenarios",
                  "  - Example: \"When was X founded?\"",
                  "  - Direct fact-based questions",
                  "",
                  "- **Limited relationship structure** in domain",
                  "  - Document collections without explicit connections",
                  "  - Knowledge domains with weak entity relationships",
                  "",
                  "- **Fast response time** is critical",
                  "  - Real-time applications",
                  "  - High-throughput query scenarios",
                  "",
                  "#### Use Hybrid GraphRAG When:",
                  "",
                  "**Best Choice for Production Systems:**",
                  "",
                  "- **Query types vary** significantly",
                  "  - Production systems with diverse users",
                  "  - Multiple use cases in single application",
                  "",
                  "- **Maximum coverage** is needed",
                  "  - Research and analysis scenarios",
                  "  - Comprehensive knowledge exploration",
                  "",
                  "- **Both factual accuracy and insight discovery** are important",
                  "  - Business intelligence applications",
                  "  - Research and development environments",
                  "",
                  "- **You want the best of both worlds**",
                  "  - Most real-world applications",
                  "  - When unsure which approach is optimal",
                  "",
                  "### GraphRAG vs Vector RAG: Concrete Examples",
                  "",
                  "#### Example Query Analysis",
                  "",
                  "**Complex Query Example:**",
                  "\"What are the environmental impacts of technologies used by Apple's automotive partners?\"",
                  "",
                  "#### Vector RAG Approach",
                  "",
                  "**Process:**",
                  "1. Search for \"environmental impacts technologies\"",
                  "2. Search for \"Apple automotive partners\"",
                  "3. Try to connect results manually",
                  "4. Struggle with multi-step reasoning",
                  "",
                  "**Result:**",
                  "- Finds documents about each topic separately",
                  "- **Limitation:** Struggles to connect disparate information",
                  "- **Issue:** Missing relationship-based insights",
                  "",
                  "#### GraphRAG Approach",
                  "",
                  "**Process:**",
                  "1. Find Apple entity in knowledge graph",
                  "2. **First Hop:** Apple \u2192 partners_with \u2192 [Automotive Companies]",
                  "3. **Second Hop:** [Automotive Companies] \u2192 uses_technology \u2192 [Technologies]",
                  "4. **Third Hop:** [Technologies] \u2192 has_environmental_impact \u2192 [Impacts]",
                  "",
                  "**Result:**",
                  "- Discovers specific impact chains through relationships",
                  "- **Advantage:** Finds information no single document contains",
                  "- **Strength:** Multi-hop reasoning reveals hidden connections",
                  "",
                  "#### Hybrid Approach: Best of Both Worlds",
                  "",
                  "**Process:**",
                  "1. **Vector Component:** Understands \"environmental impacts\" semantically",
                  "2. **Graph Component:** Follows the relationship chain systematically",
                  "3. **Fusion:** Combines semantic understanding with relationship reasoning",
                  "4. **Validation:** Cross-references findings from both methods",
                  "",
                  "**Result:**",
                  "- **Coverage:** Best comprehensive coverage of the topic",
                  "- **Accuracy:** Highest accuracy through dual validation",
                  "- **Insight:** Both factual information and relationship insights",
                  "- **Quality:** Superior answer quality for complex queries",
                  "",
                  "---",
                  "",
                  "## \ud83d\udcdd Multiple Choice Test - Session 6",
                  "",
                  "Test your understanding of graph-based RAG systems and GraphRAG implementations.",
                  "",
                  "**Question 1:** What is the primary advantage of GraphRAG over traditional vector-based RAG?  ",
                  "A) Faster query processing  ",
                  "B) Lower computational requirements  ",
                  "C) Multi-hop reasoning through explicit relationship modeling  ",
                  "D) Simpler system architecture  ",
                  "",
                  "**Question 2:** In knowledge graph construction, what is the purpose of entity standardization?  ",
                  "A) To reduce memory usage  ",
                  "B) To merge different mentions of the same entity (e.g., \"Apple Inc.\" and \"Apple\")  ",
                  "C) To improve query speed  ",
                  "D) To compress graph storage  ",
                  "",
                  "**Question 3:** Which graph traversal algorithm is most suitable for finding related entities within a limited number of hops?  ",
                  "A) Depth-First Search (DFS)  ",
                  "B) Breadth-First Search (BFS)  ",
                  "C) Dijkstra's algorithm  ",
                  "D) A* search  ",
                  "",
                  "**Question 4:** In Code GraphRAG, what information is typically extracted from Abstract Syntax Trees (ASTs)?  ",
                  "A) Only function definitions  ",
                  "B) Function calls, imports, class hierarchies, and variable dependencies  ",
                  "C) Only variable names  ",
                  "D) Just file names and sizes  ",
                  "",
                  "**Question 5:** What is the key benefit of hybrid graph-vector search?  ",
                  "A) Reduced computational cost  ",
                  "B) Combining structural relationships with semantic similarity  ",
                  "C) Simpler implementation  ",
                  "D) Faster indexing  ",
                  "",
                  "**Question 6:** When should you choose Neo4j over a simple graph data structure for GraphRAG?  ",
                  "A) Always, regardless of scale  ",
                  "B) When you need persistent storage and complex queries at scale  ",
                  "C) Only for small datasets  ",
                  "D) Never, simple structures are always better  ",
                  "",
                  "**Question 7:** What is the primary challenge in multi-hop graph traversal for RAG?  ",
                  "A) Memory limitations  ",
                  "B) Balancing comprehensiveness with relevance and avoiding information explosion  ",
                  "C) Slow database queries  ",
                  "D) Complex code implementation  ",
                  "",
                  "**Question 8:** In production GraphRAG systems, what is the most important consideration for incremental updates?  ",
                  "A) Minimizing downtime while maintaining graph consistency  ",
                  "B) Reducing storage costs  ",
                  "C) Maximizing query speed  ",
                  "D) Simplifying the codebase  ",
                  "",
                  "[**\ud83d\uddc2\ufe0f View Test Solutions \u2192**](Session6_Test_Solutions.md)",
                  "",
                  "---",
                  "",
                  "## \ud83e\udded Navigation",
                  "",
                  "**Previous:** [Session 5 - RAG Evaluation & Quality Assessment](Session5_RAG_Evaluation_Quality_Assessment.md)",
                  "",
                  "## Optional Deep Dive Modules",
                  "",
                  "- \ud83d\udd2c **[Module A: Advanced Graph Algorithms](Session6_ModuleA_Advanced_Algorithms.md)** - Complex graph traversal and reasoning patterns",
                  "- \ud83c\udfed **[Module B: Production GraphRAG](Session6_ModuleB_Production_Systems.md)** - Enterprise graph database deployment",
                  "",
                  "**Next:** [Session 7 - Agentic RAG Systems \u2192](Session7_Agentic_RAG_Systems.md)",
                  ""
                ],
                "line_count": 366
              }
            ],
            "needs_refactoring": true
          }
        ]
      },
      "script": "/Users/q284340/Agentic/nano-degree/scripts/detect-large-code-blocks.py"
    },
    "check_markdown_formatting": {
      "success": true,
      "data": {
        "summary": {
          "total_files": 1,
          "files_with_issues": 1,
          "total_issues": 1
        },
        "files": [
          {
            "file": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session6_Graph_Based_RAG.md",
            "total_issues": 1,
            "issues": [
              {
                "type": "unclosed_code_block",
                "line": 3446,
                "message": "Code block starting at line 3446 is never closed",
                "suggestion": "Add closing ``` marker"
              }
            ],
            "needs_fixing": true
          }
        ]
      },
      "script": "/Users/q284340/Agentic/nano-degree/scripts/check-markdown-formatting.py"
    },
    "detect_insufficient_explanations": {
      "success": true,
      "data": {
        "summary": {
          "total_files": 1,
          "files_needing_improvement": 1,
          "total_issues": 28,
          "average_quality_score": 82.82208588957054
        },
        "files": [
          {
            "total_code_blocks": 163,
            "total_issues": 28,
            "issues": [
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 94,
                "code_block_size": 15,
                "word_count": 5,
                "message": "Code block (15 lines) at line 75 has insufficient explanation (5 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 94,
                "code_block_size": 15,
                "message": "Code block at line 75 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 133,
                "code_block_size": 8,
                "word_count": 5,
                "message": "Code block (8 lines) at line 122 has insufficient explanation (5 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 133,
                "code_block_size": 8,
                "message": "Code block at line 122 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 181,
                "code_block_size": 9,
                "word_count": 8,
                "message": "Code block (9 lines) at line 169 has insufficient explanation (8 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 181,
                "code_block_size": 9,
                "message": "Code block at line 169 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 289,
                "code_block_size": 13,
                "word_count": 7,
                "message": "Code block (13 lines) at line 272 has insufficient explanation (7 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 289,
                "code_block_size": 13,
                "message": "Code block at line 272 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 412,
                "code_block_size": 12,
                "word_count": 7,
                "message": "Code block (12 lines) at line 396 has insufficient explanation (7 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 684,
                "code_block_size": 14,
                "word_count": 6,
                "message": "Code block (14 lines) at line 668 has insufficient explanation (6 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 684,
                "code_block_size": 14,
                "message": "Code block at line 668 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 821,
                "code_block_size": 10,
                "word_count": 1,
                "message": "Code block (10 lines) at line 808 has insufficient explanation (1 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 821,
                "code_block_size": 10,
                "message": "Code block at line 808 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 1094,
                "code_block_size": 19,
                "word_count": 6,
                "message": "Code block (19 lines) at line 1060 has insufficient explanation (6 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 1094,
                "code_block_size": 19,
                "message": "Code block at line 1060 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 1988,
                "code_block_size": 6,
                "message": "Code block at line 1978 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 2558,
                "code_block_size": 19,
                "word_count": 6,
                "message": "Code block (19 lines) at line 2537 has insufficient explanation (6 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 2558,
                "code_block_size": 19,
                "message": "Code block at line 2537 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 2625,
                "code_block_size": 14,
                "word_count": 5,
                "message": "Code block (14 lines) at line 2605 has insufficient explanation (5 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 2625,
                "code_block_size": 14,
                "message": "Code block at line 2605 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 2652,
                "code_block_size": 18,
                "word_count": 6,
                "message": "Code block (18 lines) at line 2630 has insufficient explanation (6 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 2652,
                "code_block_size": 18,
                "message": "Code block at line 2630 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 2681,
                "code_block_size": 18,
                "word_count": 7,
                "message": "Code block (18 lines) at line 2657 has insufficient explanation (7 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 2681,
                "code_block_size": 18,
                "message": "Code block at line 2657 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "generic_explanation",
                "severity": "medium",
                "line": 2862,
                "code_block_size": 8,
                "message": "Code block at line 2850 has generic/lazy explanation",
                "suggestion": "Replace generic phrases with specific, educational explanations"
              },
              {
                "type": "insufficient_explanation",
                "severity": "high",
                "line": 3445,
                "code_block_size": 263,
                "word_count": 1,
                "message": "Code block (263 lines) at line 3077 has insufficient explanation (1 words)",
                "suggestion": "Expand explanation to at least 30-50 words covering functionality, purpose, and educational insights"
              },
              {
                "type": "lacks_educational_context",
                "severity": "medium",
                "line": 3445,
                "code_block_size": 263,
                "message": "Code block at line 3077 explanation lacks educational context",
                "suggestion": "Add context explaining WHY this approach is used and HOW it benefits the student"
              },
              {
                "type": "consecutive_code_blocks",
                "severity": "medium",
                "line": 820,
                "message": "Consecutive code blocks at lines 808 and 822 without intermediate explanation",
                "suggestion": "Add transitional explanation between code blocks to maintain narrative flow"
              }
            ],
            "needs_improvement": true,
            "quality_score": 82.82208588957054,
            "file": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session6_Graph_Based_RAG.md"
          }
        ]
      },
      "script": "/Users/q284340/Agentic/nano-degree/scripts/detect-insufficient-explanations.py"
    }
  },
  "priority_files": [
    {
      "file": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session6_Graph_Based_RAG.md",
      "file_name": "Session6_Graph_Based_RAG.md",
      "priority_score": 63.43558282208589,
      "issues": [
        "6 large code blocks",
        "82.8% explanation quality"
      ]
    }
  ]
}