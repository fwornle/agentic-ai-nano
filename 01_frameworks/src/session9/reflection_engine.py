"""
Reflection Engine for Continuous Agent Improvement
Session 8: Self-Improvement and Adaptive Learning Patterns

This module implements sophisticated reflection patterns that enable agents
to learn from experience, identify improvement opportunities, and adapt
their strategies for enhanced performance over time.
"""

from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import logging
import json
import statistics
from datetime import datetime, timedelta
from collections import defaultdict, deque
import uuid

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ReflectionType(Enum):
    """Types of reflection processes"""
    PERFORMANCE = "performance"
    STRATEGY = "strategy"
    LEARNING = "learning"
    ERROR_ANALYSIS = "error_analysis"
    PATTERN_RECOGNITION = "pattern_recognition"
    ADAPTATION = "adaptation"
    META_COGNITION = "meta_cognition"


class InsightCategory(Enum):
    """Categories of insights generated through reflection"""
    SUCCESS_PATTERN = "success_pattern"
    FAILURE_PATTERN = "failure_pattern"
    EFFICIENCY_OPPORTUNITY = "efficiency_opportunity"
    RISK_MITIGATION = "risk_mitigation"
    STRATEGY_REFINEMENT = "strategy_refinement"
    CAPABILITY_GAP = "capability_gap"
    OPTIMIZATION_POTENTIAL = "optimization_potential"


class AdaptationPriority(Enum):
    """Priority levels for adaptations"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    EXPERIMENTAL = "experimental"


@dataclass
class ExecutionExperience:
    """Represents an execution experience for reflection"""
    experience_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    task_description: str = ""
    execution_context: Dict[str, Any] = field(default_factory=dict)
    execution_steps: List[Dict[str, Any]] = field(default_factory=list)
    outcome: Dict[str, Any] = field(default_factory=dict)
    performance_metrics: Dict[str, float] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)
    duration: timedelta = field(default=timedelta(0))
    success: bool = False
    error_messages: List[str] = field(default_factory=list)
    resource_usage: Dict[str, Any] = field(default_factory=dict)
    agent_actions: List[Dict[str, Any]] = field(default_factory=list)
    environmental_factors: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert experience to dictionary"""
        return {
            'experience_id': self.experience_id,
            'task_description': self.task_description,
            'execution_context': self.execution_context,
            'outcome': self.outcome,
            'performance_metrics': self.performance_metrics,
            'timestamp': self.timestamp.isoformat(),
            'duration': self.duration.total_seconds(),
            'success': self.success,
            'error_messages': self.error_messages,
            'resource_usage': self.resource_usage
        }


@dataclass
class ReflectionInsight:
    """Represents an insight generated through reflection"""
    insight_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    category: InsightCategory = InsightCategory.SUCCESS_PATTERN
    title: str = ""
    description: str = ""
    supporting_evidence: List[str] = field(default_factory=list)
    confidence_score: float = 0.0
    applicability_context: Dict[str, Any] = field(default_factory=dict)
    potential_impact: float = 0.0
    generated_at: datetime = field(default_factory=datetime.now)
    validation_status: str = "pending"  # pending, validated, rejected
    related_experiences: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert insight to dictionary"""
        return {
            'insight_id': self.insight_id,
            'category': self.category.value,
            'title': self.title,
            'description': self.description,
            'confidence_score': self.confidence_score,
            'potential_impact': self.potential_impact,
            'generated_at': self.generated_at.isoformat(),
            'validation_status': self.validation_status,
            'supporting_evidence': self.supporting_evidence
        }


@dataclass
class AdaptationRecommendation:
    """Represents a recommended adaptation based on reflection"""
    recommendation_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    adaptation_type: str = ""
    priority: AdaptationPriority = AdaptationPriority.MEDIUM
    description: str = ""
    specific_actions: List[str] = field(default_factory=list)
    expected_benefits: List[str] = field(default_factory=list)
    implementation_complexity: str = "medium"  # low, medium, high
    estimated_effort: timedelta = field(default_factory=lambda: timedelta(hours=1))
    risk_assessment: Dict[str, Any] = field(default_factory=dict)
    success_metrics: List[str] = field(default_factory=list)
    supporting_insights: List[str] = field(default_factory=list)
    implementation_status: str = "proposed"  # proposed, approved, implementing, completed, rejected
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert recommendation to dictionary"""
        return {
            'recommendation_id': self.recommendation_id,
            'adaptation_type': self.adaptation_type,
            'priority': self.priority.value,
            'description': self.description,
            'specific_actions': self.specific_actions,
            'expected_benefits': self.expected_benefits,
            'implementation_complexity': self.implementation_complexity,
            'estimated_effort': self.estimated_effort.total_seconds(),
            'implementation_status': self.implementation_status
        }


class ReflectionEngine:
    """
    Advanced reflection engine for continuous agent improvement through
    experience analysis, pattern recognition, and adaptive learning.
    """
    
    def __init__(self, agent_id: str, reflection_config: Optional[Dict[str, Any]] = None):
        self.agent_id = agent_id
        self.config = reflection_config or self._default_config()
        
        # Experience storage and management
        self.experience_buffer: deque = deque(maxlen=self.config['max_experiences'])
        self.insights_database: List[ReflectionInsight] = []
        self.adaptation_recommendations: List[AdaptationRecommendation] = []
        
        # Learning and pattern recognition
        self.pattern_detector = PatternDetector()
        self.performance_analyzer = PerformanceAnalyzer()
        self.strategy_optimizer = StrategyOptimizer()
        
        # Reflection statistics
        self.reflection_stats = {
            'total_reflections': 0,
            'insights_generated': 0,
            'adaptations_proposed': 0,
            'adaptations_implemented': 0,
            'performance_improvements': 0,
            'reflection_time_total': timedelta(0)
        }
        
        logger.info(f"Reflection engine initialized for agent {agent_id}")
    
    def _default_config(self) -> Dict[str, Any]:
        """Default configuration for reflection engine"""
        return {
            'max_experiences': 1000,
            'reflection_frequency': timedelta(hours=1),
            'min_experiences_for_pattern': 5,
            'confidence_threshold': 0.7,
            'adaptation_threshold': 0.8,
            'enable_meta_cognition': True,
            'enable_continuous_learning': True
        }
    
    async def reflect_on_execution(self, experience: ExecutionExperience) -> Dict[str, Any]:
        """
        Conduct comprehensive reflection on execution experience
        
        Args:
            experience: The execution experience to reflect on
            
        Returns:
            Dictionary containing reflection results and insights
        """
        reflection_start = datetime.now()
        self.reflection_stats['total_reflections'] += 1
        
        logger.info(f"Starting reflection on experience: {experience.experience_id}")
        
        # Store experience
        self.experience_buffer.append(experience)
        
        # Multi-phase reflection process
        reflection_results = {}
        
        # Phase 1: Individual experience analysis
        individual_analysis = await self._analyze_individual_experience(experience)
        reflection_results['individual_analysis'] = individual_analysis
        
        # Phase 2: Pattern identification across experiences
        if len(self.experience_buffer) >= self.config['min_experiences_for_pattern']:
            pattern_analysis = await self._identify_cross_experience_patterns(experience)
            reflection_results['pattern_analysis'] = pattern_analysis
        else:
            reflection_results['pattern_analysis'] = {'patterns': [], 'insufficient_data': True}
        
        # Phase 3: Performance trend analysis
        performance_analysis = await self._analyze_performance_trends(experience)
        reflection_results['performance_analysis'] = performance_analysis
        
        # Phase 4: Strategy effectiveness evaluation
        strategy_analysis = await self._evaluate_strategy_effectiveness(experience)
        reflection_results['strategy_analysis'] = strategy_analysis
        
        # Phase 5: Generate insights and recommendations
        insights = await self._generate_insights_from_analysis([
            individual_analysis, 
            reflection_results['pattern_analysis'],
            performance_analysis,
            strategy_analysis
        ])
        reflection_results['insights'] = insights
        
        # Phase 6: Create adaptation recommendations
        recommendations = await self._create_adaptation_recommendations(insights)
        reflection_results['recommendations'] = recommendations
        
        # Phase 7: Meta-cognitive reflection (if enabled)
        if self.config['enable_meta_cognition']:
            meta_reflection = await self._conduct_meta_reflection(reflection_results)
            reflection_results['meta_reflection'] = meta_reflection
        
        # Update statistics
        reflection_duration = datetime.now() - reflection_start
        self.reflection_stats['reflection_time_total'] += reflection_duration
        
        # Store insights and recommendations
        self.insights_database.extend(insights)
        self.adaptation_recommendations.extend(recommendations)
        
        logger.info(f"Reflection completed in {reflection_duration.total_seconds():.2f}s")
        logger.info(f"Generated {len(insights)} insights and {len(recommendations)} recommendations")
        
        return {
            'reflection_id': str(uuid.uuid4()),
            'agent_id': self.agent_id,
            'experience_id': experience.experience_id,
            'reflection_duration': reflection_duration,
            'results': reflection_results,
            'summary': await self._create_reflection_summary(reflection_results)
        }
    
    async def _analyze_individual_experience(self, experience: ExecutionExperience) -> Dict[str, Any]:
        """Analyze a single execution experience in detail"""
        
        analysis = {
            'success_factors': [],
            'failure_factors': [],
            'efficiency_metrics': {},
            'resource_utilization': {},
            'decision_quality': {},
            'learning_opportunities': []
        }
        
        # Success/failure factor analysis
        if experience.success:
            analysis['success_factors'] = await self._identify_success_factors(experience)
        else:
            analysis['failure_factors'] = await self._identify_failure_factors(experience)
        
        # Efficiency analysis
        analysis['efficiency_metrics'] = await self._calculate_efficiency_metrics(experience)
        
        # Resource utilization analysis
        analysis['resource_utilization'] = await self._analyze_resource_utilization(experience)
        
        # Decision quality assessment
        analysis['decision_quality'] = await self._assess_decision_quality(experience)
        
        # Identify learning opportunities
        analysis['learning_opportunities'] = await self._identify_learning_opportunities(experience)
        
        return analysis
    
    async def _identify_success_factors(self, experience: ExecutionExperience) -> List[Dict[str, Any]]:
        """Identify factors that contributed to successful execution"""
        
        success_factors = []
        
        # Analyze performance metrics
        for metric, value in experience.performance_metrics.items():
            if value > 0.8:  # High performance threshold
                success_factors.append({
                    'factor': f'high_{metric}',
                    'value': value,
                    'contribution': 'performance_excellence'
                })
        
        # Analyze execution steps for successful patterns
        efficient_steps = [
            step for step in experience.execution_steps
            if step.get('duration', float('inf')) < 30  # Quick execution
            and step.get('success', False)
        ]
        
        if len(efficient_steps) / len(experience.execution_steps) > 0.8:
            success_factors.append({
                'factor': 'efficient_execution_pattern',
                'value': len(efficient_steps) / len(experience.execution_steps),
                'contribution': 'operational_efficiency'
            })
        
        # Resource efficiency analysis
        if experience.resource_usage:
            total_resources_used = sum(experience.resource_usage.values())
            if total_resources_used < experience.execution_context.get('resource_budget', float('inf')):
                success_factors.append({
                    'factor': 'resource_efficiency',
                    'value': total_resources_used,
                    'contribution': 'cost_optimization'
                })
        
        return success_factors
    
    async def _identify_failure_factors(self, experience: ExecutionExperience) -> List[Dict[str, Any]]:
        """Identify factors that contributed to execution failure"""
        
        failure_factors = []
        
        # Error message analysis
        for error_msg in experience.error_messages:
            failure_factors.append({
                'factor': 'error_occurrence',
                'error_type': self._classify_error_type(error_msg),
                'error_message': error_msg,
                'impact': 'execution_failure'
            })
        
        # Performance degradation analysis
        for metric, value in experience.performance_metrics.items():
            if value < 0.3:  # Low performance threshold
                failure_factors.append({
                    'factor': f'low_{metric}',
                    'value': value,
                    'impact': 'performance_degradation'
                })
        
        # Resource constraint analysis
        if experience.resource_usage:
            resource_budget = experience.execution_context.get('resource_budget', {})
            for resource, used in experience.resource_usage.items():
                budget = resource_budget.get(resource, float('inf'))
                if used > budget * 1.2:  # 20% over budget
                    failure_factors.append({
                        'factor': 'resource_overrun',
                        'resource': resource,
                        'used': used,
                        'budget': budget,
                        'impact': 'constraint_violation'
                    })
        
        return failure_factors
    
    def _classify_error_type(self, error_message: str) -> str:
        """Classify error messages into categories"""
        
        error_msg_lower = error_message.lower()
        
        if 'timeout' in error_msg_lower or 'time' in error_msg_lower:
            return 'timeout_error'
        elif 'resource' in error_msg_lower or 'memory' in error_msg_lower:
            return 'resource_error'
        elif 'permission' in error_msg_lower or 'access' in error_msg_lower:
            return 'permission_error'
        elif 'network' in error_msg_lower or 'connection' in error_msg_lower:
            return 'network_error'
        elif 'validation' in error_msg_lower or 'invalid' in error_msg_lower:
            return 'validation_error'
        else:
            return 'generic_error'
    
    async def _calculate_efficiency_metrics(self, experience: ExecutionExperience) -> Dict[str, float]:
        """Calculate various efficiency metrics for the execution"""
        
        metrics = {}
        
        # Time efficiency
        estimated_duration = experience.execution_context.get('estimated_duration', experience.duration)
        if estimated_duration and estimated_duration.total_seconds() > 0:
            metrics['time_efficiency'] = min(1.0, estimated_duration.total_seconds() / experience.duration.total_seconds())
        
        # Success rate of steps
        if experience.execution_steps:
            successful_steps = sum(1 for step in experience.execution_steps if step.get('success', False))
            metrics['step_success_rate'] = successful_steps / len(experience.execution_steps)
        
        # Resource efficiency
        if experience.resource_usage:
            total_available = sum(experience.execution_context.get('available_resources', {}).values())
            total_used = sum(experience.resource_usage.values())
            if total_available > 0:
                metrics['resource_efficiency'] = 1.0 - (total_used / total_available)
        
        # Overall performance score
        if experience.performance_metrics:
            metrics['overall_performance'] = statistics.mean(experience.performance_metrics.values())
        
        return metrics
    
    async def _analyze_resource_utilization(self, experience: ExecutionExperience) -> Dict[str, Any]:
        """Analyze resource utilization patterns"""
        
        utilization_analysis = {
            'efficiency_score': 0.0,
            'bottlenecks': [],
            'optimization_opportunities': [],
            'usage_patterns': {}
        }
        
        if not experience.resource_usage:
            return utilization_analysis
        
        available_resources = experience.execution_context.get('available_resources', {})
        
        for resource, used in experience.resource_usage.items():
            available = available_resources.get(resource, used)
            utilization_rate = used / available if available > 0 else 1.0
            
            utilization_analysis['usage_patterns'][resource] = {
                'used': used,
                'available': available,
                'utilization_rate': utilization_rate
            }
            
            # Identify bottlenecks (>90% utilization)
            if utilization_rate > 0.9:
                utilization_analysis['bottlenecks'].append({
                    'resource': resource,
                    'utilization_rate': utilization_rate
                })
            
            # Identify optimization opportunities (<50% utilization)
            elif utilization_rate < 0.5:
                utilization_analysis['optimization_opportunities'].append({
                    'resource': resource,
                    'utilization_rate': utilization_rate,
                    'suggestion': 'reduce_allocation'
                })
        
        # Calculate overall efficiency score
        if utilization_analysis['usage_patterns']:
            rates = [p['utilization_rate'] for p in utilization_analysis['usage_patterns'].values()]
            # Optimal utilization is around 70-80%
            optimal_rates = [abs(rate - 0.75) for rate in rates]
            utilization_analysis['efficiency_score'] = 1.0 - statistics.mean(optimal_rates)
        
        return utilization_analysis
    
    async def _assess_decision_quality(self, experience: ExecutionExperience) -> Dict[str, Any]:
        """Assess the quality of decisions made during execution"""
        
        decision_assessment = {
            'decision_points': [],
            'overall_quality_score': 0.0,
            'improvement_suggestions': []
        }
        
        # Analyze agent actions as decision points
        for action in experience.agent_actions:
            decision_point = {
                'action': action.get('action', 'unknown'),
                'context': action.get('context', {}),
                'outcome': action.get('outcome', 'unknown'),
                'quality_score': 0.0
            }
            
            # Score decision based on outcome
            if action.get('success', False):
                decision_point['quality_score'] = 0.8
                if action.get('efficiency', 0) > 0.8:
                    decision_point['quality_score'] = 1.0
            else:
                decision_point['quality_score'] = 0.2
                if action.get('recoverable', False):
                    decision_point['quality_score'] = 0.4
            
            decision_assessment['decision_points'].append(decision_point)
        
        # Calculate overall quality score
        if decision_assessment['decision_points']:
            scores = [dp['quality_score'] for dp in decision_assessment['decision_points']]
            decision_assessment['overall_quality_score'] = statistics.mean(scores)
        
        # Generate improvement suggestions
        low_quality_decisions = [
            dp for dp in decision_assessment['decision_points']
            if dp['quality_score'] < 0.5
        ]
        
        if low_quality_decisions:
            decision_assessment['improvement_suggestions'].append(
                f"Review and improve decision-making for actions: {[dp['action'] for dp in low_quality_decisions]}"
            )
        
        return decision_assessment
    
    async def _identify_learning_opportunities(self, experience: ExecutionExperience) -> List[Dict[str, Any]]:
        """Identify opportunities for learning and improvement"""
        
        opportunities = []
        
        # Skill gap identification
        if not experience.success:
            opportunities.append({
                'type': 'skill_improvement',
                'description': 'Failed execution suggests need for skill enhancement',
                'specific_areas': [error.split(':')[0] for error in experience.error_messages],
                'priority': AdaptationPriority.HIGH.value
            })
        
        # Efficiency improvement opportunities
        efficiency_metrics = await self._calculate_efficiency_metrics(experience)
        for metric, value in efficiency_metrics.items():
            if value < 0.6:  # Below acceptable threshold
                opportunities.append({
                    'type': 'efficiency_improvement',
                    'description': f'Low {metric} suggests optimization potential',
                    'metric': metric,
                    'current_value': value,
                    'target_value': 0.8,
                    'priority': AdaptationPriority.MEDIUM.value
                })
        
        # Strategy refinement opportunities
        if len(experience.execution_steps) > 10:  # Complex execution
            opportunities.append({
                'type': 'strategy_simplification',
                'description': 'Complex execution pattern suggests need for strategy simplification',
                'current_complexity': len(experience.execution_steps),
                'priority': AdaptationPriority.MEDIUM.value
            })
        
        return opportunities
    
    async def _identify_cross_experience_patterns(self, current_experience: ExecutionExperience) -> Dict[str, Any]:
        """Identify patterns across multiple experiences"""
        
        if len(self.experience_buffer) < self.config['min_experiences_for_pattern']:
            return {'patterns': [], 'insufficient_data': True}
        
        pattern_analysis = {
            'success_patterns': [],
            'failure_patterns': [],
            'efficiency_patterns': [],
            'resource_patterns': [],
            'temporal_patterns': []
        }
        
        experiences = list(self.experience_buffer)
        
        # Success/failure patterns
        pattern_analysis['success_patterns'] = await self.pattern_detector.detect_success_patterns(experiences)
        pattern_analysis['failure_patterns'] = await self.pattern_detector.detect_failure_patterns(experiences)
        
        # Efficiency patterns
        pattern_analysis['efficiency_patterns'] = await self.pattern_detector.detect_efficiency_patterns(experiences)
        
        # Resource usage patterns
        pattern_analysis['resource_patterns'] = await self.pattern_detector.detect_resource_patterns(experiences)
        
        # Temporal patterns
        pattern_analysis['temporal_patterns'] = await self.pattern_detector.detect_temporal_patterns(experiences)
        
        return pattern_analysis
    
    async def _analyze_performance_trends(self, current_experience: ExecutionExperience) -> Dict[str, Any]:
        """Analyze performance trends over time"""
        
        return await self.performance_analyzer.analyze_trends(
            list(self.experience_buffer), current_experience
        )
    
    async def _evaluate_strategy_effectiveness(self, current_experience: ExecutionExperience) -> Dict[str, Any]:
        """Evaluate the effectiveness of current strategies"""
        
        return await self.strategy_optimizer.evaluate_effectiveness(
            list(self.experience_buffer), current_experience
        )
    
    async def _generate_insights_from_analysis(self, analyses: List[Dict[str, Any]]) -> List[ReflectionInsight]:
        """Generate insights from various analyses"""
        
        insights = []
        
        for analysis in analyses:
            if not analysis or analysis.get('insufficient_data'):
                continue
            
            # Generate insights from success patterns
            if 'success_factors' in analysis:
                for factor in analysis['success_factors']:
                    insight = ReflectionInsight(
                        category=InsightCategory.SUCCESS_PATTERN,
                        title=f"Success Factor: {factor['factor']}",
                        description=f"Factor {factor['factor']} contributes to {factor['contribution']}",
                        confidence_score=min(1.0, factor['value']) if isinstance(factor['value'], (int, float)) else 0.7,
                        potential_impact=0.8,
                        supporting_evidence=[json.dumps(factor)]
                    )
                    insights.append(insight)
            
            # Generate insights from failure patterns
            if 'failure_factors' in analysis:
                for factor in analysis['failure_factors']:
                    insight = ReflectionInsight(
                        category=InsightCategory.FAILURE_PATTERN,
                        title=f"Failure Factor: {factor['factor']}",
                        description=f"Factor {factor['factor']} leads to {factor['impact']}",
                        confidence_score=0.8,
                        potential_impact=0.9,
                        supporting_evidence=[json.dumps(factor)]
                    )
                    insights.append(insight)
            
            # Generate insights from efficiency metrics
            if 'efficiency_metrics' in analysis:
                for metric, value in analysis['efficiency_metrics'].items():
                    if value < 0.5:  # Low efficiency
                        insight = ReflectionInsight(
                            category=InsightCategory.EFFICIENCY_OPPORTUNITY,
                            title=f"Low {metric}",
                            description=f"Current {metric} of {value:.2f} indicates optimization potential",
                            confidence_score=0.7,
                            potential_impact=0.6,
                            supporting_evidence=[f"{metric}: {value}"]
                        )
                        insights.append(insight)
        
        # Filter insights by confidence threshold
        filtered_insights = [
            insight for insight in insights
            if insight.confidence_score >= self.config['confidence_threshold']
        ]
        
        self.reflection_stats['insights_generated'] += len(filtered_insights)
        
        return filtered_insights
    
    async def _create_adaptation_recommendations(self, insights: List[ReflectionInsight]) -> List[AdaptationRecommendation]:
        """Create adaptation recommendations based on insights"""
        
        recommendations = []
        
        # Group insights by category
        insights_by_category = defaultdict(list)
        for insight in insights:
            insights_by_category[insight.category].append(insight)
        
        # Create recommendations for each category
        for category, category_insights in insights_by_category.items():
            if category == InsightCategory.FAILURE_PATTERN:
                recommendation = AdaptationRecommendation(
                    adaptation_type="failure_prevention",
                    priority=AdaptationPriority.HIGH,
                    description=f"Address {len(category_insights)} identified failure patterns",
                    specific_actions=[
                        f"Implement mitigation for {insight.title}" for insight in category_insights
                    ],
                    expected_benefits=["Reduced failure rate", "Improved reliability"],
                    implementation_complexity="medium",
                    supporting_insights=[insight.insight_id for insight in category_insights]
                )
                recommendations.append(recommendation)
            
            elif category == InsightCategory.EFFICIENCY_OPPORTUNITY:
                recommendation = AdaptationRecommendation(
                    adaptation_type="efficiency_optimization",
                    priority=AdaptationPriority.MEDIUM,
                    description=f"Optimize {len(category_insights)} identified efficiency opportunities",
                    specific_actions=[
                        f"Improve {insight.title}" for insight in category_insights
                    ],
                    expected_benefits=["Reduced resource usage", "Faster execution"],
                    implementation_complexity="low",
                    supporting_insights=[insight.insight_id for insight in category_insights]
                )
                recommendations.append(recommendation)
            
            elif category == InsightCategory.SUCCESS_PATTERN:
                recommendation = AdaptationRecommendation(
                    adaptation_type="success_reinforcement",
                    priority=AdaptationPriority.MEDIUM,
                    description=f"Reinforce {len(category_insights)} successful patterns",
                    specific_actions=[
                        f"Systematize {insight.title}" for insight in category_insights
                    ],
                    expected_benefits=["Increased success rate", "More consistent performance"],
                    implementation_complexity="low",
                    supporting_insights=[insight.insight_id for insight in category_insights]
                )
                recommendations.append(recommendation)
        
        # Filter by adaptation threshold
        high_impact_recommendations = [
            rec for rec in recommendations
            if any(
                insight.potential_impact >= self.config['adaptation_threshold']
                for insight_id in rec.supporting_insights
                for insight in insights if insight.insight_id == insight_id
            )
        ]
        
        self.reflection_stats['adaptations_proposed'] += len(high_impact_recommendations)
        
        return high_impact_recommendations
    
    async def _conduct_meta_reflection(self, reflection_results: Dict[str, Any]) -> Dict[str, Any]:
        """Conduct meta-reflection on the reflection process itself"""
        
        meta_reflection = {
            'reflection_quality_assessment': {},
            'process_improvement_suggestions': [],
            'confidence_in_insights': 0.0,
            'recommendation_prioritization': {}
        }
        
        # Assess quality of insights generated
        insights = reflection_results.get('insights', [])
        if insights:
            confidence_scores = [insight.confidence_score for insight in insights]
            meta_reflection['confidence_in_insights'] = statistics.mean(confidence_scores)
            
            if meta_reflection['confidence_in_insights'] < 0.6:
                meta_reflection['process_improvement_suggestions'].append(
                    "Consider gathering more diverse experiences to improve insight quality"
                )
        
        # Assess recommendation quality
        recommendations = reflection_results.get('recommendations', [])
        if recommendations:
            priority_distribution = defaultdict(int)
            for rec in recommendations:
                priority_distribution[rec.priority.value] += 1
            
            meta_reflection['recommendation_prioritization'] = dict(priority_distribution)
            
            if priority_distribution['high'] == 0 and priority_distribution['critical'] == 0:
                meta_reflection['process_improvement_suggestions'].append(
                    "No high-priority adaptations identified - may need more sensitive detection thresholds"
                )
        
        return meta_reflection
    
    async def _create_reflection_summary(self, reflection_results: Dict[str, Any]) -> Dict[str, Any]:
        """Create a summary of reflection results"""
        
        summary = {
            'key_insights_count': len(reflection_results.get('insights', [])),
            'recommendations_count': len(reflection_results.get('recommendations', [])),
            'top_insights': [],
            'priority_recommendations': [],
            'overall_assessment': {}
        }
        
        # Top insights by confidence and impact
        insights = reflection_results.get('insights', [])
        if insights:
            sorted_insights = sorted(
                insights, 
                key=lambda x: x.confidence_score * x.potential_impact, 
                reverse=True
            )
            summary['top_insights'] = [
                {
                    'title': insight.title,
                    'category': insight.category.value,
                    'score': insight.confidence_score * insight.potential_impact
                }
                for insight in sorted_insights[:3]
            ]
        
        # Priority recommendations
        recommendations = reflection_results.get('recommendations', [])
        high_priority_recs = [
            rec for rec in recommendations
            if rec.priority in [AdaptationPriority.CRITICAL, AdaptationPriority.HIGH]
        ]
        summary['priority_recommendations'] = [
            {
                'type': rec.adaptation_type,
                'priority': rec.priority.value,
                'description': rec.description
            }
            for rec in high_priority_recs
        ]
        
        # Overall assessment
        performance_analysis = reflection_results.get('performance_analysis', {})
        summary['overall_assessment'] = {
            'performance_trend': performance_analysis.get('trend', 'stable'),
            'learning_velocity': 'moderate',  # Could be calculated from adaptation success
            'reflection_depth': 'comprehensive' if len(insights) > 5 else 'basic'
        }
        
        return summary
    
    def get_adaptation_recommendations(self, priority_filter: Optional[AdaptationPriority] = None) -> List[AdaptationRecommendation]:
        """Get adaptation recommendations, optionally filtered by priority"""
        
        if priority_filter:
            return [
                rec for rec in self.adaptation_recommendations
                if rec.priority == priority_filter
            ]
        
        return self.adaptation_recommendations.copy()
    
    def get_insights_by_category(self, category: InsightCategory) -> List[ReflectionInsight]:
        """Get insights filtered by category"""
        
        return [
            insight for insight in self.insights_database
            if insight.category == category
        ]
    
    def get_reflection_statistics(self) -> Dict[str, Any]:
        """Get comprehensive reflection statistics"""
        
        stats = self.reflection_stats.copy()
        
        # Add derived statistics
        if stats['total_reflections'] > 0:
            stats['average_insights_per_reflection'] = stats['insights_generated'] / stats['total_reflections']
            stats['average_reflection_time'] = stats['reflection_time_total'].total_seconds() / stats['total_reflections']
        
        if stats['adaptations_proposed'] > 0:
            stats['adaptation_implementation_rate'] = stats['adaptations_implemented'] / stats['adaptations_proposed']
        
        return stats


class PatternDetector:
    """Detects patterns across execution experiences"""
    
    async def detect_success_patterns(self, experiences: List[ExecutionExperience]) -> List[Dict[str, Any]]:
        """Detect patterns in successful executions"""
        
        successful_experiences = [exp for exp in experiences if exp.success]
        if len(successful_experiences) < 3:
            return []
        
        patterns = []
        
        # Common resource usage patterns in successful executions
        resource_patterns = self._analyze_resource_patterns(successful_experiences)
        if resource_patterns:
            patterns.append({
                'type': 'resource_usage',
                'description': 'Successful executions follow similar resource usage patterns',
                'pattern_data': resource_patterns,
                'confidence': 0.8
            })
        
        # Timing patterns
        timing_patterns = self._analyze_timing_patterns(successful_experiences)
        if timing_patterns:
            patterns.append({
                'type': 'execution_timing',
                'description': 'Successful executions exhibit consistent timing patterns',
                'pattern_data': timing_patterns,
                'confidence': 0.7
            })
        
        return patterns
    
    async def detect_failure_patterns(self, experiences: List[ExecutionExperience]) -> List[Dict[str, Any]]:
        """Detect patterns in failed executions"""
        
        failed_experiences = [exp for exp in experiences if not exp.success]
        if len(failed_experiences) < 2:
            return []
        
        patterns = []
        
        # Common error patterns
        error_patterns = self._analyze_error_patterns(failed_experiences)
        if error_patterns:
            patterns.append({
                'type': 'error_occurrence',
                'description': 'Failures often involve similar error types',
                'pattern_data': error_patterns,
                'confidence': 0.9
            })
        
        return patterns
    
    async def detect_efficiency_patterns(self, experiences: List[ExecutionExperience]) -> List[Dict[str, Any]]:
        """Detect efficiency-related patterns"""
        
        if len(experiences) < 5:
            return []
        
        # Calculate efficiency scores for each experience
        efficiency_data = []
        for exp in experiences:
            if exp.duration.total_seconds() > 0:
                estimated = exp.execution_context.get('estimated_duration', exp.duration)
                efficiency = estimated.total_seconds() / exp.duration.total_seconds()
                efficiency_data.append((exp, efficiency))
        
        # Find patterns in high-efficiency executions
        high_efficiency = [data for data in efficiency_data if data[1] > 1.2]  # 20% faster than estimated
        
        if len(high_efficiency) >= 3:
            return [{
                'type': 'high_efficiency',
                'description': 'High-efficiency executions share common characteristics',
                'pattern_data': {
                    'average_efficiency': statistics.mean([data[1] for data in high_efficiency]),
                    'common_factors': 'fast_execution_steps'
                },
                'confidence': 0.7
            }]
        
        return []
    
    async def detect_resource_patterns(self, experiences: List[ExecutionExperience]) -> List[Dict[str, Any]]:
        """Detect resource usage patterns"""
        
        # This would analyze resource usage across experiences
        # Simplified implementation for demonstration
        return []
    
    async def detect_temporal_patterns(self, experiences: List[ExecutionExperience]) -> List[Dict[str, Any]]:
        """Detect temporal patterns in execution"""
        
        # This would analyze timing and scheduling patterns
        # Simplified implementation for demonstration
        return []
    
    def _analyze_resource_patterns(self, experiences: List[ExecutionExperience]) -> Dict[str, Any]:
        """Analyze resource usage patterns"""
        
        resource_usage = defaultdict(list)
        for exp in experiences:
            for resource, usage in exp.resource_usage.items():
                resource_usage[resource].append(usage)
        
        patterns = {}
        for resource, usages in resource_usage.items():
            if len(usages) >= 3:
                patterns[resource] = {
                    'average_usage': statistics.mean(usages),
                    'usage_consistency': 1.0 - (statistics.stdev(usages) / statistics.mean(usages))
                }
        
        return patterns
    
    def _analyze_timing_patterns(self, experiences: List[ExecutionExperience]) -> Dict[str, Any]:
        """Analyze timing patterns"""
        
        durations = [exp.duration.total_seconds() for exp in experiences]
        if len(durations) >= 3:
            return {
                'average_duration': statistics.mean(durations),
                'duration_consistency': 1.0 - (statistics.stdev(durations) / statistics.mean(durations))
            }
        
        return {}
    
    def _analyze_error_patterns(self, experiences: List[ExecutionExperience]) -> Dict[str, Any]:
        """Analyze error patterns in failed executions"""
        
        error_types = defaultdict(int)
        for exp in experiences:
            for error in exp.error_messages:
                error_type = self._classify_error(error)
                error_types[error_type] += 1
        
        return dict(error_types)
    
    def _classify_error(self, error_message: str) -> str:
        """Classify error message into category"""
        
        error_lower = error_message.lower()
        
        if 'timeout' in error_lower:
            return 'timeout'
        elif 'resource' in error_lower:
            return 'resource'
        elif 'permission' in error_lower:
            return 'permission'
        elif 'network' in error_lower:
            return 'network'
        else:
            return 'other'


class PerformanceAnalyzer:
    """Analyzes performance trends over time"""
    
    async def analyze_trends(self, experiences: List[ExecutionExperience], 
                           current_experience: ExecutionExperience) -> Dict[str, Any]:
        """Analyze performance trends"""
        
        if len(experiences) < 3:
            return {'trend': 'insufficient_data', 'recommendation': 'gather_more_data'}
        
        # Calculate performance scores over time
        performance_scores = []
        for exp in experiences:
            if exp.performance_metrics:
                score = statistics.mean(exp.performance_metrics.values())
            else:
                score = 1.0 if exp.success else 0.0
            
            performance_scores.append((exp.timestamp, score))
        
        # Sort by timestamp
        performance_scores.sort(key=lambda x: x[0])
        
        # Analyze trend
        scores = [score for _, score in performance_scores]
        
        if len(scores) >= 3:
            # Simple trend analysis
            recent_avg = statistics.mean(scores[-3:])
            early_avg = statistics.mean(scores[:3])
            
            if recent_avg > early_avg + 0.1:
                trend = 'improving'
            elif recent_avg < early_avg - 0.1:
                trend = 'declining'
            else:
                trend = 'stable'
        else:
            trend = 'stable'
        
        return {
            'trend': trend,
            'current_performance': scores[-1] if scores else 0.0,
            'average_performance': statistics.mean(scores) if scores else 0.0,
            'performance_variance': statistics.stdev(scores) if len(scores) > 1 else 0.0,
            'recommendation': self._get_trend_recommendation(trend)
        }
    
    def _get_trend_recommendation(self, trend: str) -> str:
        """Get recommendation based on performance trend"""
        
        if trend == 'improving':
            return 'maintain_current_strategies'
        elif trend == 'declining':
            return 'investigate_and_adapt'
        else:
            return 'continue_monitoring'


class StrategyOptimizer:
    """Optimizes strategies based on effectiveness analysis"""
    
    async def evaluate_effectiveness(self, experiences: List[ExecutionExperience],
                                   current_experience: ExecutionExperience) -> Dict[str, Any]:
        """Evaluate strategy effectiveness"""
        
        if len(experiences) < 2:
            return {'effectiveness': 'unknown', 'recommendation': 'insufficient_data'}
        
        # Group experiences by strategy patterns (simplified)
        strategy_groups = defaultdict(list)
        for exp in experiences:
            # Simplified strategy categorization based on execution steps
            strategy_key = len(exp.execution_steps)  # Use number of steps as strategy indicator
            strategy_groups[strategy_key].append(exp)
        
        # Evaluate each strategy
        strategy_effectiveness = {}
        for strategy, exps in strategy_groups.items():
            if len(exps) >= 2:
                success_rate = sum(1 for exp in exps if exp.success) / len(exps)
                avg_duration = statistics.mean([exp.duration.total_seconds() for exp in exps])
                
                strategy_effectiveness[strategy] = {
                    'success_rate': success_rate,
                    'average_duration': avg_duration,
                    'experience_count': len(exps)
                }
        
        # Find best strategy
        best_strategy = None
        best_score = 0.0
        
        for strategy, metrics in strategy_effectiveness.items():
            # Combined score: success rate weighted by efficiency
            score = metrics['success_rate'] * (1.0 / (metrics['average_duration'] / 100))
            if score > best_score:
                best_score = score
                best_strategy = strategy
        
        return {
            'best_strategy': best_strategy,
            'strategy_effectiveness': strategy_effectiveness,
            'recommendation': 'adopt_best_strategy' if best_strategy else 'continue_experimentation'
        }


# Demonstration and testing functions
async def demonstrate_reflection_engine():
    """Demonstrate reflection engine capabilities"""
    
    print("ðŸ§  Advanced Reflection Engine Demonstration")
    print("=" * 50)
    
    # Initialize reflection engine
    reflection_engine = ReflectionEngine("demo_agent")
    
    # Create sample execution experiences
    experiences = [
        ExecutionExperience(
            task_description="Data Analysis Task 1",
            execution_context={
                'estimated_duration': timedelta(minutes=30),
                'available_resources': {'cpu': 4, 'memory': 8},
                'resource_budget': {'cpu': 3, 'memory': 6}
            },
            execution_steps=[
                {'step': 'data_loading', 'duration': 10, 'success': True},
                {'step': 'data_processing', 'duration': 15, 'success': True},
                {'step': 'analysis', 'duration': 20, 'success': True}
            ],
            outcome={'result': 'success', 'quality_score': 0.9},
            performance_metrics={'accuracy': 0.92, 'efficiency': 0.85},
            duration=timedelta(minutes=25),
            success=True,
            resource_usage={'cpu': 2, 'memory': 5},
            agent_actions=[
                {'action': 'optimize_query', 'success': True, 'efficiency': 0.9},
                {'action': 'parallel_processing', 'success': True, 'efficiency': 0.8}
            ]
        ),
        
        ExecutionExperience(
            task_description="Data Analysis Task 2",
            execution_context={
                'estimated_duration': timedelta(minutes=35),
                'available_resources': {'cpu': 4, 'memory': 8},
                'resource_budget': {'cpu': 4, 'memory': 8}
            },
            execution_steps=[
                {'step': 'data_loading', 'duration': 12, 'success': True},
                {'step': 'data_processing', 'duration': 25, 'success': False},
                {'step': 'error_recovery', 'duration': 10, 'success': True}
            ],
            outcome={'result': 'partial_failure', 'quality_score': 0.6},
            performance_metrics={'accuracy': 0.75, 'efficiency': 0.60},
            duration=timedelta(minutes=47),
            success=False,
            error_messages=['Processing timeout: Resource constraint violation'],
            resource_usage={'cpu': 4.2, 'memory': 9},
            agent_actions=[
                {'action': 'standard_processing', 'success': False, 'efficiency': 0.3},
                {'action': 'error_recovery', 'success': True, 'efficiency': 0.7}
            ]
        ),
        
        ExecutionExperience(
            task_description="Data Analysis Task 3",
            execution_context={
                'estimated_duration': timedelta(minutes=30),
                'available_resources': {'cpu': 6, 'memory': 10},
                'resource_budget': {'cpu': 4, 'memory': 8}
            },
            execution_steps=[
                {'step': 'data_loading', 'duration': 8, 'success': True},
                {'step': 'optimized_processing', 'duration': 18, 'success': True},
                {'step': 'analysis', 'duration': 15, 'success': True}
            ],
            outcome={'result': 'success', 'quality_score': 0.95},
            performance_metrics={'accuracy': 0.94, 'efficiency': 0.90},
            duration=timedelta(minutes=28),
            success=True,
            resource_usage={'cpu': 3, 'memory': 6},
            agent_actions=[
                {'action': 'optimized_query', 'success': True, 'efficiency': 0.95},
                {'action': 'smart_caching', 'success': True, 'efficiency': 0.85}
            ]
        )
    ]
    
    print(f"ðŸ“Š Processing {len(experiences)} execution experiences...")
    
    # Conduct reflection on each experience
    reflection_results = []
    for i, experience in enumerate(experiences, 1):
        print(f"\nðŸ” Reflecting on Experience {i}: {experience.task_description}")
        
        reflection_result = await reflection_engine.reflect_on_execution(experience)
        reflection_results.append(reflection_result)
        
        # Display key results
        summary = reflection_result['summary']
        print(f"   âœ¨ Insights generated: {summary['key_insights_count']}")
        print(f"   ðŸ’¡ Recommendations: {summary['recommendations_count']}")
        
        # Show top insights
        if summary['top_insights']:
            print(f"   ðŸŽ¯ Top insight: {summary['top_insights'][0]['title']}")
        
        # Show priority recommendations
        if summary['priority_recommendations']:
            print(f"   âš¡ Priority action: {summary['priority_recommendations'][0]['description']}")
    
    # Display overall reflection statistics
    print(f"\nðŸ“ˆ Reflection Engine Statistics:")
    stats = reflection_engine.get_reflection_statistics()
    print(f"   - Total reflections: {stats['total_reflections']}")
    print(f"   - Insights generated: {stats['insights_generated']}")
    print(f"   - Adaptations proposed: {stats['adaptations_proposed']}")
    print(f"   - Avg insights per reflection: {stats.get('average_insights_per_reflection', 0):.1f}")
    print(f"   - Avg reflection time: {stats.get('average_reflection_time', 0):.2f}s")
    
    # Show adaptation recommendations by priority
    print(f"\nðŸŽ¯ Adaptation Recommendations by Priority:")
    for priority in [AdaptationPriority.HIGH, AdaptationPriority.MEDIUM, AdaptationPriority.LOW]:
        recommendations = reflection_engine.get_adaptation_recommendations(priority)
        if recommendations:
            print(f"   {priority.value.upper()}: {len(recommendations)} recommendations")
            for rec in recommendations[:2]:  # Show first 2
                print(f"      - {rec.description}")
    
    # Show insights by category
    print(f"\nðŸ” Insights by Category:")
    for category in [InsightCategory.SUCCESS_PATTERN, InsightCategory.FAILURE_PATTERN, 
                     InsightCategory.EFFICIENCY_OPPORTUNITY]:
        insights = reflection_engine.get_insights_by_category(category)
        if insights:
            print(f"   {category.value}: {len(insights)} insights")
            for insight in insights[:2]:  # Show first 2
                print(f"      - {insight.title} (confidence: {insight.confidence_score:.2f})")
    
    return {
        'reflection_engine': reflection_engine,
        'reflection_results': reflection_results,
        'statistics': stats
    }


if __name__ == "__main__":
    asyncio.run(demonstrate_reflection_engine())