# src/session4/performance_optimization.py
"""
Performance optimization patterns for CrewAI production systems.
Demonstrates optimization techniques, parallel execution, and resource management.
"""

from crewai import Agent, Task, Crew, Process
from crewai.tools import SerperDevTool, WebsiteSearchTool
from langchain.llms import OpenAI
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import Dict, List, Any, Optional
import time
import asyncio
from datetime import datetime
import psutil
import threading
import queue

class PerformanceOptimizedCrew:
    """Crew system optimized for performance and scalability"""
    
    def __init__(self, llm=None, max_workers: int = 4, optimization_level: str = 'balanced'):
        self.llm = llm or OpenAI(temperature=0.3)  # Lower temperature for faster processing
        self.max_workers = max_workers
        self.optimization_level = optimization_level
        self.performance_metrics = PerformanceMetrics()
        
        # Optimization settings based on level
        self.optimization_settings = self._get_optimization_settings(optimization_level)
        
    def _get_optimization_settings(self, level: str) -> Dict[str, Any]:
        """Get optimization settings based on performance level"""
        
        settings = {
            'speed': {
                'agent_verbosity': False,
                'max_iterations': 2,
                'memory_enabled': False,
                'tool_timeout': 10,
                'parallel_execution': True,
                'batch_size': 10
            },
            'balanced': {
                'agent_verbosity': True,
                'max_iterations': 3,
                'memory_enabled': True,
                'tool_timeout': 20,
                'parallel_execution': True,
                'batch_size': 5
            },
            'quality': {
                'agent_verbosity': True,
                'max_iterations': 5,
                'memory_enabled': True,
                'tool_timeout': 30,
                'parallel_execution': False,
                'batch_size': 3
            }
        }
        
        return settings.get(level, settings['balanced'])
    
    def create_optimized_crew(self):
        """Create performance-optimized crew with tuned settings"""
        
        settings = self.optimization_settings
        
        # Fast execution agents with minimal overhead
        fast_researcher = Agent(
            role='Fast Research Specialist',
            goal='Execute research tasks quickly and efficiently while maintaining quality',
            backstory="""You prioritize speed and efficiency while maintaining acceptable 
            quality standards. You use streamlined approaches and focus on essential information.""",
            tools=[SerperDevTool(), WebsiteSearchTool()] if settings['tool_timeout'] > 15 else [SerperDevTool()],
            llm=self.llm,
            verbose=settings['agent_verbosity'],
            max_iter=settings['max_iterations'],
            memory=settings['memory_enabled']
        )
        
        fast_analyzer = Agent(
            role='Fast Analysis Specialist',
            goal='Perform rapid analysis with focused scope and clear outputs',
            backstory="""You specialize in quick, focused analysis that delivers 
            actionable insights efficiently. You avoid over-analysis and focus 
            on key decision-making information.""",
            llm=self.llm,
            verbose=settings['agent_verbosity'],
            max_iter=settings['max_iterations'],
            memory=settings['memory_enabled']
        )
        
        # Batch processor for handling multiple similar tasks efficiently
        batch_processor = Agent(
            role='Batch Processing Specialist',
            goal='Process multiple similar tasks efficiently using batch optimization',
            backstory="""You excel at batch processing and parallel execution. 
            You optimize workflows for throughput and can handle multiple similar 
            tasks simultaneously without sacrificing quality.""",
            llm=self.llm,
            verbose=settings['agent_verbosity'],
            max_iter=settings['max_iterations'],
            memory=settings['memory_enabled']
        )
        
        # Performance monitor and optimizer
        perf_monitor = Agent(\n            role='Performance Monitor',\n            goal='Monitor and optimize crew performance in real-time',\n            backstory=\"\"\"You track performance metrics, identify bottlenecks, \n            and suggest optimizations. You continuously monitor system resources \n            and execution efficiency.\"\"\",\n            llm=self.llm,\n            verbose=settings['agent_verbosity'],\n            max_iter=2,  # Keep monitoring lightweight\n            memory=False  # Reduce memory overhead for monitoring\n        )\n        \n        # Optimized task definitions\n        research_tasks = self._create_optimized_research_tasks(\n            fast_researcher, settings['batch_size']\n        )\n        \n        # Batch analysis task\n        batch_analysis = Task(\n            description=\"\"\"Analyze all research results using batch processing optimization.\n            \n            Batch processing strategy:\n            1. Group similar analysis tasks together\n            2. Use parallel processing where possible\n            3. Optimize for throughput while maintaining quality\n            4. Focus on key insights and actionable recommendations\n            5. Streamline output format for efficiency\n            \n            Performance targets:\n            - Complete analysis within 30 seconds per batch\n            - Maintain minimum 80% quality score\n            - Optimize resource utilization\"\"\",\n            agent=batch_processor,\n            expected_output=\"Efficient batch analysis results with performance metrics\",\n            context=research_tasks\n        )\n        \n        # Performance monitoring task\n        perf_task = Task(\n            description=\"\"\"Monitor crew performance and provide real-time optimization suggestions.\n            \n            Monitoring objectives:\n            1. Track execution times and identify slowdowns\n            2. Monitor resource utilization (CPU, memory)\n            3. Measure throughput and quality metrics\n            4. Identify optimization opportunities\n            5. Provide actionable performance recommendations\n            \n            Optimization focus areas:\n            - Task execution efficiency\n            - Resource utilization optimization\n            - Bottleneck identification and resolution\n            - Quality vs. speed trade-off analysis\"\"\",\n            agent=perf_monitor,\n            expected_output=\"Performance monitoring report with optimization recommendations\",\n            context=research_tasks + [batch_analysis]\n        )\n        \n        all_tasks = research_tasks + [batch_analysis, perf_task]\n        \n        # Choose process type based on optimization settings\n        process_type = Process.parallel if settings['parallel_execution'] else Process.sequential\n        \n        return Crew(\n            agents=[fast_researcher, fast_analyzer, batch_processor, perf_monitor],\n            tasks=all_tasks,\n            process=process_type,\n            verbose=1 if settings['agent_verbosity'] else 0,\n            memory=settings['memory_enabled']\n        )\n    \n    def _create_optimized_research_tasks(self, agent: Agent, batch_size: int) -> List[Task]:\n        \"\"\"Create optimized research tasks for batch processing\"\"\"\n        \n        research_topics = [\n            \"AI agent framework performance benchmarks\",\n            \"Enterprise AI implementation best practices\",\n            \"Multi-agent system scalability patterns\",\n            \"Production AI deployment strategies\",\n            \"AI agent monitoring and observability\"\n        ][:batch_size]\n        \n        tasks = []\n        for i, topic in enumerate(research_topics):\n            task = Task(\n                description=f\"\"\"Quick research on: {topic}\n                \n                Research efficiency requirements:\n                1. Focus on most recent and credible sources\n                2. Prioritize actionable insights over comprehensive coverage\n                3. Use streamlined search strategies\n                4. Optimize for speed while maintaining minimum quality\n                5. Structure output for easy aggregation\n                \n                Performance targets:\n                - Complete research within 20 seconds\n                - Identify 3-5 key insights minimum\n                - Include 2-3 credible source citations\"\"\",\n                agent=agent,\n                expected_output=f\"Streamlined research summary for {topic} with key insights and sources\"\n            )\n            tasks.append(task)\n        \n        return tasks\n    \n    def measure_performance(self, crew: Crew) -> Dict[str, Any]:\n        \"\"\"Measure comprehensive crew performance metrics\"\"\"\n        \n        # Resource monitoring setup\n        resource_monitor = ResourceMonitor()\n        resource_monitor.start_monitoring()\n        \n        start_time = time.time()\n        start_cpu = psutil.cpu_percent()\n        start_memory = psutil.virtual_memory().percent\n        \n        try:\n            # Execute crew with performance tracking\n            result = crew.kickoff()\n            \n            end_time = time.time()\n            end_cpu = psutil.cpu_percent()\n            end_memory = psutil.virtual_memory().percent\n            \n        finally:\n            resource_monitor.stop_monitoring()\n        \n        execution_time = end_time - start_time\n        resource_usage = resource_monitor.get_usage_summary()\n        \n        # Calculate performance metrics\n        metrics = {\n            'execution_time': execution_time,\n            'tasks_completed': len(crew.tasks),\n            'agents_used': len(crew.agents),\n            'throughput': len(crew.tasks) / execution_time,\n            'cpu_usage_change': end_cpu - start_cpu,\n            'memory_usage_change': end_memory - start_memory,\n            'resource_efficiency': self._calculate_resource_efficiency(\n                execution_time, resource_usage\n            ),\n            'quality_score': self._estimate_quality_score(result),\n            'optimization_level': self.optimization_level,\n            'result': result\n        }\n        \n        # Record metrics for analysis\n        self.performance_metrics.record_execution(metrics)\n        \n        return metrics\n    \n    def _calculate_resource_efficiency(self, execution_time: float, \n                                     resource_usage: Dict[str, Any]) -> float:\n        \"\"\"Calculate resource efficiency score\"\"\"\n        \n        # Normalize metrics (lower is better for time, moderate usage is best for resources)\n        time_score = max(0, 1 - (execution_time / 120))  # Penalty after 2 minutes\n        cpu_score = 1 - (resource_usage.get('avg_cpu_percent', 50) / 100)\n        memory_score = 1 - (resource_usage.get('avg_memory_percent', 50) / 100)\n        \n        # Weighted efficiency score\n        efficiency = (time_score * 0.5 + cpu_score * 0.3 + memory_score * 0.2)\n        return max(0, min(1, efficiency))\n    \n    def _estimate_quality_score(self, result: Any) -> float:\n        \"\"\"Estimate quality score based on result characteristics\"\"\"\n        \n        if not result:\n            return 0.0\n        \n        result_str = str(result)\n        \n        # Simple quality heuristics\n        quality_factors = {\n            'length': min(len(result_str) / 1000, 1.0),  # Reasonable length\n            'structure': 0.8 if '\\n' in result_str else 0.5,  # Has structure\n            'completeness': 0.9 if len(result_str) > 500 else 0.6  # Sufficient detail\n        }\n        \n        return sum(quality_factors.values()) / len(quality_factors)\n    \n    def run_performance_benchmark(self, iterations: int = 3) -> Dict[str, Any]:\n        \"\"\"Run performance benchmark across multiple iterations\"\"\"\n        \n        print(f\"Running performance benchmark with {iterations} iterations...\")\n        \n        results = []\n        for i in range(iterations):\n            print(f\"Iteration {i+1}/{iterations}\")\n            \n            crew = self.create_optimized_crew()\n            metrics = self.measure_performance(crew)\n            results.append(metrics)\n            \n            # Brief pause between iterations\n            time.sleep(2)\n        \n        # Calculate benchmark summary\n        benchmark_summary = self._calculate_benchmark_summary(results)\n        \n        return {\n            'individual_results': results,\n            'benchmark_summary': benchmark_summary,\n            'optimization_level': self.optimization_level\n        }\n    \n    def _calculate_benchmark_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Calculate benchmark summary statistics\"\"\"\n        \n        if not results:\n            return {}\n        \n        metrics_to_average = [\n            'execution_time', 'throughput', 'resource_efficiency', 'quality_score'\n        ]\n        \n        summary = {}\n        for metric in metrics_to_average:\n            values = [r[metric] for r in results if metric in r]\n            if values:\n                summary[f'avg_{metric}'] = sum(values) / len(values)\n                summary[f'min_{metric}'] = min(values)\n                summary[f'max_{metric}'] = max(values)\n        \n        # Calculate consistency (lower standard deviation is better)\n        execution_times = [r['execution_time'] for r in results]\n        if len(execution_times) > 1:\n            avg_time = sum(execution_times) / len(execution_times)\n            variance = sum((t - avg_time) ** 2 for t in execution_times) / len(execution_times)\n            summary['time_consistency'] = 1 / (1 + variance)  # Higher is better\n        \n        return summary\n\n\nclass ResourceMonitor:\n    \"\"\"Monitor system resource usage during execution\"\"\"\n    \n    def __init__(self, sampling_interval: float = 0.5):\n        self.sampling_interval = sampling_interval\n        self.monitoring = False\n        self.measurements = []\n        self.monitor_thread = None\n    \n    def start_monitoring(self):\n        \"\"\"Start resource monitoring in background thread\"\"\"\n        self.monitoring = True\n        self.measurements = []\n        self.monitor_thread = threading.Thread(target=self._monitor_resources)\n        self.monitor_thread.start()\n    \n    def stop_monitoring(self):\n        \"\"\"Stop resource monitoring\"\"\"\n        self.monitoring = False\n        if self.monitor_thread:\n            self.monitor_thread.join()\n    \n    def _monitor_resources(self):\n        \"\"\"Monitor resources in background thread\"\"\"\n        while self.monitoring:\n            measurement = {\n                'timestamp': time.time(),\n                'cpu_percent': psutil.cpu_percent(),\n                'memory_percent': psutil.virtual_memory().percent,\n                'disk_io': psutil.disk_io_counters()._asdict() if psutil.disk_io_counters() else {},\n                'network_io': psutil.net_io_counters()._asdict() if psutil.net_io_counters() else {}\n            }\n            self.measurements.append(measurement)\n            time.sleep(self.sampling_interval)\n    \n    def get_usage_summary(self) -> Dict[str, Any]:\n        \"\"\"Get resource usage summary\"\"\"\n        if not self.measurements:\n            return {}\n        \n        cpu_values = [m['cpu_percent'] for m in self.measurements]\n        memory_values = [m['memory_percent'] for m in self.measurements]\n        \n        return {\n            'duration': self.measurements[-1]['timestamp'] - self.measurements[0]['timestamp'],\n            'avg_cpu_percent': sum(cpu_values) / len(cpu_values),\n            'max_cpu_percent': max(cpu_values),\n            'avg_memory_percent': sum(memory_values) / len(memory_values),\n            'max_memory_percent': max(memory_values),\n            'total_measurements': len(self.measurements)\n        }\n\n\nclass PerformanceMetrics:\n    \"\"\"Track and analyze performance metrics over time\"\"\"\n    \n    def __init__(self):\n        self.execution_history = []\n        self.performance_trends = {}\n    \n    def record_execution(self, metrics: Dict[str, Any]):\n        \"\"\"Record execution metrics\"\"\"\n        metrics['timestamp'] = datetime.now().isoformat()\n        self.execution_history.append(metrics)\n        \n        # Keep only recent history\n        if len(self.execution_history) > 100:\n            self.execution_history = self.execution_history[-100:]\n    \n    def get_performance_trends(self) -> Dict[str, Any]:\n        \"\"\"Analyze performance trends\"\"\"\n        if len(self.execution_history) < 3:\n            return {'insufficient_data': True}\n        \n        # Calculate trends for key metrics\n        recent_executions = self.execution_history[-10:]\n        older_executions = self.execution_history[-20:-10] if len(self.execution_history) >= 20 else []\n        \n        trends = {}\n        \n        for metric in ['execution_time', 'throughput', 'quality_score', 'resource_efficiency']:\n            recent_avg = sum(e.get(metric, 0) for e in recent_executions) / len(recent_executions)\n            \n            if older_executions:\n                older_avg = sum(e.get(metric, 0) for e in older_executions) / len(older_executions)\n                trend_direction = 'improving' if recent_avg > older_avg else 'degrading' if recent_avg < older_avg else 'stable'\n                trend_magnitude = abs(recent_avg - older_avg) / older_avg if older_avg > 0 else 0\n            else:\n                trend_direction = 'unknown'\n                trend_magnitude = 0\n            \n            trends[metric] = {\n                'recent_average': recent_avg,\n                'trend_direction': trend_direction,\n                'trend_magnitude': trend_magnitude\n            }\n        \n        return trends\n    \n    def get_performance_summary(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive performance summary\"\"\"\n        if not self.execution_history:\n            return {'no_data': True}\n        \n        latest = self.execution_history[-1]\n        trends = self.get_performance_trends()\n        \n        return {\n            'latest_execution': latest,\n            'total_executions': len(self.execution_history),\n            'performance_trends': trends,\n            'optimization_recommendations': self._generate_optimization_recommendations(trends)\n        }\n    \n    def _generate_optimization_recommendations(self, trends: Dict[str, Any]) -> List[str]:\n        \"\"\"Generate optimization recommendations based on trends\"\"\"\n        recommendations = []\n        \n        if not trends or trends.get('insufficient_data'):\n            return ['Insufficient data for recommendations']\n        \n        # Execution time recommendations\n        time_trend = trends.get('execution_time', {})\n        if time_trend.get('trend_direction') == 'degrading':\n            recommendations.append(\"Execution time is increasing - consider optimizing agent iterations or tool usage\")\n        \n        # Throughput recommendations\n        throughput_trend = trends.get('throughput', {})\n        if throughput_trend.get('trend_direction') == 'degrading':\n            recommendations.append(\"Throughput is declining - consider parallel execution or batch processing\")\n        \n        # Quality recommendations\n        quality_trend = trends.get('quality_score', {})\n        if quality_trend.get('recent_average', 0) < 0.7:\n            recommendations.append(\"Quality scores are low - consider increasing agent iterations or improving prompts\")\n        \n        # Resource efficiency recommendations\n        efficiency_trend = trends.get('resource_efficiency', {})\n        if efficiency_trend.get('recent_average', 0) < 0.6:\n            recommendations.append(\"Resource efficiency is low - consider optimizing tool usage and memory settings\")\n        \n        if not recommendations:\n            recommendations.append(\"Performance is stable - no immediate optimizations needed\")\n        \n        return recommendations\n\n\ndef run_performance_optimization_demo():\n    \"\"\"Demonstrate performance optimization patterns\"\"\"\n    \n    print(\"=== CrewAI Performance Optimization Demo ===\")\n    \n    # Test different optimization levels\n    optimization_levels = ['speed', 'balanced', 'quality']\n    results = {}\n    \n    for level in optimization_levels:\n        print(f\"\\n--- Testing {level} optimization ---\")\n        \n        optimizer = PerformanceOptimizedCrew(\n            max_workers=4,\n            optimization_level=level\n        )\n        \n        benchmark_result = optimizer.run_performance_benchmark(iterations=2)\n        results[level] = benchmark_result\n        \n        summary = benchmark_result['benchmark_summary']\n        print(f\"Average execution time: {summary.get('avg_execution_time', 0):.2f}s\")\n        print(f\"Average throughput: {summary.get('avg_throughput', 0):.2f} tasks/sec\")\n        print(f\"Average quality score: {summary.get('avg_quality_score', 0):.2f}\")\n        print(f\"Resource efficiency: {summary.get('avg_resource_efficiency', 0):.2f}\")\n    \n    # Performance comparison\n    print(\"\\n=== Performance Comparison ===\")\n    for level in optimization_levels:\n        summary = results[level]['benchmark_summary']\n        print(f\"{level.title()} - Time: {summary.get('avg_execution_time', 0):.1f}s, \"\n              f\"Quality: {summary.get('avg_quality_score', 0):.2f}, \"\n              f\"Efficiency: {summary.get('avg_resource_efficiency', 0):.2f}\")\n    \n    return results\n\n\nif __name__ == \"__main__\":\n    run_performance_optimization_demo()