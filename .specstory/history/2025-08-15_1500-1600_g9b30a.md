# WORK SESSION (1500-1600)

**Generated:** 2025-09-15T19:51:41.789Z
**Work Period:** 1500-1600
**Focus:** Live session logging
**Duration:** ~60 minutes

---

## Session Overview

This session captures real-time tool interactions and exchanges.

---

## Key Activities

### Text Exchange - 2025-09-05 14:10:49 UTC [16:10:49 CEST]

**User Message:** {}

**Assistant Response:** (No response)

**Type:** Text-only exchange (no tool calls)

---

### Text Exchange - 2025-09-05 14:10:49 UTC [16:10:49 CEST]

**User Message:** {}

**Assistant Response:** (No response)

**Type:** Text-only exchange (no tool calls)

---

### Text Exchange - 2025-09-05 14:10:49 UTC [16:10:49 CEST]

**User Message:** {}

**Assistant Response:** (No response)

**Type:** Text-only exchange (no tool calls)

---

### Text Exchange - 2025-09-05 14:10:49 UTC [16:10:49 CEST]

**User Message:** {}

**Assistant Response:** (No response)

**Type:** Text-only exchange (no tool calls)

---

### Bash - 2025-08-15 13:05:59 UTC [15:05:59 CEST]

**User Request:** proceeed

**Tool:** Bash
**Input:** ```json
{
  "command": "git show e01c998:01_frameworks/Session7_First_ADK_Agent.md | wc -l",
  "description": "Check original Session 7 line count"
}
```

**Result:** ✅ Success

---

### Bash - 2025-08-15 13:05:59 UTC [15:05:59 CEST]

**User Request:** proceeed

**Tool:** Bash
**Input:** ```json
{
  "command": "wc -l /Users/q284340/Agentic/nano-degree/01_frameworks/Session7_First_ADK_Agent.md",
  "description": "Check current Session 7 line count"
}
```

**Result:** ✅ Success

---

### Bash - 2025-08-15 13:05:59 UTC [15:05:59 CEST]

**User Request:** proceeed

**Tool:** Bash
**Input:** ```json
{
  "command": "git show e01c998:01_frameworks/Session7_First_ADK_Agent.md > /tmp/session7_original.md",
  "description": "Extract original Session 7 content"
}
```

**Result:** ✅ Success

---

### Read - 2025-08-15 13:05:59 UTC [15:05:59 CEST]

**User Request:** proceeed

**Tool:** Read
**Input:** ```json
{
  "file_path": "/tmp/session7_original.md",
  "limit": 200
}
```

**Result:** ✅ Success

---

### Read - 2025-08-15 13:05:59 UTC [15:05:59 CEST]

**User Request:** proceeed

**Tool:** Read
**Input:** ```json
{
  "file_path": "/tmp/session7_original.md",
  "offset": 400,
  "limit": 400
}
```

**Result:** ✅ Success

---

### Write - 2025-08-15 13:05:59 UTC [15:05:59 CEST]

**User Request:** proceeed

**Tool:** Write
**Input:** ```json
{
  "file_path": "/Users/q284340/Agentic/nano-degree/01_frameworks/Session7_ModuleA_Advanced_ADK_Integration.md",
  "content": "# Session 7 - Module A: Advanced ADK Integration (35 minutes)\n\n**Prerequisites**: [Session 7 Core Section Complete](Session7_First_ADK_Agent.md)  \n**Target Audience**: Developers building production ADK agents  \n**Cognitive Load**: 4 advanced concepts\n\n---\n\n## 🎯 Module Overview\n\nThis module explores sophisticated ADK (Agent Development Kit) integration patterns including advanced Gemini model usage, MCP server orchestration, conversation memory systems, and production deployment strategies. You'll learn to build enterprise-grade ADK agents that can handle complex workflows with persistent state and multi-service coordination.\n\n### Learning Objectives\nBy the end of this module, you will:\n- Implement advanced Gemini model integration with function calling and streaming\n- Design sophisticated MCP server orchestration for multi-service coordination\n- Build persistent conversation memory systems with context-aware responses\n- Create production deployment strategies for Google Cloud Platform\n\n---\n\n## Part 1: Advanced Gemini Integration (20 minutes)\n\n### Sophisticated Model Configuration\n\n🗂️ **File**: `src/session7/advanced_gemini.py` - Advanced Gemini model patterns\n\nBeyond basic chat, enterprise ADK agents require sophisticated Gemini integration with function calling, streaming, and advanced configuration:\n\n```python\nfrom typing import Dict, List, Any, Optional, Callable, AsyncGenerator\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nimport asyncio\nimport json\nimport logging\nfrom enum import Enum\n\nimport vertexai\nfrom vertexai.generative_models import (\n    GenerativeModel, \n    Tool, \n    FunctionDeclaration,\n    Part,\n    Content,\n    ChatSession\n)\nfrom google.cloud import aiplatform\n\nclass ModelCapability(Enum):\n    \"\"\"ADK agent model capabilities\"\"\"\n    TEXT_GENERATION = \"text_generation\"\n    FUNCTION_CALLING = \"function_calling\"\n    CODE_GENERATION = \"code_generation\"\n    MULTIMODAL = \"multimodal\"\n    STREAMING = \"streaming\"\n\n@dataclass\nclass ModelConfiguration:\n    \"\"\"Advanced Gemini model configuration\"\"\"\n    model_name: str = \"gemini-2.0-flash-exp\"\n    temperature: float = 0.7\n    max_output_tokens: int = 2048\n    top_p: float = 0.8\n    top_k: int = 40\n    \n    # Advanced features\n    enable_function_calling: bool = True\n    enable_streaming: bool = False\n    enable_safety_settings: bool = True\n    \n    # Performance tuning\n    request_timeout: int = 60\n    retry_count: int = 3\n    concurrent_requests: int = 5\n\nclass AdvancedGeminiAgent:\n    \"\"\"Production-ready Gemini agent with advanced capabilities\"\"\"\n    \n    def __init__(self, config: ModelConfiguration, project_id: str, location: str):\n        self.config = config\n        self.project_id = project_id\n        self.location = location\n        self.logger = logging.getLogger(__name__)\n        \n        # Initialize Vertex AI\n        vertexai.init(project=project_id, location=location)\n        \n        # Model and session management\n        self.model: Optional[GenerativeModel] = None\n        self.chat_session: Optional[ChatSession] = None\n        self.function_registry: Dict[str, Callable] = {}\n        self.tool_declarations: List[FunctionDeclaration] = []\n        \n        # Performance tracking\n        self.request_metrics = {\n            \"total_requests\": 0,\n            \"successful_requests\": 0,\n            \"failed_requests\": 0,\n            \"average_response_time\": 0.0,\n            \"function_calls\": 0\n        }\n        \n        self._initialize_model()\n    \n    def _initialize_model(self):\n        \"\"\"Initialize Gemini model with advanced configuration\"\"\"\n        \n        try:\n            # Configure safety settings\n            safety_settings = None\n            if self.config.enable_safety_settings:\n                from vertexai.generative_models import HarmCategory, HarmBlockThreshold\n                safety_settings = {\n                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                }\n            \n            # Generation configuration\n            generation_config = {\n                \"temperature\": self.config.temperature,\n                \"max_output_tokens\": self.config.max_output_tokens,\n                \"top_p\": self.config.top_p,\n                \"top_k\": self.config.top_k\n            }\n            \n            # Create tools if function calling is enabled\n            tools = None\n            if self.config.enable_function_calling and self.tool_declarations:\n                tools = [Tool(function_declarations=self.tool_declarations)]\n            \n            # Initialize model\n            self.model = GenerativeModel(\n                model_name=self.config.model_name,\n                generation_config=generation_config,\n                safety_settings=safety_settings,\n                tools=tools\n            )\n            \n            # Initialize chat session\n            self.chat_session = self.model.start_chat()\n            \n            self.logger.info(f\"Initialized Gemini model: {self.config.model_name}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to initialize Gemini model: {e}\")\n            raise\n    \n    def register_function(self, name: str, description: str, \n                         parameters: Dict[str, Any], \n                         function: Callable):\n        \"\"\"Register a function for Gemini function calling\"\"\"\n        \n        # Create function declaration\n        function_declaration = FunctionDeclaration(\n            name=name,\n            description=description,\n            parameters=parameters\n        )\n        \n        self.tool_declarations.append(function_declaration)\n        self.function_registry[name] = function\n        \n        # Reinitialize model with new tools\n        self._initialize_model()\n        \n        self.logger.info(f\"Registered function: {name}\")\n    \n    async def generate_response(self, prompt: str, \n                              context: Dict[str, Any] = None,\n                              use_streaming: bool = None) -> str:\n        \"\"\"Generate response with advanced features\"\"\"\n        \n        if use_streaming is None:\n            use_streaming = self.config.enable_streaming\n        \n        start_time = datetime.now()\n        \n        try:\n            self.request_metrics[\"total_requests\"] += 1\n            \n            # Prepare context-enhanced prompt\n            enhanced_prompt = self._enhance_prompt_with_context(prompt, context or {})\n            \n            if use_streaming:\n                response = await self._generate_streaming_response(enhanced_prompt)\n            else:\n                response = await self._generate_standard_response(enhanced_prompt)\n            \n            # Update metrics\n            response_time = (datetime.now() - start_time).total_seconds()\n            self._update_metrics(response_time, success=True)\n            \n            return response\n            \n        except Exception as e:\n            self.logger.error(f\"Response generation failed: {e}\")\n            self._update_metrics(0, success=False)\n            raise\n    \n    async def _generate_standard_response(self, prompt: str) -> str:\n        \"\"\"Generate standard response with function calling support\"\"\"\n        \n        response = await self.chat_session.send_message_async(prompt)\n        \n        # Check if function calling is required\n        if response.candidates[0].function_calls:\n            # Handle function calls\n            function_responses = await self._handle_function_calls(\n                response.candidates[0].function_calls\n            )\n            \n            # Send function responses back to model\n            response = await self.chat_session.send_message_async(function_responses)\n        \n        return response.text\n    \n    async def _generate_streaming_response(self, prompt: str) -> str:\n        \"\"\"Generate streaming response for real-time applications\"\"\"\n        \n        response_chunks = []\n        \n        async for chunk in self.chat_session.send_message_async(prompt, stream=True):\n            if chunk.text:\n                response_chunks.append(chunk.text)\n                # Yield partial response for real-time display\n                yield chunk.text\n        \n        return \"\".join(response_chunks)\n    \n    async def _handle_function_calls(self, function_calls: List[Any]) -> List[Part]:\n        \"\"\"Handle function calls from Gemini model\"\"\"\n        \n        function_responses = []\n        \n        for function_call in function_calls:\n            function_name = function_call.name\n            function_args = function_call.args\n            \n            self.request_metrics[\"function_calls\"] += 1\n            \n            if function_name in self.function_registry:\n                try:\n                    # Execute registered function\n                    function_impl = self.function_registry[function_name]\n                    \n                    if asyncio.iscoroutinefunction(function_impl):\n                        result = await function_impl(**function_args)\n                    else:\n                        result = function_impl(**function_args)\n                    \n                    # Create function response\n                    function_response = Part.from_function_response(\n                        name=function_name,\n                        response={\"result\": result}\n                    )\n                    function_responses.append(function_response)\n                    \n                except Exception as e:\n                    self.logger.error(f\"Function {function_name} execution failed: {e}\")\n                    error_response = Part.from_function_response(\n                        name=function_name,\n                        response={\"error\": str(e)}\n                    )\n                    function_responses.append(error_response)\n            else:\n                self.logger.warning(f\"Unknown function called: {function_name}\")\n        \n        return function_responses\n    \n    def _enhance_prompt_with_context(self, prompt: str, context: Dict[str, Any]) -> str:\n        \"\"\"Enhance prompt with contextual information\"\"\"\n        \n        if not context:\n            return prompt\n        \n        context_parts = []\n        \n        # Add user context\n        if \"user_id\" in context:\n            context_parts.append(f\"User ID: {context['user_id']}\")\n        \n        # Add conversation context\n        if \"conversation_history\" in context:\n            history = context[\"conversation_history\"]\n            if history:\n                context_parts.append(\"Recent conversation:\")\n                for turn in history[-3:]:  # Last 3 turns\n                    role = turn.get(\"role\", \"unknown\")\n                    content = turn.get(\"content\", \"\")\n                    context_parts.append(f\"  {role}: {content[:100]}...\")\n        \n        # Add business context\n        if \"business_context\" in context:\n            business_ctx = context[\"business_context\"]\n            context_parts.append(f\"Business context: {business_ctx}\")\n        \n        # Add temporal context\n        context_parts.append(f\"Current time: {datetime.now().isoformat()}\")\n        \n        if context_parts:\n            enhanced_prompt = f\"\"\"Context:\n{chr(10).join(context_parts)}\n\nUser request: {prompt}\"\"\"\n            return enhanced_prompt\n        \n        return prompt\n    \n    def _update_metrics(self, response_time: float, success: bool):\n        \"\"\"Update performance metrics\"\"\"\n        \n        if success:\n            self.request_metrics[\"successful_requests\"] += 1\n        else:\n            self.request_metrics[\"failed_requests\"] += 1\n        \n        # Update average response time\n        total_successful = self.request_metrics[\"successful_requests\"]\n        if total_successful > 0:\n            current_avg = self.request_metrics[\"average_response_time\"]\n            new_avg = ((current_avg * (total_successful - 1)) + response_time) / total_successful\n            self.request_metrics[\"average_response_time\"] = new_avg\n\nclass MCPGeminiOrchestrator:\n    \"\"\"Orchestrates multiple MCP services with Gemini intelligence\"\"\"\n    \n    def __init__(self, gemini_agent: AdvancedGeminiAgent):\n        self.gemini_agent = gemini_agent\n        self.mcp_services: Dict[str, Any] = {}\n        self.service_capabilities: Dict[str, List[str]] = {}\n        self.orchestration_history: List[Dict[str, Any]] = []\n        \n    def register_mcp_service(self, service_name: str, \n                           service_client: Any, \n                           capabilities: List[str]):\n        \"\"\"Register MCP service with its capabilities\"\"\"\n        \n        self.mcp_services[service_name] = service_client\n        self.service_capabilities[service_name] = capabilities\n        \n        # Register service functions with Gemini\n        self._register_service_functions(service_name, capabilities)\n    \n    def _register_service_functions(self, service_name: str, capabilities: List[str]):\n        \"\"\"Register MCP service functions with Gemini\"\"\"\n        \n        for capability in capabilities:\n            function_name = f\"{service_name}_{capability}\"\n            \n            # Create function declaration\n            parameters = {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": f\"Query for {capability} in {service_name}\"\n                    },\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"description\": \"Additional parameters for the service call\"\n                    }\n                },\n                \"required\": [\"query\"]\n            }\n            \n            # Create wrapper function\n            async def service_function(query: str, parameters: Dict[str, Any] = None):\n                return await self._call_mcp_service(service_name, capability, query, parameters or {})\n            \n            self.gemini_agent.register_function(\n                name=function_name,\n                description=f\"Use {service_name} service for {capability}\",\n                parameters=parameters,\n                function=service_function\n            )\n    \n    async def _call_mcp_service(self, service_name: str, \n                               capability: str, \n                               query: str, \n                               parameters: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Call MCP service with error handling and logging\"\"\"\n        \n        service_client = self.mcp_services.get(service_name)\n        if not service_client:\n            return {\"error\": f\"Service {service_name} not available\"}\n        \n        try:\n            # Record orchestration attempt\n            orchestration_record = {\n                \"timestamp\": datetime.now(),\n                \"service\": service_name,\n                \"capability\": capability,\n                \"query\": query,\n                \"parameters\": parameters\n            }\n            \n            # Call MCP service\n            result = await service_client.call_tool(capability, {\n                \"query\": query,\n                **parameters\n            })\n            \n            orchestration_record[\"result\"] = result\n            orchestration_record[\"status\"] = \"success\"\n            \n            self.orchestration_history.append(orchestration_record)\n            return result\n            \n        except Exception as e:\n            error_result = {\"error\": str(e)}\n            orchestration_record[\"result\"] = error_result\n            orchestration_record[\"status\"] = \"error\"\n            \n            self.orchestration_history.append(orchestration_record)\n            return error_result\n    \n    async def intelligent_service_selection(self, user_query: str) -> Dict[str, Any]:\n        \"\"\"Use Gemini to intelligently select and orchestrate MCP services\"\"\"\n        \n        # Analyze query to determine required services\n        analysis_prompt = f\"\"\"\n        Analyze this user query and determine which services would be most helpful:\n        Query: \"{user_query}\"\n        \n        Available services and their capabilities:\n        {self._format_service_capabilities()}\n        \n        Respond with a JSON object indicating:\n        1. Which services to use\n        2. What specific capabilities to invoke\n        3. The order of operations\n        4. Any parameters needed\n        \"\"\"\n        \n        service_plan = await self.gemini_agent.generate_response(analysis_prompt)\n        \n        try:\n            # Parse service execution plan\n            plan = json.loads(service_plan)\n            execution_results = []\n            \n            # Execute services according to plan\n            for step in plan.get(\"steps\", []):\n                service_name = step.get(\"service\")\n                capability = step.get(\"capability\")\n                query = step.get(\"query\", user_query)\n                parameters = step.get(\"parameters\", {})\n                \n                result = await self._call_mcp_service(\n                    service_name, capability, query, parameters\n                )\n                execution_results.append({\n                    \"step\": step,\n                    \"result\": result\n                })\n            \n            return {\n                \"plan\": plan,\n                \"execution_results\": execution_results,\n                \"status\": \"completed\"\n            }\n            \n        except json.JSONDecodeError:\n            # Fallback to direct service calls\n            return await self._fallback_service_execution(user_query)\n    \n    def _format_service_capabilities(self) -> str:\n        \"\"\"Format service capabilities for Gemini analysis\"\"\"\n        \n        formatted = []\n        for service_name, capabilities in self.service_capabilities.items():\n            caps_str = \", \".join(capabilities)\n            formatted.append(f\"- {service_name}: {caps_str}\")\n        \n        return \"\\n\".join(formatted)\n```\n\n---\n\n## Part 2: Production Memory and Context Systems (15 minutes)\n\n### Advanced Conversation Memory\n\n🗂️ **File**: `src/session7/advanced_memory.py` - Production memory systems\n\n```python\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nimport json\nimport sqlite3\nimport asyncio\nimport threading\nfrom enum import Enum\n\nclass MemoryType(Enum):\n    \"\"\"Types of memory for different use cases\"\"\"\n    SHORT_TERM = \"short_term\"      # Current conversation\n    LONG_TERM = \"long_term\"        # Persistent user history\n    WORKING = \"working\"            # Active task context\n    EPISODIC = \"episodic\"         # Specific event memories\n    SEMANTIC = \"semantic\"         # General knowledge\n\n@dataclass\nclass MemoryEntry:\n    \"\"\"Individual memory entry with metadata\"\"\"\n    content: str\n    memory_type: MemoryType\n    timestamp: datetime\n    importance: float = 0.5  # 0.0 to 1.0\n    context: Dict[str, Any] = field(default_factory=dict)\n    tags: List[str] = field(default_factory=list)\n    embedding: Optional[List[float]] = None\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"content\": self.content,\n            \"memory_type\": self.memory_type.value,\n            \"timestamp\": self.timestamp.isoformat(),\n            \"importance\": self.importance,\n            \"context\": self.context,\n            \"tags\": self.tags,\n            \"embedding\": self.embedding\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'MemoryEntry':\n        return cls(\n            content=data[\"content\"],\n            memory_type=MemoryType(data[\"memory_type\"]),\n            timestamp=datetime.fromisoformat(data[\"timestamp\"]),\n            importance=data.get(\"importance\", 0.5),\n            context=data.get(\"context\", {}),\n            tags=data.get(\"tags\", []),\n            embedding=data.get(\"embedding\")\n        )\n\nclass AdvancedConversationMemory:\n    \"\"\"Advanced memory system with multiple memory types and persistence\"\"\"\n    \n    def __init__(self, user_id: str, db_path: str = \"agent_memory.db\"):\n        self.user_id = user_id\n        self.db_path = db_path\n        self.memories: Dict[MemoryType, List[MemoryEntry]] = {\n            memory_type: [] for memory_type in MemoryType\n        }\n        self.memory_lock = threading.Lock()\n        \n        # Configuration\n        self.max_short_term_memories = 50\n        self.max_working_memories = 20\n        self.importance_threshold = 0.3\n        self.retention_days = 90\n        \n        # Initialize database\n        self._initialize_database()\n        self._load_memories()\n    \n    def _initialize_database(self):\n        \"\"\"Initialize SQLite database for memory persistence\"\"\"\n        \n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS memories (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    user_id TEXT NOT NULL,\n                    content TEXT NOT NULL,\n                    memory_type TEXT NOT NULL,\n                    timestamp TEXT NOT NULL,\n                    importance REAL NOT NULL,\n                    context TEXT,\n                    tags TEXT,\n                    embedding TEXT,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                )\n            \"\"\")\n            \n            conn.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_user_type_time \n                ON memories(user_id, memory_type, timestamp)\n            \"\"\")\n    \n    def _load_memories(self):\n        \"\"\"Load memories from database\"\"\"\n        \n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.execute(\"\"\"\n                SELECT content, memory_type, timestamp, importance, context, tags, embedding\n                FROM memories \n                WHERE user_id = ? AND timestamp > ?\n                ORDER BY timestamp DESC\n            \"\"\", (self.user_id, (datetime.now() - timedelta(days=self.retention_days)).isoformat()))\n            \n            for row in cursor.fetchall():\n                content, memory_type, timestamp, importance, context, tags, embedding = row\n                \n                memory_entry = MemoryEntry(\n                    content=content,\n                    memory_type=MemoryType(memory_type),\n                    timestamp=datetime.fromisoformat(timestamp),\n                    importance=importance,\n                    context=json.loads(context or \"{}\"),\n                    tags=json.loads(tags or \"[]\"),\n                    embedding=json.loads(embedding) if embedding else None\n                )\n                \n                self.memories[memory_entry.memory_type].append(memory_entry)\n    \n    def add_memory(self, content: str, memory_type: MemoryType, \n                   importance: float = 0.5, context: Dict[str, Any] = None,\n                   tags: List[str] = None):\n        \"\"\"Add new memory entry with automatic management\"\"\"\n        \n        memory_entry = MemoryEntry(\n            content=content,\n            memory_type=memory_type,\n            timestamp=datetime.now(),\n            importance=importance,\n            context=context or {},\n            tags=tags or []\n        )\n        \n        with self.memory_lock:\n            # Add to in-memory storage\n            self.memories[memory_type].append(memory_entry)\n            \n            # Manage memory limits\n            self._manage_memory_limits(memory_type)\n            \n            # Persist to database\n            self._persist_memory(memory_entry)\n    \n    def _manage_memory_limits(self, memory_type: MemoryType):\n        \"\"\"Manage memory limits and cleanup old entries\"\"\"\n        \n        memories = self.memories[memory_type]\n        \n        if memory_type == MemoryType.SHORT_TERM:\n            if len(memories) > self.max_short_term_memories:\n                # Keep most recent and most important\n                memories.sort(key=lambda m: (m.importance, m.timestamp), reverse=True)\n                self.memories[memory_type] = memories[:self.max_short_term_memories]\n        \n        elif memory_type == MemoryType.WORKING:\n            if len(memories) > self.max_working_memories:\n                # Keep most recent working memories\n                memories.sort(key=lambda m: m.timestamp, reverse=True)\n                self.memories[memory_type] = memories[:self.max_working_memories]\n        \n        # For long-term memories, use importance-based retention\n        elif memory_type == MemoryType.LONG_TERM:\n            # Remove low-importance old memories\n            cutoff_date = datetime.now() - timedelta(days=30)\n            self.memories[memory_type] = [\n                m for m in memories \n                if m.importance >= self.importance_threshold or m.timestamp > cutoff_date\n            ]\n    \n    def _persist_memory(self, memory_entry: MemoryEntry):\n        \"\"\"Persist memory entry to database\"\"\"\n        \n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                INSERT INTO memories (user_id, content, memory_type, timestamp, \n                                    importance, context, tags, embedding)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", (\n                self.user_id,\n                memory_entry.content,\n                memory_entry.memory_type.value,\n                memory_entry.timestamp.isoformat(),\n                memory_entry.importance,\n                json.dumps(memory_entry.context),\n                json.dumps(memory_entry.tags),\n                json.dumps(memory_entry.embedding) if memory_entry.embedding else None\n            ))\n    \n    def retrieve_relevant_memories(self, query: str, \n                                 memory_types: List[MemoryType] = None,\n                                 max_memories: int = 10) -> List[MemoryEntry]:\n        \"\"\"Retrieve memories relevant to a query\"\"\"\n        \n        if memory_types is None:\n            memory_types = list(MemoryType)\n        \n        relevant_memories = []\n        \n        with self.memory_lock:\n            for memory_type in memory_types:\n                for memory in self.memories[memory_type]:\n                    # Simple relevance scoring (could be enhanced with embeddings)\n                    relevance_score = self._calculate_relevance(query, memory)\n                    \n                    if relevance_score > 0.1:  # Minimum relevance threshold\n                        relevant_memories.append((memory, relevance_score))\n        \n        # Sort by relevance and return top memories\n        relevant_memories.sort(key=lambda x: x[1], reverse=True)\n        return [memory for memory, _ in relevant_memories[:max_memories]]\n    \n    def _calculate_relevance(self, query: str, memory: MemoryEntry) -> float:\n        \"\"\"Calculate relevance score between query and memory\"\"\"\n        \n        query_lower = query.lower()\n        content_lower = memory.content.lower()\n        \n        # Simple keyword matching (could be enhanced with semantic similarity)\n        query_words = set(query_lower.split())\n        content_words = set(content_lower.split())\n        \n        if not query_words:\n            return 0.0\n        \n        # Calculate word overlap\n        overlap = len(query_words.intersection(content_words))\n        relevance = overlap / len(query_words)\n        \n        # Boost relevance based on importance and recency\n        importance_boost = memory.importance * 0.2\n        \n        # Recency boost (memories from last 24 hours get boost)\n        if datetime.now() - memory.timestamp < timedelta(hours=24):\n            recency_boost = 0.1\n        else:\n            recency_boost = 0.0\n        \n        return min(relevance + importance_boost + recency_boost, 1.0)\n    \n    def get_conversation_context(self, max_turns: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Get recent conversation context for prompt enhancement\"\"\"\n        \n        short_term_memories = self.memories[MemoryType.SHORT_TERM]\n        \n        # Sort by timestamp and get recent memories\n        recent_memories = sorted(\n            short_term_memories,\n            key=lambda m: m.timestamp,\n            reverse=True\n        )[:max_turns]\n        \n        # Convert to conversation format\n        context = []\n        for memory in reversed(recent_memories):  # Chronological order\n            context.append({\n                \"timestamp\": memory.timestamp.isoformat(),\n                \"content\": memory.content,\n                \"importance\": memory.importance,\n                \"tags\": memory.tags\n            })\n        \n        return context\n    \n    def consolidate_memories(self):\n        \"\"\"Consolidate and organize memories for better retrieval\"\"\"\n        \n        with self.memory_lock:\n            # Promote important short-term memories to long-term\n            short_term_memories = self.memories[MemoryType.SHORT_TERM]\n            \n            for memory in short_term_memories[:]:\n                if memory.importance >= 0.8:  # High importance threshold\n                    # Move to long-term memory\n                    memory.memory_type = MemoryType.LONG_TERM\n                    self.memories[MemoryType.LONG_TERM].append(memory)\n                    self.memories[MemoryType.SHORT_TERM].remove(memory)\n                    \n                    # Update in database\n                    self._update_memory_type(memory)\n    \n    def _update_memory_type(self, memory: MemoryEntry):\n        \"\"\"Update memory type in database\"\"\"\n        \n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                UPDATE memories \n                SET memory_type = ? \n                WHERE user_id = ? AND content = ? AND timestamp = ?\n            \"\"\", (\n                memory.memory_type.value,\n                self.user_id,\n                memory.content,\n                memory.timestamp.isoformat()\n            ))\n```\n\n---\n\n## 🎯 Module Summary\n\nYou've now mastered advanced ADK integration patterns:\n\n✅ **Advanced Gemini Integration**: Implemented sophisticated model configuration with function calling and streaming  \n✅ **MCP Service Orchestration**: Built intelligent multi-service coordination with Gemini-driven selection  \n✅ **Production Memory Systems**: Created persistent conversation memory with multiple memory types  \n✅ **Enterprise Context Management**: Designed context-aware response generation with relevance scoring\n\n### Next Steps\n- **Continue to Module B**: [Enterprise Agent Systems](Session7_ModuleB_Enterprise_Agent_Systems.md) for production deployment patterns\n- **Return to Core**: [Session 7 Main](Session7_First_ADK_Agent.md)\n- **Advance to Session 8**: [Agno Production Ready Agents](Session8_Agno_Production_Ready_Agents.md)\n\n---\n\n**🗂️ Source Files for Module A:**\n- `src/session7/advanced_gemini.py` - Sophisticated Gemini model integration\n- `src/session7/advanced_memory.py` - Production memory systems\n- `src/session7/mcp_orchestration.py` - Multi-service coordination patterns"
}
```

**Result:** ✅ Success

---

### Bash - 2025-08-15 13:05:59 UTC [15:05:59 CEST]

**User Request:** proceeed

**Tool:** Bash
**Input:** ```json
{
  "command": "git show e01c998:01_frameworks/Session7_First_ADK_Agent.md | wc -l",
  "description": "Check original Session 7 line count"
}
```

**Result:** ✅ Success

---

### Bash - 2025-08-15 13:05:59 UTC [15:05:59 CEST]

**User Request:** proceeed

**Tool:** Bash
**Input:** ```json
{
  "command": "wc -l /Users/q284340/Agentic/nano-degree/01_frameworks/Session7_First_ADK_Agent.md",
  "description": "Check current Session 7 line count"
}
```

**Result:** ✅ Success

---

### Bash - 2025-08-15 13:05:59 UTC [15:05:59 CEST]

**User Request:** proceeed

**Tool:** Bash
**Input:** ```json
{
  "command": "git show e01c998:01_frameworks/Session7_First_ADK_Agent.md > /tmp/session7_original.md",
  "description": "Extract original Session 7 content"
}
```

**Result:** ✅ Success

---

### Read - 2025-08-15 13:05:59 UTC [15:05:59 CEST]

**User Request:** proceeed

**Tool:** Read
**Input:** ```json
{
  "file_path": "/tmp/session7_original.md",
  "limit": 200
}
```

**Result:** ✅ Success

---

### Read - 2025-08-15 13:05:59 UTC [15:05:59 CEST]

**User Request:** proceeed

**Tool:** Read
**Input:** ```json
{
  "file_path": "/tmp/session7_original.md",
  "offset": 400,
  "limit": 400
}
```

**Result:** ✅ Success

---

### Write - 2025-08-15 13:05:59 UTC [15:05:59 CEST]

**User Request:** proceeed

**Tool:** Write
**Input:** ```json
{
  "file_path": "/Users/q284340/Agentic/nano-degree/01_frameworks/Session7_ModuleA_Advanced_ADK_Integration.md",
  "content": "# Session 7 - Module A: Advanced ADK Integration (35 minutes)\n\n**Prerequisites**: [Session 7 Core Section Complete](Session7_First_ADK_Agent.md)  \n**Target Audience**: Developers building production ADK agents  \n**Cognitive Load**: 4 advanced concepts\n\n---\n\n## 🎯 Module Overview\n\nThis module explores sophisticated ADK (Agent Development Kit) integration patterns including advanced Gemini model usage, MCP server orchestration, conversation memory systems, and production deployment strategies. You'll learn to build enterprise-grade ADK agents that can handle complex workflows with persistent state and multi-service coordination.\n\n### Learning Objectives\nBy the end of this module, you will:\n- Implement advanced Gemini model integration with function calling and streaming\n- Design sophisticated MCP server orchestration for multi-service coordination\n- Build persistent conversation memory systems with context-aware responses\n- Create production deployment strategies for Google Cloud Platform\n\n---\n\n## Part 1: Advanced Gemini Integration (20 minutes)\n\n### Sophisticated Model Configuration\n\n🗂️ **File**: `src/session7/advanced_gemini.py` - Advanced Gemini model patterns\n\nBeyond basic chat, enterprise ADK agents require sophisticated Gemini integration with function calling, streaming, and advanced configuration:\n\n```python\nfrom typing import Dict, List, Any, Optional, Callable, AsyncGenerator\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nimport asyncio\nimport json\nimport logging\nfrom enum import Enum\n\nimport vertexai\nfrom vertexai.generative_models import (\n    GenerativeModel, \n    Tool, \n    FunctionDeclaration,\n    Part,\n    Content,\n    ChatSession\n)\nfrom google.cloud import aiplatform\n\nclass ModelCapability(Enum):\n    \"\"\"ADK agent model capabilities\"\"\"\n    TEXT_GENERATION = \"text_generation\"\n    FUNCTION_CALLING = \"function_calling\"\n    CODE_GENERATION = \"code_generation\"\n    MULTIMODAL = \"multimodal\"\n    STREAMING = \"streaming\"\n\n@dataclass\nclass ModelConfiguration:\n    \"\"\"Advanced Gemini model configuration\"\"\"\n    model_name: str = \"gemini-2.0-flash-exp\"\n    temperature: float = 0.7\n    max_output_tokens: int = 2048\n    top_p: float = 0.8\n    top_k: int = 40\n    \n    # Advanced features\n    enable_function_calling: bool = True\n    enable_streaming: bool = False\n    enable_safety_settings: bool = True\n    \n    # Performance tuning\n    request_timeout: int = 60\n    retry_count: int = 3\n    concurrent_requests: int = 5\n\nclass AdvancedGeminiAgent:\n    \"\"\"Production-ready Gemini agent with advanced capabilities\"\"\"\n    \n    def __init__(self, config: ModelConfiguration, project_id: str, location: str):\n        self.config = config\n        self.project_id = project_id\n        self.location = location\n        self.logger = logging.getLogger(__name__)\n        \n        # Initialize Vertex AI\n        vertexai.init(project=project_id, location=location)\n        \n        # Model and session management\n        self.model: Optional[GenerativeModel] = None\n        self.chat_session: Optional[ChatSession] = None\n        self.function_registry: Dict[str, Callable] = {}\n        self.tool_declarations: List[FunctionDeclaration] = []\n        \n        # Performance tracking\n        self.request_metrics = {\n            \"total_requests\": 0,\n            \"successful_requests\": 0,\n            \"failed_requests\": 0,\n            \"average_response_time\": 0.0,\n            \"function_calls\": 0\n        }\n        \n        self._initialize_model()\n    \n    def _initialize_model(self):\n        \"\"\"Initialize Gemini model with advanced configuration\"\"\"\n        \n        try:\n            # Configure safety settings\n            safety_settings = None\n            if self.config.enable_safety_settings:\n                from vertexai.generative_models import HarmCategory, HarmBlockThreshold\n                safety_settings = {\n                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n                }\n            \n            # Generation configuration\n            generation_config = {\n                \"temperature\": self.config.temperature,\n                \"max_output_tokens\": self.config.max_output_tokens,\n                \"top_p\": self.config.top_p,\n                \"top_k\": self.config.top_k\n            }\n            \n            # Create tools if function calling is enabled\n            tools = None\n            if self.config.enable_function_calling and self.tool_declarations:\n                tools = [Tool(function_declarations=self.tool_declarations)]\n            \n            # Initialize model\n            self.model = GenerativeModel(\n                model_name=self.config.model_name,\n                generation_config=generation_config,\n                safety_settings=safety_settings,\n                tools=tools\n            )\n            \n            # Initialize chat session\n            self.chat_session = self.model.start_chat()\n            \n            self.logger.info(f\"Initialized Gemini model: {self.config.model_name}\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to initialize Gemini model: {e}\")\n            raise\n    \n    def register_function(self, name: str, description: str, \n                         parameters: Dict[str, Any], \n                         function: Callable):\n        \"\"\"Register a function for Gemini function calling\"\"\"\n        \n        # Create function declaration\n        function_declaration = FunctionDeclaration(\n            name=name,\n            description=description,\n            parameters=parameters\n        )\n        \n        self.tool_declarations.append(function_declaration)\n        self.function_registry[name] = function\n        \n        # Reinitialize model with new tools\n        self._initialize_model()\n        \n        self.logger.info(f\"Registered function: {name}\")\n    \n    async def generate_response(self, prompt: str, \n                              context: Dict[str, Any] = None,\n                              use_streaming: bool = None) -> str:\n        \"\"\"Generate response with advanced features\"\"\"\n        \n        if use_streaming is None:\n            use_streaming = self.config.enable_streaming\n        \n        start_time = datetime.now()\n        \n        try:\n            self.request_metrics[\"total_requests\"] += 1\n            \n            # Prepare context-enhanced prompt\n            enhanced_prompt = self._enhance_prompt_with_context(prompt, context or {})\n            \n            if use_streaming:\n                response = await self._generate_streaming_response(enhanced_prompt)\n            else:\n                response = await self._generate_standard_response(enhanced_prompt)\n            \n            # Update metrics\n            response_time = (datetime.now() - start_time).total_seconds()\n            self._update_metrics(response_time, success=True)\n            \n            return response\n            \n        except Exception as e:\n            self.logger.error(f\"Response generation failed: {e}\")\n            self._update_metrics(0, success=False)\n            raise\n    \n    async def _generate_standard_response(self, prompt: str) -> str:\n        \"\"\"Generate standard response with function calling support\"\"\"\n        \n        response = await self.chat_session.send_message_async(prompt)\n        \n        # Check if function calling is required\n        if response.candidates[0].function_calls:\n            # Handle function calls\n            function_responses = await self._handle_function_calls(\n                response.candidates[0].function_calls\n            )\n            \n            # Send function responses back to model\n            response = await self.chat_session.send_message_async(function_responses)\n        \n        return response.text\n    \n    async def _generate_streaming_response(self, prompt: str) -> str:\n        \"\"\"Generate streaming response for real-time applications\"\"\"\n        \n        response_chunks = []\n        \n        async for chunk in self.chat_session.send_message_async(prompt, stream=True):\n            if chunk.text:\n                response_chunks.append(chunk.text)\n                # Yield partial response for real-time display\n                yield chunk.text\n        \n        return \"\".join(response_chunks)\n    \n    async def _handle_function_calls(self, function_calls: List[Any]) -> List[Part]:\n        \"\"\"Handle function calls from Gemini model\"\"\"\n        \n        function_responses = []\n        \n        for function_call in function_calls:\n            function_name = function_call.name\n            function_args = function_call.args\n            \n            self.request_metrics[\"function_calls\"] += 1\n            \n            if function_name in self.function_registry:\n                try:\n                    # Execute registered function\n                    function_impl = self.function_registry[function_name]\n                    \n                    if asyncio.iscoroutinefunction(function_impl):\n                        result = await function_impl(**function_args)\n                    else:\n                        result = function_impl(**function_args)\n                    \n                    # Create function response\n                    function_response = Part.from_function_response(\n                        name=function_name,\n                        response={\"result\": result}\n                    )\n                    function_responses.append(function_response)\n                    \n                except Exception as e:\n                    self.logger.error(f\"Function {function_name} execution failed: {e}\")\n                    error_response = Part.from_function_response(\n                        name=function_name,\n                        response={\"error\": str(e)}\n                    )\n                    function_responses.append(error_response)\n            else:\n                self.logger.warning(f\"Unknown function called: {function_name}\")\n        \n        return function_responses\n    \n    def _enhance_prompt_with_context(self, prompt: str, context: Dict[str, Any]) -> str:\n        \"\"\"Enhance prompt with contextual information\"\"\"\n        \n        if not context:\n            return prompt\n        \n        context_parts = []\n        \n        # Add user context\n        if \"user_id\" in context:\n            context_parts.append(f\"User ID: {context['user_id']}\")\n        \n        # Add conversation context\n        if \"conversation_history\" in context:\n            history = context[\"conversation_history\"]\n            if history:\n                context_parts.append(\"Recent conversation:\")\n                for turn in history[-3:]:  # Last 3 turns\n                    role = turn.get(\"role\", \"unknown\")\n                    content = turn.get(\"content\", \"\")\n                    context_parts.append(f\"  {role}: {content[:100]}...\")\n        \n        # Add business context\n        if \"business_context\" in context:\n            business_ctx = context[\"business_context\"]\n            context_parts.append(f\"Business context: {business_ctx}\")\n        \n        # Add temporal context\n        context_parts.append(f\"Current time: {datetime.now().isoformat()}\")\n        \n        if context_parts:\n            enhanced_prompt = f\"\"\"Context:\n{chr(10).join(context_parts)}\n\nUser request: {prompt}\"\"\"\n            return enhanced_prompt\n        \n        return prompt\n    \n    def _update_metrics(self, response_time: float, success: bool):\n        \"\"\"Update performance metrics\"\"\"\n        \n        if success:\n            self.request_metrics[\"successful_requests\"] += 1\n        else:\n            self.request_metrics[\"failed_requests\"] += 1\n        \n        # Update average response time\n        total_successful = self.request_metrics[\"successful_requests\"]\n        if total_successful > 0:\n            current_avg = self.request_metrics[\"average_response_time\"]\n            new_avg = ((current_avg * (total_successful - 1)) + response_time) / total_successful\n            self.request_metrics[\"average_response_time\"] = new_avg\n\nclass MCPGeminiOrchestrator:\n    \"\"\"Orchestrates multiple MCP services with Gemini intelligence\"\"\"\n    \n    def __init__(self, gemini_agent: AdvancedGeminiAgent):\n        self.gemini_agent = gemini_agent\n        self.mcp_services: Dict[str, Any] = {}\n        self.service_capabilities: Dict[str, List[str]] = {}\n        self.orchestration_history: List[Dict[str, Any]] = []\n        \n    def register_mcp_service(self, service_name: str, \n                           service_client: Any, \n                           capabilities: List[str]):\n        \"\"\"Register MCP service with its capabilities\"\"\"\n        \n        self.mcp_services[service_name] = service_client\n        self.service_capabilities[service_name] = capabilities\n        \n        # Register service functions with Gemini\n        self._register_service_functions(service_name, capabilities)\n    \n    def _register_service_functions(self, service_name: str, capabilities: List[str]):\n        \"\"\"Register MCP service functions with Gemini\"\"\"\n        \n        for capability in capabilities:\n            function_name = f\"{service_name}_{capability}\"\n            \n            # Create function declaration\n            parameters = {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": f\"Query for {capability} in {service_name}\"\n                    },\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"description\": \"Additional parameters for the service call\"\n                    }\n                },\n                \"required\": [\"query\"]\n            }\n            \n            # Create wrapper function\n            async def service_function(query: str, parameters: Dict[str, Any] = None):\n                return await self._call_mcp_service(service_name, capability, query, parameters or {})\n            \n            self.gemini_agent.register_function(\n                name=function_name,\n                description=f\"Use {service_name} service for {capability}\",\n                parameters=parameters,\n                function=service_function\n            )\n    \n    async def _call_mcp_service(self, service_name: str, \n                               capability: str, \n                               query: str, \n                               parameters: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Call MCP service with error handling and logging\"\"\"\n        \n        service_client = self.mcp_services.get(service_name)\n        if not service_client:\n            return {\"error\": f\"Service {service_name} not available\"}\n        \n        try:\n            # Record orchestration attempt\n            orchestration_record = {\n                \"timestamp\": datetime.now(),\n                \"service\": service_name,\n                \"capability\": capability,\n                \"query\": query,\n                \"parameters\": parameters\n            }\n            \n            # Call MCP service\n            result = await service_client.call_tool(capability, {\n                \"query\": query,\n                **parameters\n            })\n            \n            orchestration_record[\"result\"] = result\n            orchestration_record[\"status\"] = \"success\"\n            \n            self.orchestration_history.append(orchestration_record)\n            return result\n            \n        except Exception as e:\n            error_result = {\"error\": str(e)}\n            orchestration_record[\"result\"] = error_result\n            orchestration_record[\"status\"] = \"error\"\n            \n            self.orchestration_history.append(orchestration_record)\n            return error_result\n    \n    async def intelligent_service_selection(self, user_query: str) -> Dict[str, Any]:\n        \"\"\"Use Gemini to intelligently select and orchestrate MCP services\"\"\"\n        \n        # Analyze query to determine required services\n        analysis_prompt = f\"\"\"\n        Analyze this user query and determine which services would be most helpful:\n        Query: \"{user_query}\"\n        \n        Available services and their capabilities:\n        {self._format_service_capabilities()}\n        \n        Respond with a JSON object indicating:\n        1. Which services to use\n        2. What specific capabilities to invoke\n        3. The order of operations\n        4. Any parameters needed\n        \"\"\"\n        \n        service_plan = await self.gemini_agent.generate_response(analysis_prompt)\n        \n        try:\n            # Parse service execution plan\n            plan = json.loads(service_plan)\n            execution_results = []\n            \n            # Execute services according to plan\n            for step in plan.get(\"steps\", []):\n                service_name = step.get(\"service\")\n                capability = step.get(\"capability\")\n                query = step.get(\"query\", user_query)\n                parameters = step.get(\"parameters\", {})\n                \n                result = await self._call_mcp_service(\n                    service_name, capability, query, parameters\n                )\n                execution_results.append({\n                    \"step\": step,\n                    \"result\": result\n                })\n            \n            return {\n                \"plan\": plan,\n                \"execution_results\": execution_results,\n                \"status\": \"completed\"\n            }\n            \n        except json.JSONDecodeError:\n            # Fallback to direct service calls\n            return await self._fallback_service_execution(user_query)\n    \n    def _format_service_capabilities(self) -> str:\n        \"\"\"Format service capabilities for Gemini analysis\"\"\"\n        \n        formatted = []\n        for service_name, capabilities in self.service_capabilities.items():\n            caps_str = \", \".join(capabilities)\n            formatted.append(f\"- {service_name}: {caps_str}\")\n        \n        return \"\\n\".join(formatted)\n```\n\n---\n\n## Part 2: Production Memory and Context Systems (15 minutes)\n\n### Advanced Conversation Memory\n\n🗂️ **File**: `src/session7/advanced_memory.py` - Production memory systems\n\n```python\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nimport json\nimport sqlite3\nimport asyncio\nimport threading\nfrom enum import Enum\n\nclass MemoryType(Enum):\n    \"\"\"Types of memory for different use cases\"\"\"\n    SHORT_TERM = \"short_term\"      # Current conversation\n    LONG_TERM = \"long_term\"        # Persistent user history\n    WORKING = \"working\"            # Active task context\n    EPISODIC = \"episodic\"         # Specific event memories\n    SEMANTIC = \"semantic\"         # General knowledge\n\n@dataclass\nclass MemoryEntry:\n    \"\"\"Individual memory entry with metadata\"\"\"\n    content: str\n    memory_type: MemoryType\n    timestamp: datetime\n    importance: float = 0.5  # 0.0 to 1.0\n    context: Dict[str, Any] = field(default_factory=dict)\n    tags: List[str] = field(default_factory=list)\n    embedding: Optional[List[float]] = None\n    \n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"content\": self.content,\n            \"memory_type\": self.memory_type.value,\n            \"timestamp\": self.timestamp.isoformat(),\n            \"importance\": self.importance,\n            \"context\": self.context,\n            \"tags\": self.tags,\n            \"embedding\": self.embedding\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'MemoryEntry':\n        return cls(\n            content=data[\"content\"],\n            memory_type=MemoryType(data[\"memory_type\"]),\n            timestamp=datetime.fromisoformat(data[\"timestamp\"]),\n            importance=data.get(\"importance\", 0.5),\n            context=data.get(\"context\", {}),\n            tags=data.get(\"tags\", []),\n            embedding=data.get(\"embedding\")\n        )\n\nclass AdvancedConversationMemory:\n    \"\"\"Advanced memory system with multiple memory types and persistence\"\"\"\n    \n    def __init__(self, user_id: str, db_path: str = \"agent_memory.db\"):\n        self.user_id = user_id\n        self.db_path = db_path\n        self.memories: Dict[MemoryType, List[MemoryEntry]] = {\n            memory_type: [] for memory_type in MemoryType\n        }\n        self.memory_lock = threading.Lock()\n        \n        # Configuration\n        self.max_short_term_memories = 50\n        self.max_working_memories = 20\n        self.importance_threshold = 0.3\n        self.retention_days = 90\n        \n        # Initialize database\n        self._initialize_database()\n        self._load_memories()\n    \n    def _initialize_database(self):\n        \"\"\"Initialize SQLite database for memory persistence\"\"\"\n        \n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS memories (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    user_id TEXT NOT NULL,\n                    content TEXT NOT NULL,\n                    memory_type TEXT NOT NULL,\n                    timestamp TEXT NOT NULL,\n                    importance REAL NOT NULL,\n                    context TEXT,\n                    tags TEXT,\n                    embedding TEXT,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                )\n            \"\"\")\n            \n            conn.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_user_type_time \n                ON memories(user_id, memory_type, timestamp)\n            \"\"\")\n    \n    def _load_memories(self):\n        \"\"\"Load memories from database\"\"\"\n        \n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.execute(\"\"\"\n                SELECT content, memory_type, timestamp, importance, context, tags, embedding\n                FROM memories \n                WHERE user_id = ? AND timestamp > ?\n                ORDER BY timestamp DESC\n            \"\"\", (self.user_id, (datetime.now() - timedelta(days=self.retention_days)).isoformat()))\n            \n            for row in cursor.fetchall():\n                content, memory_type, timestamp, importance, context, tags, embedding = row\n                \n                memory_entry = MemoryEntry(\n                    content=content,\n                    memory_type=MemoryType(memory_type),\n                    timestamp=datetime.fromisoformat(timestamp),\n                    importance=importance,\n                    context=json.loads(context or \"{}\"),\n                    tags=json.loads(tags or \"[]\"),\n                    embedding=json.loads(embedding) if embedding else None\n                )\n                \n                self.memories[memory_entry.memory_type].append(memory_entry)\n    \n    def add_memory(self, content: str, memory_type: MemoryType, \n                   importance: float = 0.5, context: Dict[str, Any] = None,\n                   tags: List[str] = None):\n        \"\"\"Add new memory entry with automatic management\"\"\"\n        \n        memory_entry = MemoryEntry(\n            content=content,\n            memory_type=memory_type,\n            timestamp=datetime.now(),\n            importance=importance,\n            context=context or {},\n            tags=tags or []\n        )\n        \n        with self.memory_lock:\n            # Add to in-memory storage\n            self.memories[memory_type].append(memory_entry)\n            \n            # Manage memory limits\n            self._manage_memory_limits(memory_type)\n            \n            # Persist to database\n            self._persist_memory(memory_entry)\n    \n    def _manage_memory_limits(self, memory_type: MemoryType):\n        \"\"\"Manage memory limits and cleanup old entries\"\"\"\n        \n        memories = self.memories[memory_type]\n        \n        if memory_type == MemoryType.SHORT_TERM:\n            if len(memories) > self.max_short_term_memories:\n                # Keep most recent and most important\n                memories.sort(key=lambda m: (m.importance, m.timestamp), reverse=True)\n                self.memories[memory_type] = memories[:self.max_short_term_memories]\n        \n        elif memory_type == MemoryType.WORKING:\n            if len(memories) > self.max_working_memories:\n                # Keep most recent working memories\n                memories.sort(key=lambda m: m.timestamp, reverse=True)\n                self.memories[memory_type] = memories[:self.max_working_memories]\n        \n        # For long-term memories, use importance-based retention\n        elif memory_type == MemoryType.LONG_TERM:\n            # Remove low-importance old memories\n            cutoff_date = datetime.now() - timedelta(days=30)\n            self.memories[memory_type] = [\n                m for m in memories \n                if m.importance >= self.importance_threshold or m.timestamp > cutoff_date\n            ]\n    \n    def _persist_memory(self, memory_entry: MemoryEntry):\n        \"\"\"Persist memory entry to database\"\"\"\n        \n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                INSERT INTO memories (user_id, content, memory_type, timestamp, \n                                    importance, context, tags, embedding)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", (\n                self.user_id,\n                memory_entry.content,\n                memory_entry.memory_type.value,\n                memory_entry.timestamp.isoformat(),\n                memory_entry.importance,\n                json.dumps(memory_entry.context),\n                json.dumps(memory_entry.tags),\n                json.dumps(memory_entry.embedding) if memory_entry.embedding else None\n            ))\n    \n    def retrieve_relevant_memories(self, query: str, \n                                 memory_types: List[MemoryType] = None,\n                                 max_memories: int = 10) -> List[MemoryEntry]:\n        \"\"\"Retrieve memories relevant to a query\"\"\"\n        \n        if memory_types is None:\n            memory_types = list(MemoryType)\n        \n        relevant_memories = []\n        \n        with self.memory_lock:\n            for memory_type in memory_types:\n                for memory in self.memories[memory_type]:\n                    # Simple relevance scoring (could be enhanced with embeddings)\n                    relevance_score = self._calculate_relevance(query, memory)\n                    \n                    if relevance_score > 0.1:  # Minimum relevance threshold\n                        relevant_memories.append((memory, relevance_score))\n        \n        # Sort by relevance and return top memories\n        relevant_memories.sort(key=lambda x: x[1], reverse=True)\n        return [memory for memory, _ in relevant_memories[:max_memories]]\n    \n    def _calculate_relevance(self, query: str, memory: MemoryEntry) -> float:\n        \"\"\"Calculate relevance score between query and memory\"\"\"\n        \n        query_lower = query.lower()\n        content_lower = memory.content.lower()\n        \n        # Simple keyword matching (could be enhanced with semantic similarity)\n        query_words = set(query_lower.split())\n        content_words = set(content_lower.split())\n        \n        if not query_words:\n            return 0.0\n        \n        # Calculate word overlap\n        overlap = len(query_words.intersection(content_words))\n        relevance = overlap / len(query_words)\n        \n        # Boost relevance based on importance and recency\n        importance_boost = memory.importance * 0.2\n        \n        # Recency boost (memories from last 24 hours get boost)\n        if datetime.now() - memory.timestamp < timedelta(hours=24):\n            recency_boost = 0.1\n        else:\n            recency_boost = 0.0\n        \n        return min(relevance + importance_boost + recency_boost, 1.0)\n    \n    def get_conversation_context(self, max_turns: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Get recent conversation context for prompt enhancement\"\"\"\n        \n        short_term_memories = self.memories[MemoryType.SHORT_TERM]\n        \n        # Sort by timestamp and get recent memories\n        recent_memories = sorted(\n            short_term_memories,\n            key=lambda m: m.timestamp,\n            reverse=True\n        )[:max_turns]\n        \n        # Convert to conversation format\n        context = []\n        for memory in reversed(recent_memories):  # Chronological order\n            context.append({\n                \"timestamp\": memory.timestamp.isoformat(),\n                \"content\": memory.content,\n                \"importance\": memory.importance,\n                \"tags\": memory.tags\n            })\n        \n        return context\n    \n    def consolidate_memories(self):\n        \"\"\"Consolidate and organize memories for better retrieval\"\"\"\n        \n        with self.memory_lock:\n            # Promote important short-term memories to long-term\n            short_term_memories = self.memories[MemoryType.SHORT_TERM]\n            \n            for memory in short_term_memories[:]:\n                if memory.importance >= 0.8:  # High importance threshold\n                    # Move to long-term memory\n                    memory.memory_type = MemoryType.LONG_TERM\n                    self.memories[MemoryType.LONG_TERM].append(memory)\n                    self.memories[MemoryType.SHORT_TERM].remove(memory)\n                    \n                    # Update in database\n                    self._update_memory_type(memory)\n    \n    def _update_memory_type(self, memory: MemoryEntry):\n        \"\"\"Update memory type in database\"\"\"\n        \n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute(\"\"\"\n                UPDATE memories \n                SET memory_type = ? \n                WHERE user_id = ? AND content = ? AND timestamp = ?\n            \"\"\", (\n                memory.memory_type.value,\n                self.user_id,\n                memory.content,\n                memory.timestamp.isoformat()\n            ))\n```\n\n---\n\n## 🎯 Module Summary\n\nYou've now mastered advanced ADK integration patterns:\n\n✅ **Advanced Gemini Integration**: Implemented sophisticated model configuration with function calling and streaming  \n✅ **MCP Service Orchestration**: Built intelligent multi-service coordination with Gemini-driven selection  \n✅ **Production Memory Systems**: Created persistent conversation memory with multiple memory types  \n✅ **Enterprise Context Management**: Designed context-aware response generation with relevance scoring\n\n### Next Steps\n- **Continue to Module B**: [Enterprise Agent Systems](Session7_ModuleB_Enterprise_Agent_Systems.md) for production deployment patterns\n- **Return to Core**: [Session 7 Main](Session7_First_ADK_Agent.md)\n- **Advance to Session 8**: [Agno Production Ready Agents](Session8_Agno_Production_Ready_Agents.md)\n\n---\n\n**🗂️ Source Files for Module A:**\n- `src/session7/advanced_gemini.py` - Sophisticated Gemini model integration\n- `src/session7/advanced_memory.py` - Production memory systems\n- `src/session7/mcp_orchestration.py` - Multi-service coordination patterns"
}
```

**Result:** ✅ Success

---

### Edit - 2025-09-07 10:59:32 UTC [12:59:32 CEST]

**User Request:** yes revert

**Tool:** Edit
**Input:** ```json
{
  "file_path": "/Users/q284340/Agentic/coding/scripts/combined-status-line.js",
  "old_string": "          // Only check USER messages for redirect detection\n          // Skip system and assistant messages\n          if (entry.type !== 'user') continue;",
  "new_string": "          // Check both USER and ASSISTANT messages for redirect detection\n          // Skip system messages (hooks, internal operations)\n          if (entry.type === 'system') continue;\n          \n          // Only check user and assistant messages\n          if (entry.type !== 'user' && entry.type !== 'assistant') continue;"
}
```

**Result:** ✅ Success

---

### Edit - 2025-09-07 10:59:32 UTC [12:59:32 CEST]

**User Request:** yes revert

**Tool:** Edit
**Input:** ```json
{
  "file_path": "/Users/q284340/Agentic/coding/scripts/combined-status-line.js",
  "old_string": "          // Only check the actual user-typed message content, not tool results\n          let actualContent = '';\n          if (entry.message && entry.message.content) {\n            // Handle different content formats\n            if (typeof entry.message.content === 'string') {\n              actualContent = entry.message.content.toLowerCase();\n            } else if (Array.isArray(entry.message.content)) {\n              // For arrays, only check text content, not tool results\n              for (const item of entry.message.content) {\n                if (item.type === 'text' && item.text) {\n                  actualContent += item.text.toLowerCase() + ' ';\n                }\n                // Skip tool_result type entries\n              }\n            }\n          } else if (entry.content && typeof entry.content === 'string') {\n            actualContent = entry.content.toLowerCase();\n          }",
  "new_string": "          // Extract message content based on entry type\n          let actualContent = '';\n          if (entry.message && entry.message.content) {\n            // Handle different content formats\n            if (typeof entry.message.content === 'string') {\n              actualContent = entry.message.content.toLowerCase();\n            } else if (Array.isArray(entry.message.content)) {\n              for (const item of entry.message.content) {\n                if (item.type === 'text' && item.text) {\n                  actualContent += item.text.toLowerCase() + ' ';\n                } else if (entry.type === 'user' && item.type === 'tool_result') {\n                  // Skip tool results for user messages (they contain previous outputs)\n                  continue;\n                } else if (entry.type === 'assistant' && item.type === 'tool_use') {\n                  // Include tool usage from assistant messages for coding detection\n                  actualContent += JSON.stringify(item.input).toLowerCase() + ' ';\n                }\n              }\n            }\n          } else if (entry.content && typeof entry.content === 'string') {\n            actualContent = entry.content.toLowerCase();\n          }"
}
```

**Result:** ✅ Success

---

### Bash - 2025-09-07 10:59:32 UTC [12:59:32 CEST]

**User Request:** yes revert

**Tool:** Bash
**Input:** ```json
{
  "command": "echo '{\"transcript_path\":\"/Users/q284340/.claude/projects/-Users-q284340-Agentic-nano-degree/7dafc21b-cbac-4fae-9e36-c366b64b5a2a.jsonl\"}' | DEBUG_STATUS=1 CODING_TARGET_PROJECT=\"/Users/q284340/Agentic/nano-degree\" node scripts/combined-status-line.js 2>&1 | tail -3",
  "description": "Test reverted logic - should still show no redirect for modules question"
}
```

**Result:** ✅ Success

---

### Edit - 2025-09-07 10:59:32 UTC [12:59:32 CEST]

**User Request:** yes revert

**Tool:** Edit
**Input:** ```json
{
  "file_path": "/Users/q284340/Agentic/coding/scripts/combined-status-line.js",
  "old_string": "      // Check only the most recent exchange (current prompt) for coding activity\n      // Look for the last user message and any assistant responses/tool calls after it\n      let currentExchangeLines = [];\n      let foundLastUserMessage = false;\n      \n      // Start from the end and collect the current exchange\n      for (let i = lines.length - 1; i >= 0; i--) {\n        const line = lines[i];\n        if (!line.trim()) continue;\n        \n        try {\n          const entry = JSON.parse(line);\n          currentExchangeLines.unshift(line);\n          \n          // If we hit a user message and already found one, stop (previous exchange)\n          if (entry.type === 'user' && foundLastUserMessage) {\n            currentExchangeLines.shift(); // Remove this older user message\n            break;\n          }\n          \n          // Mark that we found the most recent user message\n          if (entry.type === 'user') {\n            foundLastUserMessage = true;\n          }\n        } catch (parseError) {\n          // Skip malformed JSON lines\n          continue;\n        }\n      }",
  "new_string": "      // Find the most recent complete exchange (user prompt + assistant responses)\n      let currentExchangeLines = [];\n      let lastUserMessageIndex = -1;\n      \n      // First, find the last user message\n      for (let i = lines.length - 1; i >= 0; i--) {\n        const line = lines[i];\n        if (!line.trim()) continue;\n        \n        try {\n          const entry = JSON.parse(line);\n          if (entry.type === 'user' && !entry.isMeta) {\n            lastUserMessageIndex = i;\n            break;\n          }\n        } catch (parseError) {\n          continue;\n        }\n      }\n      \n      if (lastUserMessageIndex === -1) {\n        return { active: false }; // No user messages found\n      }\n      \n      // Collect the exchange: last user message + subsequent assistant responses\n      for (let i = lastUserMessageIndex; i < lines.length; i++) {\n        const line = lines[i];\n        if (!line.trim()) continue;\n        \n        try {\n          const entry = JSON.parse(line);\n          // Include the user message and any assistant responses that follow\n          if (entry.type === 'user' || entry.type === 'assistant') {\n            // Stop if we hit a new user message (unless it's the first one we found)\n            if (entry.type === 'user' && i > lastUserMessageIndex) {\n              break;\n            }\n            currentExchangeLines.push(line);\n          }\n        } catch (parseError) {\n          continue;\n        }\n      }"
}
```

**Result:** ✅ Success

---

### Bash - 2025-09-07 10:59:32 UTC [12:59:32 CEST]

**User Request:** yes revert

**Tool:** Bash
**Input:** ```json
{
  "command": "echo '{\"transcript_path\":\"/Users/q284340/.claude/projects/-Users-q284340-Agentic-nano-degree/7dafc21b-cbac-4fae-9e36-c366b64b5a2a.jsonl\"}' | DEBUG_STATUS=1 CODING_TARGET_PROJECT=\"/Users/q284340/Agentic/nano-degree\" node scripts/combined-status-line.js 2>&1 | tail -5",
  "description": "Test improved exchange detection"
}
```

**Result:** ✅ Success

---

### Bash - 2025-09-07 10:59:32 UTC [12:59:32 CEST]

**User Request:** yes revert

**Tool:** Bash
**Input:** ```json
{
  "command": "tail -10 /Users/q284340/.claude/projects/-Users-q284340-Agentic-nano-degree/7dafc21b-cbac-4fae-9e36-c366b64b5a2a.jsonl | grep -E '\"type\":\"user\"' | tail -1 | jq -r '.message.content'",
  "description": "Check what the most recent user message actually is"
}
```

**Result:** ✅ Success

---

