{
  "summary": {
    "total_files": 1,
    "files_needing_refactoring": 1,
    "total_large_blocks": 11
  },
  "files": [
    {
      "file": "/Users/q284340/Agentic/nano-degree/docs-content/03_mcp-acp-a2a/Session3_LangChain_MCP_Integration.md",
      "total_code_blocks": 19,
      "large_blocks_count": 11,
      "code_blocks": [
        {
          "start_line": 46,
          "end_line": 68,
          "language": "python",
          "content": [
            "# Basic MCP-LangChain integration - The moment two worlds become one",
            "",
            "from langchain_mcp_adapters import MultiServerMCPClient",
            "from langchain.agents import create_react_agent",
            "from langchain_openai import ChatOpenAI",
            "",
            "# Connect to multiple MCP servers - Building your digital ecosystem",
            "client = MultiServerMCPClient({",
            "    \"weather\": {\"command\": \"python\", \"args\": [\"weather_server.py\"]},",
            "    \"files\": {\"command\": \"python\", \"args\": [\"file_server.py\"]}",
            "})",
            "",
            "# Get tools from all servers - Your agent's superpowers",
            "tools = client.list_tools()",
            "",
            "# Create intelligent agent - The mind that connects it all",
            "agent = create_react_agent(",
            "    llm=ChatOpenAI(model=\"gpt-4\"),",
            "    tools=tools,",
            "    prompt=\"You are a helpful assistant with access to multiple tools.\"",
            ")"
          ],
          "line_count": 21
        },
        {
          "start_line": 83,
          "end_line": 95,
          "language": "bash",
          "content": [
            "# Create your integration workspace",
            "mkdir langchain-mcp-integration",
            "cd langchain-mcp-integration",
            "",
            "# Create isolated environment - Clean slate for innovation",
            "python -m venv venv",
            "source venv/bin/activate  # On Windows: venv\\Scripts\\activate",
            "",
            "# Install the integration toolkit - Your bridge-building tools",
            "pip install langchain-mcp-adapters langgraph langchain-openai \\",
            "            langchain-anthropic python-dotenv colorama rich"
          ],
          "line_count": 11
        },
        {
          "start_line": 110,
          "end_line": 129,
          "language": "",
          "content": [
            "langchain-mcp-integration/",
            "\u251c\u2500\u2500 mcp_servers/           # Your digital workforce",
            "\u2502   \u251c\u2500\u2500 weather_server.py",
            "\u2502   \u251c\u2500\u2500 filesystem_server.py",
            "\u2502   \u2514\u2500\u2500 database_server.py",
            "\u251c\u2500\u2500 agents/                # Your intelligent coordinators",
            "\u2502   \u251c\u2500\u2500 basic_agent.py     # Single-tool demonstration",
            "\u2502   \u251c\u2500\u2500 multi_tool_agent.py # Multi-server ReAct agent",
            "\u2502   \u2514\u2500\u2500 workflow_agent.py  # LangGraph workflow agent",
            "\u251c\u2500\u2500 workflows/             # Your orchestration patterns",
            "\u2502   \u251c\u2500\u2500 research_workflow.py",
            "\u2502   \u2514\u2500\u2500 data_analysis_workflow.py",
            "\u251c\u2500\u2500 utils/                 # Your integration utilities",
            "\u2502   \u251c\u2500\u2500 mcp_manager.py     # MCP server management",
            "\u2502   \u2514\u2500\u2500 logging_config.py  # Structured logging",
            "\u251c\u2500\u2500 config.py              # Your system configuration",
            "\u251c\u2500\u2500 main.py               # Your application entry point",
            "\u2514\u2500\u2500 .env                  # Your secrets and settings"
          ],
          "line_count": 18
        },
        {
          "start_line": 143,
          "end_line": 172,
          "language": "python",
          "content": [
            "# config.py - Your integration command center",
            "",
            "import os",
            "from typing import Dict, Any, List",
            "from dataclasses import dataclass",
            "from dotenv import load_dotenv",
            "",
            "load_dotenv()",
            "",
            "@dataclass",
            "class MCPServerConfig:",
            "    \"\"\"Configuration for a single MCP server - Your digital specialist definition.\"\"\"",
            "    name: str",
            "    command: str",
            "    args: List[str]",
            "    transport: str = \"stdio\"",
            "    description: str = \"\"",
            "    timeout: int = 30",
            "    retry_attempts: int = 3",
            "",
            "@dataclass ",
            "class LLMConfig:",
            "    \"\"\"Configuration for language models - Your intelligence settings.\"\"\"",
            "    provider: str = \"openai\"",
            "    model: str = \"gpt-4\"",
            "    temperature: float = 0.7",
            "    max_tokens: int = 2000",
            "    timeout: int = 60"
          ],
          "line_count": 28
        },
        {
          "start_line": 183,
          "end_line": 233,
          "language": "python",
          "content": [
            "# Your main configuration orchestrator",
            "",
            "class Config:",
            "    \"\"\"Main configuration class for LangChain MCP integration.\"\"\"",
            "    ",
            "    # API Keys from environment variables - Security first",
            "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")",
            "    ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")",
            "    ",
            "    # LLM Configuration with environment overrides - Flexibility with defaults",
            "    LLM = LLMConfig(",
            "        provider=os.getenv(\"LLM_PROVIDER\", \"openai\"),",
            "        model=os.getenv(\"LLM_MODEL\", \"gpt-4\"),",
            "        temperature=float(os.getenv(\"LLM_TEMPERATURE\", \"0.7\")),",
            "    )",
            "    ",
            "    # MCP Server Registry - Your digital workforce roster",
            "    MCP_SERVERS = [",
            "        MCPServerConfig(",
            "            name=\"weather\",",
            "            command=\"python\",",
            "            args=[\"mcp_servers/weather_server.py\"],",
            "            description=\"Weather information and forecasts\"",
            "        ),",
            "        MCPServerConfig(",
            "            name=\"filesystem\", ",
            "            command=\"python\",",
            "            args=[\"mcp_servers/filesystem_server.py\"],",
            "            description=\"Secure file system operations\"",
            "        ),",
            "        MCPServerConfig(",
            "            name=\"database\",",
            "            command=\"python\", ",
            "            args=[\"mcp_servers/database_server.py\"],",
            "            description=\"Database query and manipulation\"",
            "        )",
            "    ]",
            "    ",
            "    # Agent Configuration - Intelligence parameters",
            "    AGENT_CONFIG = {",
            "        \"max_iterations\": int(os.getenv(\"MAX_ITERATIONS\", \"10\")),",
            "        \"verbose\": os.getenv(\"VERBOSE\", \"true\").lower() == \"true\",",
            "        \"temperature\": float(os.getenv(\"AGENT_TEMPERATURE\", \"0.7\")),",
            "        \"timeout\": int(os.getenv(\"AGENT_TIMEOUT\", \"300\"))  # 5 minutes",
            "    }",
            "    ",
            "    # Logging Configuration - Observability settings",
            "    LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\")",
            "    LOG_FORMAT = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\""
          ],
          "line_count": 49
        },
        {
          "start_line": 254,
          "end_line": 264,
          "language": "python",
          "content": [
            "# The hidden complexity behind a simple question",
            "",
            "# Customer inquiry: \"What's the weather like for my shipment to London?\"",
            "",
            "# Agent needs to orchestrate multiple systems:",
            "# 1. Query customer database for shipment details",
            "# 2. Get weather data for London",
            "# 3. Check file system for shipping policies",
            "# 4. Combine information into helpful response"
          ],
          "line_count": 9
        },
        {
          "start_line": 277,
          "end_line": 295,
          "language": "python",
          "content": [
            "# utils/mcp_manager.py - Your integration command center",
            "",
            "import asyncio",
            "import logging",
            "from typing import Dict, List, Optional",
            "from langchain_mcp_adapters import MCPAdapter",
            "from config import Config, MCPServerConfig",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "class MCPServerManager:",
            "    \"\"\"The nerve center that manages multiple MCP servers with health monitoring.\"\"\"",
            "    ",
            "    def __init__(self, server_configs: List[MCPServerConfig]):",
            "        self.server_configs = {config.name: config for config in server_configs}",
            "        self.adapters: Dict[str, MCPAdapter] = {}",
            "        self.health_status: Dict[str, bool] = {}"
          ],
          "line_count": 17
        },
        {
          "start_line": 305,
          "end_line": 341,
          "language": "python",
          "content": [
            "    async def start_all_servers(self) -> Dict[str, bool]:",
            "        \"\"\"The startup orchestration - Bringing your digital workforce online.\"\"\"",
            "        results = {}",
            "        ",
            "        for name, config in self.server_configs.items():",
            "            result = await self._start_single_server(name, config)",
            "            results[name] = result",
            "        ",
            "        return results",
            "    ",
            "    async def _start_single_server(self, name: str, config: MCPServerConfig) -> bool:",
            "        \"\"\"Individual server startup with comprehensive validation.\"\"\"",
            "        try:",
            "            logger.info(f\"Starting MCP server: {name}\")",
            "            adapter = MCPAdapter(",
            "                command=config.command,",
            "                args=config.args,",
            "                timeout=config.timeout",
            "            )",
            "            ",
            "            # The critical handshake - Test connection and discover tools",
            "            await adapter.start()",
            "            tools = await adapter.list_tools()",
            "            ",
            "            # Success - Store adapter and update status",
            "            self.adapters[name] = adapter",
            "            self.health_status[name] = True",
            "            ",
            "            logger.info(f\"Server '{name}' started with {len(tools)} tools\")",
            "            return True",
            "            ",
            "        except Exception as e:",
            "            logger.error(f\"Failed to start server '{name}': {e}\")",
            "            self.health_status[name] = False",
            "            return False"
          ],
          "line_count": 35
        },
        {
          "start_line": 343,
          "end_line": 362,
          "language": "python",
          "content": [
            "    async def get_adapter(self, server_name: str) -> Optional[MCPAdapter]:",
            "        \"\"\"Smart adapter access with automatic health checking.\"\"\"",
            "        if server_name not in self.adapters:",
            "            logger.warning(f\"Server '{server_name}' not found\")",
            "            return None",
            "        ",
            "        if not self.health_status.get(server_name, False):",
            "            logger.warning(f\"Server '{server_name}' is unhealthy\")",
            "            # The resilience factor - Attempt automatic restart",
            "            await self._restart_server(server_name)",
            "        ",
            "        return self.adapters.get(server_name)",
            "    ",
            "    async def _restart_server(self, name: str):",
            "        \"\"\"Automatic recovery - Attempt to restart a failed server.\"\"\"",
            "        config = self.server_configs.get(name)",
            "        if config:",
            "            await self._start_single_server(name, config)"
          ],
          "line_count": 18
        },
        {
          "start_line": 377,
          "end_line": 401,
          "language": "python",
          "content": [
            "# The ReAct pattern in action - How agents think and act",
            "",
            "from langchain.agents import create_react_agent, AgentExecutor",
            "from langchain_core.prompts import PromptTemplate",
            "from langchain_openai import ChatOpenAI",
            "",
            "# The thinking framework - ReAct prompt template",
            "react_prompt = PromptTemplate.from_template(\"\"\"",
            "You have access to multiple tools. Use this format:",
            "",
            "Question: {input}",
            "Thought: I need to think about which tools to use",
            "Action: [tool_name]",
            "Action Input: [input_for_tool]",
            "Observation: [result_from_tool]",
            "... (repeat Thought/Action as needed)",
            "Thought: I now have enough information",
            "Final Answer: [comprehensive_response]",
            "",
            "Available tools: {tools}",
            "Question: {input}",
            "{agent_scratchpad}",
            "\"\"\")"
          ],
          "line_count": 23
        },
        {
          "start_line": 416,
          "end_line": 451,
          "language": "python",
          "content": [
            "# mcp_servers/weather_server.py - Your meteorological consultant",
            "",
            "from mcp.server.fastmcp import FastMCP",
            "from datetime import datetime",
            "from typing import Dict",
            "",
            "mcp = FastMCP(\"Weather Server\")",
            "",
            "# Realistic weather simulation - Your data foundation",
            "WEATHER_DATA = {",
            "    \"London\": {\"temp\": 15, \"condition\": \"Cloudy\", \"humidity\": 75},",
            "    \"New York\": {\"temp\": 22, \"condition\": \"Sunny\", \"humidity\": 60},",
            "    \"Tokyo\": {\"temp\": 18, \"condition\": \"Rainy\", \"humidity\": 85},",
            "}",
            "",
            "@mcp.tool()",
            "def get_current_weather(city: str, units: str = \"celsius\") -> Dict:",
            "    \"\"\"Get current weather for a city - Your meteorological intelligence.\"\"\"",
            "    if city not in WEATHER_DATA:",
            "        return {\"error\": f\"Weather data not available for {city}\"}",
            "    ",
            "    data = WEATHER_DATA[city].copy()",
            "    if units == \"fahrenheit\":",
            "        data[\"temp\"] = (data[\"temp\"] * 9/5) + 32",
            "        data[\"units\"] = \"\u00b0F\"",
            "    else:",
            "        data[\"units\"] = \"\u00b0C\"",
            "    ",
            "    data[\"city\"] = city",
            "    data[\"timestamp\"] = datetime.now().isoformat()",
            "    return data",
            "",
            "if __name__ == \"__main__\":",
            "    mcp.run()"
          ],
          "line_count": 34
        },
        {
          "start_line": 494,
          "end_line": 501,
          "language": "",
          "content": [
            "User: \"What's the weather in London and do I have any files about UK shipping?\"",
            "",
            "Agent reasoning process:",
            "\u251c\u2500\u2500 Weather query detected \u2192 Use weather tool",
            "\u251c\u2500\u2500 File search detected \u2192 Use filesystem tool  ",
            "\u2514\u2500\u2500 Coordination needed \u2192 Use both tools, then synthesize results"
          ],
          "line_count": 6
        },
        {
          "start_line": 507,
          "end_line": 596,
          "language": "python",
          "content": [
            "# agents/basic_agent.py - Your foundation integration pattern",
            "",
            "import asyncio",
            "import logging",
            "from typing import Optional",
            "from langchain.agents import create_react_agent, AgentExecutor",
            "from langchain_core.prompts import PromptTemplate",
            "from langchain_openai import ChatOpenAI",
            "from langchain_core.tools import Tool",
            "",
            "from utils.mcp_manager import MCPServerManager",
            "from config import Config",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "class BasicMCPAgent:",
            "    \"\"\"A focused ReAct agent using a single MCP server - Your integration foundation.\"\"\"",
            "    ",
            "    def __init__(self, server_name: str, mcp_manager: MCPServerManager):",
            "        self.server_name = server_name",
            "        self.mcp_manager = mcp_manager",
            "        self.llm = None",
            "        self.agent_executor = None",
            "",
            "    async def initialize(self) -> bool:",
            "        \"\"\"The initialization sequence - Setting up your agent's capabilities.\"\"\"",
            "        try:",
            "            # Intelligence layer - Initialize LLM",
            "            self.llm = ChatOpenAI(",
            "                model=Config.LLM.model,",
            "                temperature=Config.LLM.temperature,",
            "                api_key=Config.OPENAI_API_KEY",
            "            )",
            "            ",
            "            # Tool layer - Get MCP adapter and tools",
            "            adapter = await self.mcp_manager.get_adapter(self.server_name)",
            "            if not adapter:",
            "                logger.error(f\"Failed to get adapter: {self.server_name}\")",
            "                return False",
            "            ",
            "            # Integration layer - Convert MCP tools to LangChain tools",
            "            mcp_tools = await adapter.list_tools()",
            "            langchain_tools = self._create_langchain_tools(mcp_tools, adapter)",
            "            ",
            "            # Execution layer - Create the agent executor",
            "            self.agent_executor = self._create_agent_executor(langchain_tools)",
            "            ",
            "            logger.info(f\"Agent initialized with {len(langchain_tools)} tools\")",
            "            return True",
            "            ",
            "        except Exception as e:",
            "            logger.error(f\"Failed to initialize agent: {e}\")",
            "            return False",
            "",
            "    def _create_langchain_tools(self, mcp_tools, adapter):",
            "        \"\"\"The translation layer - Converting MCP tools to LangChain format.\"\"\"",
            "        langchain_tools = []",
            "        ",
            "        for mcp_tool in mcp_tools:",
            "            # The bridge function - Wrapper for MCP tool execution",
            "            async def tool_wrapper(tool_input: str, tool_name=mcp_tool.name):",
            "                \"\"\"Wrapper to call MCP tool from LangChain context.\"\"\"",
            "                try:",
            "                    result = await adapter.call_tool(tool_name, {\"input\": tool_input})",
            "                    return str(result)",
            "                except Exception as e:",
            "                    return f\"Error calling tool {tool_name}: {str(e)}\"",
            "            ",
            "            # The LangChain interface - Create compatible tool",
            "            langchain_tool = Tool(",
            "                name=mcp_tool.name,",
            "                description=mcp_tool.description or f\"Tool from {self.server_name}\",",
            "                func=lambda x, tn=mcp_tool.name: asyncio.create_task(tool_wrapper(x, tn))",
            "            )",
            "            langchain_tools.append(langchain_tool)",
            "        ",
            "        return langchain_tools",
            "    ",
            "    async def run(self, query: str) -> str:",
            "        \"\"\"Execute the agent with a query - Your intelligence in action.\"\"\"",
            "        if not self.agent_executor:",
            "            return \"Agent not initialized. Call initialize() first.\"",
            "        ",
            "        try:",
            "            result = await self.agent_executor.ainvoke({\"input\": query})",
            "            return result[\"output\"]",
            "        except Exception as e:",
            "            return f\"Error processing request: {str(e)}\""
          ],
          "line_count": 88
        },
        {
          "start_line": 636,
          "end_line": 745,
          "language": "python",
          "content": [
            "# agents/multi_tool_agent.py - Your orchestration masterpiece",
            "",
            "import asyncio",
            "import logging",
            "from typing import Dict, List, Any, Optional",
            "from langchain.agents import create_react_agent, AgentExecutor",
            "from langchain_core.prompts import PromptTemplate",
            "from langchain_openai import ChatOpenAI",
            "from langchain_core.tools import Tool",
            "from langchain.memory import ConversationBufferWindowMemory",
            "",
            "from utils.mcp_manager import MCPServerManager",
            "from config import Config",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "class MultiToolMCPAgent:",
            "    \"\"\"Advanced ReAct agent using multiple MCP servers - Your coordination engine.\"\"\"",
            "    ",
            "    def __init__(self, mcp_manager: MCPServerManager):",
            "        self.mcp_manager = mcp_manager",
            "        self.llm = None",
            "        self.agent_executor = None",
            "        self.memory = None",
            "        self.available_tools = {}",
            "",
            "    async def initialize(self) -> bool:",
            "        \"\"\"The comprehensive initialization - Building your multi-tool intelligence.\"\"\"",
            "        try:",
            "            # Intelligence foundation - Initialize LLM",
            "            self.llm = ChatOpenAI(",
            "                model=Config.LLM.model,",
            "                temperature=Config.LLM.temperature,",
            "                api_key=Config.OPENAI_API_KEY",
            "            )",
            "            ",
            "            # Memory system - Initialize conversation memory",
            "            self.memory = ConversationBufferWindowMemory(",
            "                k=10,  # Remember last 10 exchanges",
            "                memory_key=\"chat_history\",",
            "                return_messages=True",
            "            )",
            "            ",
            "            # Tool ecosystem - Collect tools from all available servers",
            "            langchain_tools = await self._collect_all_tools()",
            "            ",
            "            if not langchain_tools:",
            "                logger.error(\"No tools available from MCP servers\")",
            "                return False",
            "            ",
            "            # Agent creation - Create enhanced agent executor",
            "            self.agent_executor = self._create_enhanced_agent(langchain_tools)",
            "            ",
            "            tool_count = len(langchain_tools)",
            "            server_count = len(self.available_tools)",
            "            logger.info(f\"Multi-tool agent: {tool_count} tools from {server_count} servers\")",
            "            ",
            "            return True",
            "            ",
            "        except Exception as e:",
            "            logger.error(f\"Failed to initialize multi-tool agent: {e}\")",
            "            return False",
            "",
            "    def _create_enhanced_agent(self, langchain_tools):",
            "        \"\"\"The intelligence amplifier - Create enhanced ReAct agent with sophisticated prompting.\"\"\"",
            "        react_prompt = PromptTemplate.from_template(\"\"\"",
            "You are an intelligent AI assistant with access to multiple specialized tools.",
            "You can use weather information, file system operations, and database queries.",
            "",
            "STRATEGIC APPROACH:",
            "1. Analyze the user's request carefully",
            "2. Identify which tools might be helpful",
            "3. Use tools in logical sequence",
            "4. Provide comprehensive, helpful responses",
            "5. If a tool fails, try alternative approaches",
            "",
            "Available tools: {tools}",
            "",
            "Use this format:",
            "Question: {input}",
            "Thought: Let me analyze what tools I need",
            "Action: [tool_name]",
            "Action Input: [input_for_tool]",
            "Observation: [result_from_tool]",
            "... (repeat as needed)",
            "Thought: I have enough information",
            "Final Answer: [comprehensive_response]",
            "",
            "Conversation history: {chat_history}",
            "Question: {input}",
            "{agent_scratchpad}",
            "\"\"\")",
            "        ",
            "        agent = create_react_agent(",
            "            llm=self.llm,",
            "            tools=langchain_tools,",
            "            prompt=react_prompt",
            "        )",
            "        ",
            "        return AgentExecutor(",
            "            agent=agent,",
            "            tools=langchain_tools,",
            "            memory=self.memory,",
            "            verbose=Config.AGENT_CONFIG[\"verbose\"],",
            "            max_iterations=Config.AGENT_CONFIG[\"max_iterations\"],",
            "            handle_parsing_errors=True,",
            "            return_intermediate_steps=True",
            "        )"
          ],
          "line_count": 108
        },
        {
          "start_line": 795,
          "end_line": 818,
          "language": "python",
          "content": [
            "# workflows/research_workflow.py - Your orchestration masterpiece",
            "",
            "import asyncio",
            "from typing import Dict, Any, List",
            "from langchain_core.messages import HumanMessage, AIMessage",
            "from langgraph.graph import StateGraph, END",
            "from dataclasses import dataclass",
            "",
            "from utils.mcp_manager import MCPServerManager",
            "from config import Config",
            "",
            "@dataclass",
            "class ResearchState:",
            "    \"\"\"State tracking data through workflow steps - Your information backbone.\"\"\"",
            "    query: str",
            "    messages: List[Any] ",
            "    research_plan: str = \"\"",
            "    weather_data: Dict = None",
            "    file_data: Dict = None",
            "    database_data: Dict = None",
            "    final_report: str = \"\"",
            "    step_count: int = 0"
          ],
          "line_count": 22
        },
        {
          "start_line": 829,
          "end_line": 858,
          "language": "python",
          "content": [
            "class ResearchWorkflow:",
            "    \"\"\"Advanced research workflow using LangGraph and multiple MCP servers.\"\"\"",
            "    ",
            "    def __init__(self, mcp_manager: MCPServerManager):",
            "        self.mcp_manager = mcp_manager",
            "        self.workflow = None",
            "    ",
            "    async def build_workflow(self) -> StateGraph:",
            "        \"\"\"The workflow architect - Build the LangGraph workflow graph.\"\"\"",
            "        workflow = StateGraph(ResearchState)",
            "        ",
            "        # Processing nodes - Your specialized workforce",
            "        workflow.add_node(\"planner\", self._planning_node)",
            "        workflow.add_node(\"weather_researcher\", self._weather_research_node)",
            "        workflow.add_node(\"file_researcher\", self._file_research_node)",
            "        workflow.add_node(\"database_researcher\", self._database_research_node)",
            "        workflow.add_node(\"synthesizer\", self._synthesis_node)",
            "        ",
            "        # Execution flow - Your process choreography",
            "        workflow.set_entry_point(\"planner\")",
            "        workflow.add_edge(\"planner\", \"weather_researcher\")",
            "        workflow.add_edge(\"weather_researcher\", \"file_researcher\")",
            "        workflow.add_edge(\"file_researcher\", \"database_researcher\")",
            "        workflow.add_edge(\"database_researcher\", \"synthesizer\")",
            "        workflow.add_edge(\"synthesizer\", END)",
            "        ",
            "        self.workflow = workflow.compile()",
            "        return self.workflow"
          ],
          "line_count": 28
        },
        {
          "start_line": 869,
          "end_line": 889,
          "language": "python",
          "content": [
            "    async def _planning_node(self, state: ResearchState) -> ResearchState:",
            "        \"\"\"The strategic planner - Plan research approach based on query analysis.\"\"\"",
            "        query_lower = state.query.lower()",
            "        plan_elements = []",
            "        ",
            "        # Intelligent analysis - Query analysis for different research domains",
            "        if any(word in query_lower for word in [\"weather\", \"climate\", \"temperature\"]):",
            "            plan_elements.append(\"- Gather weather information\")",
            "        ",
            "        if any(word in query_lower for word in [\"file\", \"document\", \"data\"]):",
            "            plan_elements.append(\"- Search relevant files\")",
            "        ",
            "        if any(word in query_lower for word in [\"database\", \"record\", \"history\"]):",
            "            plan_elements.append(\"- Query database for information\")",
            "        ",
            "        # Strategic documentation - Build research plan",
            "        state.research_plan = \"Research Plan:\\n\" + \"\\n\".join(plan_elements) if plan_elements else \"General research\"",
            "        state.step_count += 1",
            "        return state"
          ],
          "line_count": 19
        },
        {
          "start_line": 900,
          "end_line": 952,
          "language": "python",
          "content": [
            "    async def _weather_research_node(self, state: ResearchState) -> ResearchState:",
            "        \"\"\"The meteorological specialist - Research weather information if relevant.\"\"\"",
            "        if \"weather\" not in state.query.lower():",
            "            state.weather_data = {\"skipped\": True}",
            "            return state",
            "        ",
            "        try:",
            "            adapter = await self.mcp_manager.get_adapter(\"weather\")",
            "            if adapter:",
            "                cities = self._extract_cities_from_query(state.query)",
            "                weather_results = {}",
            "                ",
            "                for city in cities:",
            "                    try:",
            "                        result = await adapter.call_tool(\"get_current_weather\", {\"city\": city})",
            "                        weather_results[city] = result",
            "                    except:",
            "                        continue  # Resilient processing - Try other cities",
            "                ",
            "                state.weather_data = weather_results or {\"error\": \"No weather data\"}",
            "            else:",
            "                state.weather_data = {\"error\": \"Weather server unavailable\"}",
            "        ",
            "        except Exception as e:",
            "            state.weather_data = {\"error\": str(e)}",
            "        ",
            "        state.step_count += 1",
            "        return state",
            "    ",
            "    async def run_research(self, query: str) -> Dict[str, Any]:",
            "        \"\"\"The workflow executor - Execute the complete research workflow.\"\"\"",
            "        if not self.workflow:",
            "            await self.build_workflow()",
            "        ",
            "        initial_state = ResearchState(query=query, messages=[HumanMessage(content=query)])",
            "        ",
            "        try:",
            "            final_state = await self.workflow.ainvoke(initial_state)",
            "            return {",
            "                \"success\": True,",
            "                \"query\": query,",
            "                \"report\": final_state.final_report,",
            "                \"steps\": final_state.step_count",
            "            }",
            "        except Exception as e:",
            "            return {",
            "                \"success\": False,",
            "                \"query\": query,",
            "                \"error\": str(e),",
            "                \"report\": f\"Research workflow failed: {str(e)}\"",
            "            }"
          ],
          "line_count": 51
        },
        {
          "start_line": 1029,
          "end_line": 1041,
          "language": "python",
          "content": [
            "from langchain_mcp import MCPToolkit",
            "from langchain.agents import initialize_agent, AgentType",
            "from langchain.llm import OpenAI",
            "",
            "# Your multi-server agent setup - Building your digital travel consultant",
            "weather_toolkit = MCPToolkit.from_server(\"weather-server\")",
            "file_toolkit = MCPToolkit.from_server(\"filesystem-server\") ",
            "preference_toolkit = MCPToolkit.from_server(\"preference-server\")",
            "",
            "# Integration mastery - Combine toolkits and create agent",
            "all_tools = weather_toolkit.get_tools() + file_toolkit.get_tools() + preference_toolkit.get_tools()"
          ],
          "line_count": 11
        }
      ],
      "large_blocks": [
        {
          "start_line": 46,
          "end_line": 68,
          "language": "python",
          "content": [
            "# Basic MCP-LangChain integration - The moment two worlds become one",
            "",
            "from langchain_mcp_adapters import MultiServerMCPClient",
            "from langchain.agents import create_react_agent",
            "from langchain_openai import ChatOpenAI",
            "",
            "# Connect to multiple MCP servers - Building your digital ecosystem",
            "client = MultiServerMCPClient({",
            "    \"weather\": {\"command\": \"python\", \"args\": [\"weather_server.py\"]},",
            "    \"files\": {\"command\": \"python\", \"args\": [\"file_server.py\"]}",
            "})",
            "",
            "# Get tools from all servers - Your agent's superpowers",
            "tools = client.list_tools()",
            "",
            "# Create intelligent agent - The mind that connects it all",
            "agent = create_react_agent(",
            "    llm=ChatOpenAI(model=\"gpt-4\"),",
            "    tools=tools,",
            "    prompt=\"You are a helpful assistant with access to multiple tools.\"",
            ")"
          ],
          "line_count": 21
        },
        {
          "start_line": 143,
          "end_line": 172,
          "language": "python",
          "content": [
            "# config.py - Your integration command center",
            "",
            "import os",
            "from typing import Dict, Any, List",
            "from dataclasses import dataclass",
            "from dotenv import load_dotenv",
            "",
            "load_dotenv()",
            "",
            "@dataclass",
            "class MCPServerConfig:",
            "    \"\"\"Configuration for a single MCP server - Your digital specialist definition.\"\"\"",
            "    name: str",
            "    command: str",
            "    args: List[str]",
            "    transport: str = \"stdio\"",
            "    description: str = \"\"",
            "    timeout: int = 30",
            "    retry_attempts: int = 3",
            "",
            "@dataclass ",
            "class LLMConfig:",
            "    \"\"\"Configuration for language models - Your intelligence settings.\"\"\"",
            "    provider: str = \"openai\"",
            "    model: str = \"gpt-4\"",
            "    temperature: float = 0.7",
            "    max_tokens: int = 2000",
            "    timeout: int = 60"
          ],
          "line_count": 28
        },
        {
          "start_line": 183,
          "end_line": 233,
          "language": "python",
          "content": [
            "# Your main configuration orchestrator",
            "",
            "class Config:",
            "    \"\"\"Main configuration class for LangChain MCP integration.\"\"\"",
            "    ",
            "    # API Keys from environment variables - Security first",
            "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")",
            "    ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")",
            "    ",
            "    # LLM Configuration with environment overrides - Flexibility with defaults",
            "    LLM = LLMConfig(",
            "        provider=os.getenv(\"LLM_PROVIDER\", \"openai\"),",
            "        model=os.getenv(\"LLM_MODEL\", \"gpt-4\"),",
            "        temperature=float(os.getenv(\"LLM_TEMPERATURE\", \"0.7\")),",
            "    )",
            "    ",
            "    # MCP Server Registry - Your digital workforce roster",
            "    MCP_SERVERS = [",
            "        MCPServerConfig(",
            "            name=\"weather\",",
            "            command=\"python\",",
            "            args=[\"mcp_servers/weather_server.py\"],",
            "            description=\"Weather information and forecasts\"",
            "        ),",
            "        MCPServerConfig(",
            "            name=\"filesystem\", ",
            "            command=\"python\",",
            "            args=[\"mcp_servers/filesystem_server.py\"],",
            "            description=\"Secure file system operations\"",
            "        ),",
            "        MCPServerConfig(",
            "            name=\"database\",",
            "            command=\"python\", ",
            "            args=[\"mcp_servers/database_server.py\"],",
            "            description=\"Database query and manipulation\"",
            "        )",
            "    ]",
            "    ",
            "    # Agent Configuration - Intelligence parameters",
            "    AGENT_CONFIG = {",
            "        \"max_iterations\": int(os.getenv(\"MAX_ITERATIONS\", \"10\")),",
            "        \"verbose\": os.getenv(\"VERBOSE\", \"true\").lower() == \"true\",",
            "        \"temperature\": float(os.getenv(\"AGENT_TEMPERATURE\", \"0.7\")),",
            "        \"timeout\": int(os.getenv(\"AGENT_TIMEOUT\", \"300\"))  # 5 minutes",
            "    }",
            "    ",
            "    # Logging Configuration - Observability settings",
            "    LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\")",
            "    LOG_FORMAT = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\""
          ],
          "line_count": 49
        },
        {
          "start_line": 305,
          "end_line": 341,
          "language": "python",
          "content": [
            "    async def start_all_servers(self) -> Dict[str, bool]:",
            "        \"\"\"The startup orchestration - Bringing your digital workforce online.\"\"\"",
            "        results = {}",
            "        ",
            "        for name, config in self.server_configs.items():",
            "            result = await self._start_single_server(name, config)",
            "            results[name] = result",
            "        ",
            "        return results",
            "    ",
            "    async def _start_single_server(self, name: str, config: MCPServerConfig) -> bool:",
            "        \"\"\"Individual server startup with comprehensive validation.\"\"\"",
            "        try:",
            "            logger.info(f\"Starting MCP server: {name}\")",
            "            adapter = MCPAdapter(",
            "                command=config.command,",
            "                args=config.args,",
            "                timeout=config.timeout",
            "            )",
            "            ",
            "            # The critical handshake - Test connection and discover tools",
            "            await adapter.start()",
            "            tools = await adapter.list_tools()",
            "            ",
            "            # Success - Store adapter and update status",
            "            self.adapters[name] = adapter",
            "            self.health_status[name] = True",
            "            ",
            "            logger.info(f\"Server '{name}' started with {len(tools)} tools\")",
            "            return True",
            "            ",
            "        except Exception as e:",
            "            logger.error(f\"Failed to start server '{name}': {e}\")",
            "            self.health_status[name] = False",
            "            return False"
          ],
          "line_count": 35
        },
        {
          "start_line": 377,
          "end_line": 401,
          "language": "python",
          "content": [
            "# The ReAct pattern in action - How agents think and act",
            "",
            "from langchain.agents import create_react_agent, AgentExecutor",
            "from langchain_core.prompts import PromptTemplate",
            "from langchain_openai import ChatOpenAI",
            "",
            "# The thinking framework - ReAct prompt template",
            "react_prompt = PromptTemplate.from_template(\"\"\"",
            "You have access to multiple tools. Use this format:",
            "",
            "Question: {input}",
            "Thought: I need to think about which tools to use",
            "Action: [tool_name]",
            "Action Input: [input_for_tool]",
            "Observation: [result_from_tool]",
            "... (repeat Thought/Action as needed)",
            "Thought: I now have enough information",
            "Final Answer: [comprehensive_response]",
            "",
            "Available tools: {tools}",
            "Question: {input}",
            "{agent_scratchpad}",
            "\"\"\")"
          ],
          "line_count": 23
        },
        {
          "start_line": 416,
          "end_line": 451,
          "language": "python",
          "content": [
            "# mcp_servers/weather_server.py - Your meteorological consultant",
            "",
            "from mcp.server.fastmcp import FastMCP",
            "from datetime import datetime",
            "from typing import Dict",
            "",
            "mcp = FastMCP(\"Weather Server\")",
            "",
            "# Realistic weather simulation - Your data foundation",
            "WEATHER_DATA = {",
            "    \"London\": {\"temp\": 15, \"condition\": \"Cloudy\", \"humidity\": 75},",
            "    \"New York\": {\"temp\": 22, \"condition\": \"Sunny\", \"humidity\": 60},",
            "    \"Tokyo\": {\"temp\": 18, \"condition\": \"Rainy\", \"humidity\": 85},",
            "}",
            "",
            "@mcp.tool()",
            "def get_current_weather(city: str, units: str = \"celsius\") -> Dict:",
            "    \"\"\"Get current weather for a city - Your meteorological intelligence.\"\"\"",
            "    if city not in WEATHER_DATA:",
            "        return {\"error\": f\"Weather data not available for {city}\"}",
            "    ",
            "    data = WEATHER_DATA[city].copy()",
            "    if units == \"fahrenheit\":",
            "        data[\"temp\"] = (data[\"temp\"] * 9/5) + 32",
            "        data[\"units\"] = \"\u00b0F\"",
            "    else:",
            "        data[\"units\"] = \"\u00b0C\"",
            "    ",
            "    data[\"city\"] = city",
            "    data[\"timestamp\"] = datetime.now().isoformat()",
            "    return data",
            "",
            "if __name__ == \"__main__\":",
            "    mcp.run()"
          ],
          "line_count": 34
        },
        {
          "start_line": 507,
          "end_line": 596,
          "language": "python",
          "content": [
            "# agents/basic_agent.py - Your foundation integration pattern",
            "",
            "import asyncio",
            "import logging",
            "from typing import Optional",
            "from langchain.agents import create_react_agent, AgentExecutor",
            "from langchain_core.prompts import PromptTemplate",
            "from langchain_openai import ChatOpenAI",
            "from langchain_core.tools import Tool",
            "",
            "from utils.mcp_manager import MCPServerManager",
            "from config import Config",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "class BasicMCPAgent:",
            "    \"\"\"A focused ReAct agent using a single MCP server - Your integration foundation.\"\"\"",
            "    ",
            "    def __init__(self, server_name: str, mcp_manager: MCPServerManager):",
            "        self.server_name = server_name",
            "        self.mcp_manager = mcp_manager",
            "        self.llm = None",
            "        self.agent_executor = None",
            "",
            "    async def initialize(self) -> bool:",
            "        \"\"\"The initialization sequence - Setting up your agent's capabilities.\"\"\"",
            "        try:",
            "            # Intelligence layer - Initialize LLM",
            "            self.llm = ChatOpenAI(",
            "                model=Config.LLM.model,",
            "                temperature=Config.LLM.temperature,",
            "                api_key=Config.OPENAI_API_KEY",
            "            )",
            "            ",
            "            # Tool layer - Get MCP adapter and tools",
            "            adapter = await self.mcp_manager.get_adapter(self.server_name)",
            "            if not adapter:",
            "                logger.error(f\"Failed to get adapter: {self.server_name}\")",
            "                return False",
            "            ",
            "            # Integration layer - Convert MCP tools to LangChain tools",
            "            mcp_tools = await adapter.list_tools()",
            "            langchain_tools = self._create_langchain_tools(mcp_tools, adapter)",
            "            ",
            "            # Execution layer - Create the agent executor",
            "            self.agent_executor = self._create_agent_executor(langchain_tools)",
            "            ",
            "            logger.info(f\"Agent initialized with {len(langchain_tools)} tools\")",
            "            return True",
            "            ",
            "        except Exception as e:",
            "            logger.error(f\"Failed to initialize agent: {e}\")",
            "            return False",
            "",
            "    def _create_langchain_tools(self, mcp_tools, adapter):",
            "        \"\"\"The translation layer - Converting MCP tools to LangChain format.\"\"\"",
            "        langchain_tools = []",
            "        ",
            "        for mcp_tool in mcp_tools:",
            "            # The bridge function - Wrapper for MCP tool execution",
            "            async def tool_wrapper(tool_input: str, tool_name=mcp_tool.name):",
            "                \"\"\"Wrapper to call MCP tool from LangChain context.\"\"\"",
            "                try:",
            "                    result = await adapter.call_tool(tool_name, {\"input\": tool_input})",
            "                    return str(result)",
            "                except Exception as e:",
            "                    return f\"Error calling tool {tool_name}: {str(e)}\"",
            "            ",
            "            # The LangChain interface - Create compatible tool",
            "            langchain_tool = Tool(",
            "                name=mcp_tool.name,",
            "                description=mcp_tool.description or f\"Tool from {self.server_name}\",",
            "                func=lambda x, tn=mcp_tool.name: asyncio.create_task(tool_wrapper(x, tn))",
            "            )",
            "            langchain_tools.append(langchain_tool)",
            "        ",
            "        return langchain_tools",
            "    ",
            "    async def run(self, query: str) -> str:",
            "        \"\"\"Execute the agent with a query - Your intelligence in action.\"\"\"",
            "        if not self.agent_executor:",
            "            return \"Agent not initialized. Call initialize() first.\"",
            "        ",
            "        try:",
            "            result = await self.agent_executor.ainvoke({\"input\": query})",
            "            return result[\"output\"]",
            "        except Exception as e:",
            "            return f\"Error processing request: {str(e)}\""
          ],
          "line_count": 88
        },
        {
          "start_line": 636,
          "end_line": 745,
          "language": "python",
          "content": [
            "# agents/multi_tool_agent.py - Your orchestration masterpiece",
            "",
            "import asyncio",
            "import logging",
            "from typing import Dict, List, Any, Optional",
            "from langchain.agents import create_react_agent, AgentExecutor",
            "from langchain_core.prompts import PromptTemplate",
            "from langchain_openai import ChatOpenAI",
            "from langchain_core.tools import Tool",
            "from langchain.memory import ConversationBufferWindowMemory",
            "",
            "from utils.mcp_manager import MCPServerManager",
            "from config import Config",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "class MultiToolMCPAgent:",
            "    \"\"\"Advanced ReAct agent using multiple MCP servers - Your coordination engine.\"\"\"",
            "    ",
            "    def __init__(self, mcp_manager: MCPServerManager):",
            "        self.mcp_manager = mcp_manager",
            "        self.llm = None",
            "        self.agent_executor = None",
            "        self.memory = None",
            "        self.available_tools = {}",
            "",
            "    async def initialize(self) -> bool:",
            "        \"\"\"The comprehensive initialization - Building your multi-tool intelligence.\"\"\"",
            "        try:",
            "            # Intelligence foundation - Initialize LLM",
            "            self.llm = ChatOpenAI(",
            "                model=Config.LLM.model,",
            "                temperature=Config.LLM.temperature,",
            "                api_key=Config.OPENAI_API_KEY",
            "            )",
            "            ",
            "            # Memory system - Initialize conversation memory",
            "            self.memory = ConversationBufferWindowMemory(",
            "                k=10,  # Remember last 10 exchanges",
            "                memory_key=\"chat_history\",",
            "                return_messages=True",
            "            )",
            "            ",
            "            # Tool ecosystem - Collect tools from all available servers",
            "            langchain_tools = await self._collect_all_tools()",
            "            ",
            "            if not langchain_tools:",
            "                logger.error(\"No tools available from MCP servers\")",
            "                return False",
            "            ",
            "            # Agent creation - Create enhanced agent executor",
            "            self.agent_executor = self._create_enhanced_agent(langchain_tools)",
            "            ",
            "            tool_count = len(langchain_tools)",
            "            server_count = len(self.available_tools)",
            "            logger.info(f\"Multi-tool agent: {tool_count} tools from {server_count} servers\")",
            "            ",
            "            return True",
            "            ",
            "        except Exception as e:",
            "            logger.error(f\"Failed to initialize multi-tool agent: {e}\")",
            "            return False",
            "",
            "    def _create_enhanced_agent(self, langchain_tools):",
            "        \"\"\"The intelligence amplifier - Create enhanced ReAct agent with sophisticated prompting.\"\"\"",
            "        react_prompt = PromptTemplate.from_template(\"\"\"",
            "You are an intelligent AI assistant with access to multiple specialized tools.",
            "You can use weather information, file system operations, and database queries.",
            "",
            "STRATEGIC APPROACH:",
            "1. Analyze the user's request carefully",
            "2. Identify which tools might be helpful",
            "3. Use tools in logical sequence",
            "4. Provide comprehensive, helpful responses",
            "5. If a tool fails, try alternative approaches",
            "",
            "Available tools: {tools}",
            "",
            "Use this format:",
            "Question: {input}",
            "Thought: Let me analyze what tools I need",
            "Action: [tool_name]",
            "Action Input: [input_for_tool]",
            "Observation: [result_from_tool]",
            "... (repeat as needed)",
            "Thought: I have enough information",
            "Final Answer: [comprehensive_response]",
            "",
            "Conversation history: {chat_history}",
            "Question: {input}",
            "{agent_scratchpad}",
            "\"\"\")",
            "        ",
            "        agent = create_react_agent(",
            "            llm=self.llm,",
            "            tools=langchain_tools,",
            "            prompt=react_prompt",
            "        )",
            "        ",
            "        return AgentExecutor(",
            "            agent=agent,",
            "            tools=langchain_tools,",
            "            memory=self.memory,",
            "            verbose=Config.AGENT_CONFIG[\"verbose\"],",
            "            max_iterations=Config.AGENT_CONFIG[\"max_iterations\"],",
            "            handle_parsing_errors=True,",
            "            return_intermediate_steps=True",
            "        )"
          ],
          "line_count": 108
        },
        {
          "start_line": 795,
          "end_line": 818,
          "language": "python",
          "content": [
            "# workflows/research_workflow.py - Your orchestration masterpiece",
            "",
            "import asyncio",
            "from typing import Dict, Any, List",
            "from langchain_core.messages import HumanMessage, AIMessage",
            "from langgraph.graph import StateGraph, END",
            "from dataclasses import dataclass",
            "",
            "from utils.mcp_manager import MCPServerManager",
            "from config import Config",
            "",
            "@dataclass",
            "class ResearchState:",
            "    \"\"\"State tracking data through workflow steps - Your information backbone.\"\"\"",
            "    query: str",
            "    messages: List[Any] ",
            "    research_plan: str = \"\"",
            "    weather_data: Dict = None",
            "    file_data: Dict = None",
            "    database_data: Dict = None",
            "    final_report: str = \"\"",
            "    step_count: int = 0"
          ],
          "line_count": 22
        },
        {
          "start_line": 829,
          "end_line": 858,
          "language": "python",
          "content": [
            "class ResearchWorkflow:",
            "    \"\"\"Advanced research workflow using LangGraph and multiple MCP servers.\"\"\"",
            "    ",
            "    def __init__(self, mcp_manager: MCPServerManager):",
            "        self.mcp_manager = mcp_manager",
            "        self.workflow = None",
            "    ",
            "    async def build_workflow(self) -> StateGraph:",
            "        \"\"\"The workflow architect - Build the LangGraph workflow graph.\"\"\"",
            "        workflow = StateGraph(ResearchState)",
            "        ",
            "        # Processing nodes - Your specialized workforce",
            "        workflow.add_node(\"planner\", self._planning_node)",
            "        workflow.add_node(\"weather_researcher\", self._weather_research_node)",
            "        workflow.add_node(\"file_researcher\", self._file_research_node)",
            "        workflow.add_node(\"database_researcher\", self._database_research_node)",
            "        workflow.add_node(\"synthesizer\", self._synthesis_node)",
            "        ",
            "        # Execution flow - Your process choreography",
            "        workflow.set_entry_point(\"planner\")",
            "        workflow.add_edge(\"planner\", \"weather_researcher\")",
            "        workflow.add_edge(\"weather_researcher\", \"file_researcher\")",
            "        workflow.add_edge(\"file_researcher\", \"database_researcher\")",
            "        workflow.add_edge(\"database_researcher\", \"synthesizer\")",
            "        workflow.add_edge(\"synthesizer\", END)",
            "        ",
            "        self.workflow = workflow.compile()",
            "        return self.workflow"
          ],
          "line_count": 28
        },
        {
          "start_line": 900,
          "end_line": 952,
          "language": "python",
          "content": [
            "    async def _weather_research_node(self, state: ResearchState) -> ResearchState:",
            "        \"\"\"The meteorological specialist - Research weather information if relevant.\"\"\"",
            "        if \"weather\" not in state.query.lower():",
            "            state.weather_data = {\"skipped\": True}",
            "            return state",
            "        ",
            "        try:",
            "            adapter = await self.mcp_manager.get_adapter(\"weather\")",
            "            if adapter:",
            "                cities = self._extract_cities_from_query(state.query)",
            "                weather_results = {}",
            "                ",
            "                for city in cities:",
            "                    try:",
            "                        result = await adapter.call_tool(\"get_current_weather\", {\"city\": city})",
            "                        weather_results[city] = result",
            "                    except:",
            "                        continue  # Resilient processing - Try other cities",
            "                ",
            "                state.weather_data = weather_results or {\"error\": \"No weather data\"}",
            "            else:",
            "                state.weather_data = {\"error\": \"Weather server unavailable\"}",
            "        ",
            "        except Exception as e:",
            "            state.weather_data = {\"error\": str(e)}",
            "        ",
            "        state.step_count += 1",
            "        return state",
            "    ",
            "    async def run_research(self, query: str) -> Dict[str, Any]:",
            "        \"\"\"The workflow executor - Execute the complete research workflow.\"\"\"",
            "        if not self.workflow:",
            "            await self.build_workflow()",
            "        ",
            "        initial_state = ResearchState(query=query, messages=[HumanMessage(content=query)])",
            "        ",
            "        try:",
            "            final_state = await self.workflow.ainvoke(initial_state)",
            "            return {",
            "                \"success\": True,",
            "                \"query\": query,",
            "                \"report\": final_state.final_report,",
            "                \"steps\": final_state.step_count",
            "            }",
            "        except Exception as e:",
            "            return {",
            "                \"success\": False,",
            "                \"query\": query,",
            "                \"error\": str(e),",
            "                \"report\": f\"Research workflow failed: {str(e)}\"",
            "            }"
          ],
          "line_count": 51
        }
      ],
      "needs_refactoring": true
    }
  ]
}