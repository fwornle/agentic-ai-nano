{
  "summary": {
    "total_files": 1,
    "files_needing_refactoring": 0,
    "total_large_blocks": 0
  },
  "files": [
    {
      "file": "docs-content/02_rag/Session6_Graph_Based_RAG.md",
      "total_code_blocks": 12,
      "large_blocks_count": 0,
      "code_blocks": [
        {
          "start_line": 45,
          "end_line": 48,
          "language": "",
          "content": [
            "Traditional RAG: Document \u2192 Chunks \u2192 Uniform Embeddings \u2192 Similarity Search",
            "NodeRAG: Document \u2192 Specialized Nodes \u2192 Heterogeneous Graph \u2192 Reasoning Pathways"
          ],
          "line_count": 2
        },
        {
          "start_line": 126,
          "end_line": 138,
          "language": "python",
          "content": [
            "import spacy",
            "from typing import List, Dict, Any, Tuple",
            "import networkx as nx",
            "",
            "class TraditionalGraphRAG:",
            "    \"\"\"Traditional GraphRAG implementation\"\"\"",
            "    ",
            "    def __init__(self):",
            "        # Load spaCy model for entity extraction",
            "        self.nlp = spacy.load(\"en_core_web_sm\")",
            "        self.graph = nx.Graph()"
          ],
          "line_count": 11
        },
        {
          "start_line": 142,
          "end_line": 157,
          "language": "python",
          "content": [
            "    def extract_entities_and_relationships(self, text: str):",
            "        \"\"\"Extract entities and relationships from text\"\"\"",
            "        ",
            "        doc = self.nlp(text)",
            "        ",
            "        entities = []",
            "        for ent in doc.ents:",
            "            if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"PRODUCT\"]:",
            "                entities.append({",
            "                    'text': ent.text,",
            "                    'label': ent.label_,",
            "                    'start': ent.start_char,",
            "                    'end': ent.end_char",
            "                })"
          ],
          "line_count": 14
        },
        {
          "start_line": 161,
          "end_line": 174,
          "language": "python",
          "content": [
            "        # Simple relationship extraction using dependency parsing",
            "        relationships = []",
            "        for token in doc:",
            "            if token.dep_ in [\"nsubj\", \"dobj\"] and token.head.pos_ == \"VERB\":",
            "                relationships.append({",
            "                    'subject': token.text,",
            "                    'predicate': token.head.text,",
            "                    'object': [child.text for child in token.head.children ",
            "                             if child.dep_ in [\"dobj\", \"attr\"]]",
            "                })",
            "        ",
            "        return entities, relationships"
          ],
          "line_count": 12
        },
        {
          "start_line": 180,
          "end_line": 191,
          "language": "python",
          "content": [
            "    def build_knowledge_graph(self, documents: List[str]):",
            "        \"\"\"Build knowledge graph from multiple documents\"\"\"",
            "        ",
            "        all_entities = []",
            "        all_relationships = []",
            "        ",
            "        for doc in documents:",
            "            entities, relationships = self.extract_entities_and_relationships(doc)",
            "            all_entities.extend(entities)",
            "            all_relationships.extend(relationships)"
          ],
          "line_count": 10
        },
        {
          "start_line": 195,
          "end_line": 213,
          "language": "python",
          "content": [
            "        # Add entities as nodes",
            "        for entity in all_entities:",
            "            if not self.graph.has_node(entity['text']):",
            "                self.graph.add_node(",
            "                    entity['text'], ",
            "                    type=entity['label'],",
            "                    entity_type='traditional'",
            "                )",
            "        ",
            "        # Add relationships as edges",
            "        for rel in all_relationships:",
            "            if rel['object']:",
            "                self.graph.add_edge(",
            "                    rel['subject'],",
            "                    rel['object'][0],  # Take first object for simplicity",
            "                    relationship=rel['predicate']",
            "                )"
          ],
          "line_count": 17
        },
        {
          "start_line": 219,
          "end_line": 229,
          "language": "python",
          "content": [
            "    def query_graph(self, query: str, max_hops: int = 3):",
            "        \"\"\"Query the knowledge graph for relevant information\"\"\"",
            "        ",
            "        # Extract entities from query",
            "        query_doc = self.nlp(query)",
            "        query_entities = [ent.text for ent in query_doc.ents]",
            "        ",
            "        # Find paths between query entities",
            "        relevant_paths = []"
          ],
          "line_count": 9
        },
        {
          "start_line": 233,
          "end_line": 248,
          "language": "python",
          "content": [
            "        for i, entity1 in enumerate(query_entities):",
            "            for entity2 in query_entities[i+1:]:",
            "                if (self.graph.has_node(entity1) and ",
            "                    self.graph.has_node(entity2)):",
            "                    try:",
            "                        path = nx.shortest_path(",
            "                            self.graph, entity1, entity2",
            "                        )",
            "                        if len(path) <= max_hops + 1:",
            "                            relevant_paths.append(path)",
            "                    except nx.NetworkXNoPath:",
            "                        continue",
            "        ",
            "        return relevant_paths"
          ],
          "line_count": 14
        },
        {
          "start_line": 269,
          "end_line": 279,
          "language": "python",
          "content": [
            "import ast",
            "from typing import Dict, List, Any",
            "",
            "class CodeGraphRAG:",
            "    \"\"\"Code-specialized GraphRAG implementation\"\"\"",
            "    ",
            "    def __init__(self):",
            "        self.code_graph = nx.DiGraph()  # Directed graph for code dependencies",
            "        self.file_asts = {}"
          ],
          "line_count": 9
        },
        {
          "start_line": 283,
          "end_line": 302,
          "language": "python",
          "content": [
            "    def analyze_python_file(self, file_path: str, content: str):",
            "        \"\"\"Analyze Python file and extract code entities\"\"\"",
            "        ",
            "        try:",
            "            tree = ast.parse(content)",
            "            self.file_asts[file_path] = tree",
            "            ",
            "            # Extract functions, classes, and imports",
            "            for node in ast.walk(tree):",
            "                if isinstance(node, ast.FunctionDef):",
            "                    self.add_function_node(node, file_path)",
            "                elif isinstance(node, ast.ClassDef):",
            "                    self.add_class_node(node, file_path)",
            "                elif isinstance(node, ast.Import):",
            "                    self.add_import_relationships(node, file_path)",
            "                    ",
            "        except SyntaxError:",
            "            print(f\"Syntax error in {file_path}\")"
          ],
          "line_count": 18
        },
        {
          "start_line": 312,
          "end_line": 320,
          "language": "python",
          "content": [
            "class HybridGraphRAG:",
            "    \"\"\"Hybrid system combining graph and vector approaches\"\"\"",
            "    ",
            "    def __init__(self, graph_store, vector_store):",
            "        self.graph_rag = TraditionalGraphRAG()",
            "        self.vector_rag = VectorRAG(vector_store)",
            "        self.fusion_engine = ResultFusionEngine()"
          ],
          "line_count": 7
        },
        {
          "start_line": 349,
          "end_line": 367,
          "language": "python",
          "content": [
            "# Complete implementation available in advanced modules",
            "from traditional_graph_rag import TraditionalGraphRAG",
            "from hybrid_graph_vector import HybridGraphRAG",
            "",
            "# Initialize hybrid system",
            "hybrid_rag = HybridGraphRAG(",
            "    graph_store=\"neo4j://localhost:7687\",",
            "    vector_store=\"chroma_db\"",
            ")",
            "",
            "# Process documents",
            "documents = [\"document1.txt\", \"document2.txt\"]",
            "hybrid_rag.process_documents(documents)",
            "",
            "# Query the system",
            "result = hybrid_rag.query(\"What are the partnerships between tech companies?\")",
            "print(result)"
          ],
          "line_count": 17
        }
      ],
      "large_blocks": [],
      "needs_refactoring": false
    }
  ]
}