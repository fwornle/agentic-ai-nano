{
  "summary": {
    "total_files": 1,
    "files_needing_refactoring": 1,
    "total_large_blocks": 2
  },
  "files": [
    {
      "file": "/Users/q284340/Agentic/nano-degree/docs-content/02_rag/Session9_Production_RAG_Enterprise_Integration.md",
      "total_code_blocks": 103,
      "large_blocks_count": 2,
      "code_blocks": [
        {
          "start_line": 53,
          "end_line": 64,
          "language": "python",
          "content": [
            "",
            "# Production-ready containerized RAG system",
            "",
            "from typing import Dict, List, Any, Optional",
            "import asyncio",
            "from dataclasses import dataclass",
            "from enum import Enum",
            "import logging",
            "import time",
            "from datetime import datetime"
          ],
          "line_count": 10
        },
        {
          "start_line": 68,
          "end_line": 83,
          "language": "python",
          "content": [
            "class ServiceStatus(Enum):",
            "    HEALTHY = \"healthy\"",
            "    DEGRADED = \"degraded\"",
            "    UNHEALTHY = \"unhealthy\"",
            "",
            "@dataclass",
            "class ServiceHealth:",
            "    \"\"\"Health check result for RAG services.\"\"\"",
            "    service_name: str",
            "    status: ServiceStatus",
            "    response_time_ms: float",
            "    error_count: int",
            "    last_check: datetime",
            "    details: Dict[str, Any]"
          ],
          "line_count": 14
        },
        {
          "start_line": 89,
          "end_line": 97,
          "language": "python",
          "content": [
            "class RAGServiceOrchestrator:",
            "    \"\"\"Production orchestrator for RAG microservices.\"\"\"",
            "",
            "    def __init__(self, service_config: Dict[str, Any]):",
            "        self.service_config = service_config",
            "        self.services = {}",
            "        self.health_monitors = {}"
          ],
          "line_count": 7
        },
        {
          "start_line": 101,
          "end_line": 111,
          "language": "python",
          "content": [
            "        # Service registry maps service names to their implementation classes",
            "        self.service_registry = {",
            "            'document_processor': DocumentProcessingService,",
            "            'embeddings_service': EmbeddingService,",
            "            'vector_store': VectorStoreService,",
            "            'retrieval_service': RetrievalService,",
            "            'generation_service': GenerationService,",
            "            'orchestration_api': OrchestrationAPIService",
            "        }"
          ],
          "line_count": 9
        },
        {
          "start_line": 115,
          "end_line": 120,
          "language": "python",
          "content": [
            "        # Initialize critical production components",
            "        self.load_balancer = RAGLoadBalancer()",
            "        self.health_checker = ServiceHealthChecker()",
            "        self.logger = logging.getLogger(__name__)"
          ],
          "line_count": 4
        },
        {
          "start_line": 126,
          "end_line": 138,
          "language": "python",
          "content": [
            "    async def start_services(self) -> Dict[str, Any]:",
            "        \"\"\"Start all RAG services with health monitoring.\"\"\"",
            "",
            "        startup_results = {}",
            "",
            "        # Define startup order based on service dependencies",
            "        # Vector store must start first, followed by embeddings, then higher-level services",
            "        service_start_order = [",
            "            'vector_store', 'embeddings_service', 'document_processor',",
            "            'retrieval_service', 'generation_service', 'orchestration_api'",
            "        ]"
          ],
          "line_count": 11
        },
        {
          "start_line": 142,
          "end_line": 162,
          "language": "python",
          "content": [
            "        for service_name in service_start_order:",
            "            if service_name in self.service_config:",
            "                try:",
            "                    # Start individual service",
            "                    service_instance = await self._start_service(service_name)",
            "                    self.services[service_name] = service_instance",
            "",
            "                    # Enable health monitoring for the service",
            "                    health_monitor = await self._start_health_monitoring(",
            "                        service_name, service_instance",
            "                    )",
            "                    self.health_monitors[service_name] = health_monitor",
            "",
            "                    startup_results[service_name] = {'status': 'started', 'healthy': True}",
            "                    self.logger.info(f\"Successfully started service: {service_name}\")",
            "",
            "                except Exception as e:",
            "                    startup_results[service_name] = {'status': 'failed', 'error': str(e)}",
            "                    self.logger.error(f\"Failed to start service {service_name}: {e}\")"
          ],
          "line_count": 19
        },
        {
          "start_line": 166,
          "end_line": 177,
          "language": "python",
          "content": [
            "        # Configure load balancer with all successfully started services",
            "        await self.load_balancer.configure_services(self.services)",
            "",
            "        return {",
            "            'startup_results': startup_results,",
            "            'services_started': len([r for r in startup_results.values() ",
            "                                   if r['status'] == 'started']),",
            "            'load_balancer_configured': True,",
            "            'health_monitoring_active': True",
            "        }"
          ],
          "line_count": 10
        },
        {
          "start_line": 181,
          "end_line": 195,
          "language": "python",
          "content": [
            "    async def _start_service(self, service_name: str) -> Any:",
            "        \"\"\"Start individual RAG service with proper initialization.\"\"\"",
            "",
            "        service_class = self.service_registry[service_name]",
            "        service_config = self.service_config[service_name]",
            "",
            "        # Create service instance with configuration",
            "        service_instance = service_class(service_config)",
            "",
            "        # Initialize service (async setup, connections, etc.)",
            "        await service_instance.initialize()",
            "",
            "        return service_instance"
          ],
          "line_count": 13
        },
        {
          "start_line": 205,
          "end_line": 215,
          "language": "python",
          "content": [
            "class DocumentProcessingService:",
            "    \"\"\"Scalable document processing microservice with worker pool architecture.\"\"\"",
            "",
            "    def __init__(self, config: Dict[str, Any]):",
            "        self.config = config",
            "        # Create bounded queue to prevent memory overflow",
            "        self.processing_queue = asyncio.Queue(maxsize=config.get('max_queue_size', 1000))",
            "        self.worker_pool_size = config.get('workers', 4)",
            "        self.workers = []"
          ],
          "line_count": 9
        },
        {
          "start_line": 219,
          "end_line": 227,
          "language": "python",
          "content": [
            "        # Track processing statistics for monitoring",
            "        self.stats = {",
            "            'documents_processed': 0,",
            "            'processing_errors': 0,",
            "            'average_processing_time': 0.0,",
            "            'queue_size': 0",
            "        }"
          ],
          "line_count": 7
        },
        {
          "start_line": 233,
          "end_line": 247,
          "language": "python",
          "content": [
            "    async def initialize(self):",
            "        \"\"\"Initialize the document processing service with worker pool.\"\"\"",
            "",
            "        # Start worker processes for parallel document processing",
            "        for i in range(self.worker_pool_size):",
            "            worker = asyncio.create_task(self._document_processing_worker(f\"worker_{i}\"))",
            "            self.workers.append(worker)",
            "",
            "        # Start background queue monitoring for metrics",
            "        asyncio.create_task(self._monitor_processing_queue())",
            "",
            "        self.logger = logging.getLogger(f\"{__name__}.DocumentProcessingService\")",
            "        self.logger.info(f\"Document processing service initialized with {self.worker_pool_size} workers\")"
          ],
          "line_count": 13
        },
        {
          "start_line": 253,
          "end_line": 271,
          "language": "python",
          "content": [
            "    async def process_documents(self, documents: List[Dict[str, Any]]) -> Dict[str, Any]:",
            "        \"\"\"Process multiple documents asynchronously with job tracking.\"\"\"",
            "",
            "        processing_tasks = []",
            "        for doc in documents:",
            "            # Create unique processing ID for tracking",
            "            processing_id = f\"proc_{int(time.time() * 1000)}_{len(processing_tasks)}\"",
            "            processing_task = {",
            "                'id': processing_id,",
            "                'document': doc,",
            "                'timestamp': time.time(),",
            "                'status': 'queued'",
            "            }",
            "",
            "            # Add to processing queue",
            "            await self.processing_queue.put(processing_task)",
            "            processing_tasks.append(processing_id)"
          ],
          "line_count": 17
        },
        {
          "start_line": 275,
          "end_line": 284,
          "language": "python",
          "content": [
            "        # Return job information for client tracking",
            "        return {",
            "            'processing_job_id': f\"batch_{int(time.time())}\",",
            "            'documents_queued': len(documents),",
            "            'processing_task_ids': processing_tasks,",
            "            'estimated_completion_time': self._estimate_completion_time(len(documents)),",
            "            'current_queue_size': self.processing_queue.qsize()",
            "        }"
          ],
          "line_count": 8
        },
        {
          "start_line": 290,
          "end_line": 304,
          "language": "python",
          "content": [
            "    async def _document_processing_worker(self, worker_id: str):",
            "        \"\"\"Background worker for processing documents from the queue.\"\"\"",
            "",
            "        while True:",
            "            try:",
            "                # Get next task from queue (blocks if empty)",
            "                processing_task = await self.processing_queue.get()",
            "                start_time = time.time()",
            "",
            "                # Process the document through the pipeline",
            "                processing_result = await self._process_single_document(",
            "                    processing_task['document']",
            "                )"
          ],
          "line_count": 13
        },
        {
          "start_line": 308,
          "end_line": 321,
          "language": "python",
          "content": [
            "                # Update performance statistics",
            "                processing_time = time.time() - start_time",
            "                self._update_processing_stats(processing_time, success=True)",
            "",
            "                # Mark task as completed",
            "                self.processing_queue.task_done()",
            "                self.logger.debug(f\"Worker {worker_id} processed document in {processing_time:.2f}s\")",
            "",
            "            except Exception as e:",
            "                self.logger.error(f\"Worker {worker_id} processing error: {e}\")",
            "                self._update_processing_stats(0, success=False)",
            "                self.processing_queue.task_done()"
          ],
          "line_count": 12
        },
        {
          "start_line": 327,
          "end_line": 343,
          "language": "python",
          "content": [
            "    async def _process_single_document(self, document: Dict[str, Any]) -> Dict[str, Any]:",
            "        \"\"\"Process individual document through the complete pipeline.\"\"\"",
            "",
            "        try:",
            "            # Step 1: Parse document (extract text from PDF, Word, etc.)",
            "            parsed_content = await self._parse_document(document)",
            "",
            "            # Step 2: Clean and normalize content",
            "            extracted_content = await self._extract_and_clean_content(parsed_content)",
            "",
            "            # Step 3: Enrich with metadata (author, creation date, tags)",
            "            enriched_metadata = await self._enrich_metadata(document, extracted_content)",
            "",
            "            # Step 4: Create chunks for vector storage",
            "            chunks = await self._create_document_chunks(extracted_content, enriched_metadata)"
          ],
          "line_count": 15
        },
        {
          "start_line": 347,
          "end_line": 362,
          "language": "python",
          "content": [
            "            return {",
            "                'success': True,",
            "                'processed_content': extracted_content,",
            "                'metadata': enriched_metadata,",
            "                'chunks': chunks,",
            "                'chunk_count': len(chunks)",
            "            }",
            "",
            "        except Exception as e:",
            "            return {",
            "                'success': False,",
            "                'error': str(e),",
            "                'document_id': document.get('id', 'unknown')",
            "            }"
          ],
          "line_count": 14
        },
        {
          "start_line": 368,
          "end_line": 380,
          "language": "python",
          "content": [
            "class EmbeddingService:",
            "    \"\"\"Production embedding service with caching and batching.\"\"\"",
            "",
            "    def __init__(self, config: Dict[str, Any]):",
            "        self.config = config",
            "        self.model_name = config.get('model_name', 'text-embedding-ada-002')",
            "        self.batch_size = config.get('batch_size', 100)",
            "        self.cache_enabled = config.get('cache_enabled', True)",
            "",
            "        # Initialize embedding model",
            "        self.embedding_model = self._initialize_embedding_model()"
          ],
          "line_count": 11
        },
        {
          "start_line": 384,
          "end_line": 392,
          "language": "python",
          "content": [
            "        # Caching system",
            "        if self.cache_enabled:",
            "            self.embedding_cache = EmbeddingCache(config.get('cache_config', {}))",
            "",
            "        # Batching queue",
            "        self.embedding_queue = asyncio.Queue()",
            "        self.batch_processor = asyncio.create_task(self._batch_embedding_processor())"
          ],
          "line_count": 7
        },
        {
          "start_line": 396,
          "end_line": 404,
          "language": "python",
          "content": [
            "    async def embed_texts(self, texts: List[str],",
            "                         cache_key_prefix: str = \"\") -> Dict[str, Any]:",
            "        \"\"\"Embed multiple texts with caching and batching optimization.\"\"\"",
            "",
            "        embedding_results = {}",
            "        cache_hits = 0",
            "        cache_misses = 0"
          ],
          "line_count": 7
        },
        {
          "start_line": 408,
          "end_line": 423,
          "language": "python",
          "content": [
            "        # Check cache for existing embeddings",
            "        texts_to_embed = []",
            "        for i, text in enumerate(texts):",
            "            cache_key = f\"{cache_key_prefix}:{hash(text)}\"",
            "",
            "            if self.cache_enabled:",
            "                cached_embedding = await self.embedding_cache.get(cache_key)",
            "                if cached_embedding is not None:",
            "                    embedding_results[i] = cached_embedding",
            "                    cache_hits += 1",
            "                    continue",
            "",
            "            texts_to_embed.append((i, text, cache_key))",
            "            cache_misses += 1"
          ],
          "line_count": 14
        },
        {
          "start_line": 427,
          "end_line": 440,
          "language": "python",
          "content": [
            "        # Generate embeddings for uncached texts",
            "        if texts_to_embed:",
            "            new_embeddings = await self._generate_embeddings_batch([",
            "                text for _, text, _ in texts_to_embed",
            "            ])",
            "",
            "            # Store results and cache new embeddings",
            "            for (original_idx, text, cache_key), embedding in zip(texts_to_embed, new_embeddings):",
            "                embedding_results[original_idx] = embedding",
            "",
            "                if self.cache_enabled:",
            "                    await self.embedding_cache.set(cache_key, embedding)"
          ],
          "line_count": 12
        },
        {
          "start_line": 444,
          "end_line": 458,
          "language": "python",
          "content": [
            "        # Return embeddings in original order",
            "        ordered_embeddings = [embedding_results[i] for i in range(len(texts))]",
            "",
            "        return {",
            "            'embeddings': ordered_embeddings,",
            "            'cache_stats': {",
            "                'cache_hits': cache_hits,",
            "                'cache_misses': cache_misses,",
            "                'cache_hit_rate': cache_hits / len(texts) if texts else 0",
            "            },",
            "            'model_used': self.model_name,",
            "            'batch_size_used': min(self.batch_size, len(texts_to_embed))",
            "        }"
          ],
          "line_count": 13
        },
        {
          "start_line": 462,
          "end_line": 475,
          "language": "python",
          "content": [
            "    async def _generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:",
            "        \"\"\"Generate embeddings in optimized batches.\"\"\"",
            "",
            "        all_embeddings = []",
            "",
            "        # Process in batches",
            "        for i in range(0, len(texts), self.batch_size):",
            "            batch = texts[i:i + self.batch_size]",
            "",
            "            try:",
            "                batch_embeddings = self.embedding_model.embed_documents(batch)",
            "                all_embeddings.extend(batch_embeddings)"
          ],
          "line_count": 12
        },
        {
          "start_line": 479,
          "end_line": 493,
          "language": "python",
          "content": [
            "            except Exception as e:",
            "                self.logger.error(f\"Batch embedding error: {e}\")",
            "                # Fallback to individual processing",
            "                for text in batch:",
            "                    try:",
            "                        individual_embedding = self.embedding_model.embed_query(text)",
            "                        all_embeddings.append(individual_embedding)",
            "                    except Exception as individual_error:",
            "                        self.logger.error(f\"Individual embedding error: {individual_error}\")",
            "                        # Use zero vector as fallback",
            "                        all_embeddings.append([0.0] * 1536)  # Adjust dimension as needed",
            "",
            "        return all_embeddings"
          ],
          "line_count": 13
        },
        {
          "start_line": 497,
          "end_line": 510,
          "language": "python",
          "content": [
            "class EmbeddingCache:",
            "    \"\"\"High-performance embedding cache with TTL and LRU eviction.\"\"\"",
            "",
            "    def __init__(self, config: Dict[str, Any]):",
            "        self.config = config",
            "        self.max_size = config.get('max_size', 10000)",
            "        self.ttl_seconds = config.get('ttl_seconds', 86400)  # 24 hours",
            "",
            "        # In-memory cache with metadata",
            "        self.cache = {}",
            "        self.access_times = {}",
            "        self.creation_times = {}"
          ],
          "line_count": 12
        },
        {
          "start_line": 514,
          "end_line": 530,
          "language": "python",
          "content": [
            "    async def get(self, key: str) -> Optional[List[float]]:",
            "        \"\"\"Get embedding from cache.\"\"\"",
            "",
            "        if key not in self.cache:",
            "            return None",
            "",
            "        # Check TTL",
            "        if time.time() - self.creation_times[key] > self.ttl_seconds:",
            "            await self._evict_key(key)",
            "            return None",
            "",
            "        # Update access time for LRU",
            "        self.access_times[key] = time.time()",
            "",
            "        return self.cache[key]"
          ],
          "line_count": 15
        },
        {
          "start_line": 534,
          "end_line": 547,
          "language": "python",
          "content": [
            "    async def set(self, key: str, embedding: List[float]):",
            "        \"\"\"Set embedding in cache with eviction if needed.\"\"\"",
            "",
            "        # Evict if at capacity",
            "        if len(self.cache) >= self.max_size and key not in self.cache:",
            "            await self._evict_lru()",
            "",
            "        # Store embedding",
            "        current_time = time.time()",
            "        self.cache[key] = embedding",
            "        self.access_times[key] = current_time",
            "        self.creation_times[key] = current_time"
          ],
          "line_count": 12
        },
        {
          "start_line": 559,
          "end_line": 573,
          "language": "python",
          "content": [
            "",
            "# Production load balancing and auto-scaling",
            "",
            "class RAGLoadBalancer:",
            "    \"\"\"Intelligent load balancer for RAG services.\"\"\"",
            "",
            "    def __init__(self, config: Dict[str, Any] = None):",
            "        self.config = config or {}",
            "        ",
            "        # Initialize service tracking",
            "        self.service_instances = {}  # Track available service instances",
            "        self.health_status = {}     # Monitor instance health",
            "        self.load_metrics = {}      # Track performance metrics"
          ],
          "line_count": 13
        },
        {
          "start_line": 577,
          "end_line": 587,
          "language": "python",
          "content": [
            "        # Configure available load balancing strategies",
            "        self.strategies = {",
            "            'round_robin': self._round_robin_selection,",
            "            'least_connections': self._least_connections_selection,",
            "            'response_time': self._response_time_selection,",
            "            'resource_usage': self._resource_usage_selection",
            "        }",
            "",
            "        self.current_strategy = self.config.get('strategy', 'response_time')"
          ],
          "line_count": 9
        },
        {
          "start_line": 593,
          "end_line": 604,
          "language": "python",
          "content": [
            "    async def configure_services(self, services: Dict[str, Any]):",
            "        \"\"\"Configure services for load balancing.\"\"\"",
            "",
            "        for service_name, service_instances in services.items():",
            "            # Ensure service_instances is a list",
            "            if not isinstance(service_instances, list):",
            "                service_instances = [service_instances]",
            "",
            "            # Register service instances",
            "            self.service_instances[service_name] = service_instances"
          ],
          "line_count": 10
        },
        {
          "start_line": 608,
          "end_line": 614,
          "language": "python",
          "content": [
            "            # Initialize health status for all instances",
            "            self.health_status[service_name] = {",
            "                instance: ServiceStatus.HEALTHY",
            "                for instance in service_instances",
            "            }"
          ],
          "line_count": 5
        },
        {
          "start_line": 618,
          "end_line": 630,
          "language": "python",
          "content": [
            "            # Initialize performance metrics tracking",
            "            self.load_metrics[service_name] = {",
            "                instance: {",
            "                    'active_connections': 0,",
            "                    'avg_response_time': 0.0,",
            "                    'cpu_usage': 0.0,",
            "                    'memory_usage': 0.0,",
            "                    'error_rate': 0.0",
            "                }",
            "                for instance in service_instances",
            "            }"
          ],
          "line_count": 11
        },
        {
          "start_line": 638,
          "end_line": 644,
          "language": "python",
          "content": [
            "    async def get_service_instance(self, service_name: str) -> Optional[Any]:",
            "        \"\"\"Get optimal service instance based on current strategy.\"\"\"",
            "",
            "        if service_name not in self.service_instances:",
            "            return None"
          ],
          "line_count": 5
        },
        {
          "start_line": 648,
          "end_line": 658,
          "language": "python",
          "content": [
            "        # Filter to only healthy instances",
            "        healthy_instances = [",
            "            instance for instance in self.service_instances[service_name]",
            "            if self.health_status[service_name][instance] == ServiceStatus.HEALTHY",
            "        ]",
            "",
            "        if not healthy_instances:",
            "            self.logger.warning(f\"No healthy instances available for {service_name}\")",
            "            return None"
          ],
          "line_count": 9
        },
        {
          "start_line": 662,
          "end_line": 673,
          "language": "python",
          "content": [
            "        # Apply the configured load balancing strategy",
            "        selected_instance = await self.strategies[self.current_strategy](",
            "            service_name, healthy_instances",
            "        )",
            "",
            "        # Track the connection for load metrics",
            "        if selected_instance:",
            "            self.load_metrics[service_name][selected_instance]['active_connections'] += 1",
            "",
            "        return selected_instance"
          ],
          "line_count": 10
        },
        {
          "start_line": 681,
          "end_line": 688,
          "language": "python",
          "content": [
            "    async def _response_time_selection(self, service_name: str,",
            "                                     healthy_instances: List[Any]) -> Any:",
            "        \"\"\"Select instance with best average response time.\"\"\"",
            "",
            "        best_instance = None",
            "        best_response_time = float('inf')"
          ],
          "line_count": 6
        },
        {
          "start_line": 692,
          "end_line": 700,
          "language": "python",
          "content": [
            "        for instance in healthy_instances:",
            "            metrics = self.load_metrics[service_name][instance]",
            "            avg_response_time = metrics['avg_response_time']",
            "",
            "            # Adjust response time based on current load",
            "            # Higher active connections increase the adjusted time",
            "            adjusted_time = avg_response_time * (1 + metrics['active_connections'] * 0.1)"
          ],
          "line_count": 7
        },
        {
          "start_line": 704,
          "end_line": 710,
          "language": "python",
          "content": [
            "            if adjusted_time < best_response_time:",
            "                best_response_time = adjusted_time",
            "                best_instance = instance",
            "",
            "        return best_instance"
          ],
          "line_count": 5
        },
        {
          "start_line": 718,
          "end_line": 726,
          "language": "python",
          "content": [
            "class RAGAutoScaler:",
            "    \"\"\"Auto-scaling system for RAG services based on load and performance metrics.\"\"\"",
            "",
            "    def __init__(self, config: Dict[str, Any]):",
            "        self.config = config",
            "        self.scaling_policies = {}  # Per-service scaling configurations",
            "        self.monitoring_interval = config.get('monitoring_interval', 30)  # seconds"
          ],
          "line_count": 7
        },
        {
          "start_line": 730,
          "end_line": 739,
          "language": "python",
          "content": [
            "        # Define scale-up thresholds",
            "        self.scale_up_thresholds = config.get('scale_up', {",
            "            'cpu_threshold': 70.0,          # CPU usage percentage",
            "            'memory_threshold': 80.0,       # Memory usage percentage",
            "            'response_time_threshold': 2.0, # Response time in seconds",
            "            'queue_size_threshold': 100,    # Queue backlog size",
            "            'error_rate_threshold': 5.0     # Error rate percentage",
            "        })"
          ],
          "line_count": 8
        },
        {
          "start_line": 743,
          "end_line": 755,
          "language": "python",
          "content": [
            "        # Define scale-down thresholds (more conservative)",
            "        self.scale_down_thresholds = config.get('scale_down', {",
            "            'cpu_threshold': 30.0,",
            "            'memory_threshold': 40.0,",
            "            'response_time_threshold': 0.5,",
            "            'queue_size_threshold': 10,",
            "            'stable_duration': 300  # Require 5 minutes of stability",
            "        })",
            "",
            "        # Start continuous monitoring",
            "        self.monitoring_task = asyncio.create_task(self._continuous_monitoring())"
          ],
          "line_count": 11
        },
        {
          "start_line": 763,
          "end_line": 772,
          "language": "python",
          "content": [
            "    async def register_service_for_scaling(self, service_name: str,",
            "                                         scaling_config: Dict[str, Any]):",
            "        \"\"\"Register service for auto-scaling with specific configuration.\"\"\"",
            "",
            "        self.scaling_policies[service_name] = {",
            "            'min_instances': scaling_config.get('min_instances', 1),",
            "            'max_instances': scaling_config.get('max_instances', 10),",
            "            'current_instances': scaling_config.get('current_instances', 1),"
          ],
          "line_count": 8
        },
        {
          "start_line": 776,
          "end_line": 782,
          "language": "python",
          "content": [
            "            'scaling_cooldown': scaling_config.get('cooldown', 300),  # Prevent rapid scaling",
            "            'last_scaling_action': 0,                                 # Track last action time",
            "            'stability_window': [],                                   # Track stability for scale-down",
            "            'custom_thresholds': scaling_config.get('thresholds', {}) # Service-specific thresholds",
            "        }"
          ],
          "line_count": 5
        },
        {
          "start_line": 790,
          "end_line": 800,
          "language": "python",
          "content": [
            "    async def _continuous_monitoring(self):",
            "        \"\"\"Continuously monitor services and trigger scaling decisions.\"\"\"",
            "",
            "        while True:",
            "            try:",
            "                # Check each registered service",
            "                for service_name in self.scaling_policies.keys():",
            "                    # Collect current performance metrics",
            "                    current_metrics = await self._collect_service_metrics(service_name)"
          ],
          "line_count": 9
        },
        {
          "start_line": 804,
          "end_line": 813,
          "language": "python",
          "content": [
            "                    # Evaluate if scaling action is needed",
            "                    scaling_decision = await self._evaluate_scaling_decision(",
            "                        service_name, current_metrics",
            "                    )",
            "",
            "                    # Execute scaling action if required",
            "                    if scaling_decision['action'] != 'none':",
            "                        await self._execute_scaling_action(service_name, scaling_decision)"
          ],
          "line_count": 8
        },
        {
          "start_line": 817,
          "end_line": 824,
          "language": "python",
          "content": [
            "                # Wait before next monitoring cycle",
            "                await asyncio.sleep(self.monitoring_interval)",
            "",
            "            except Exception as e:",
            "                self.logger.error(f\"Auto-scaling monitoring error: {e}\")",
            "                await asyncio.sleep(self.monitoring_interval)"
          ],
          "line_count": 6
        },
        {
          "start_line": 832,
          "end_line": 864,
          "language": "python",
          "content": [
            "    async def _evaluate_scaling_decision(self, service_name: str,",
            "                                       metrics: Dict[str, Any]) -> Dict[str, Any]:",
            "        \"\"\"Evaluate whether scaling action is needed.\"\"\"",
            "",
            "        policy = self.scaling_policies[service_name]",
            "        current_time = time.time()",
            "",
            "        # Respect cooldown period to prevent rapid scaling",
            "        if current_time - policy['last_scaling_action'] < policy['scaling_cooldown']:",
            "            return {'action': 'none', 'reason': 'cooldown_active'}",
            "",
            "        # Check if any scale-up condition is met",
            "        scale_up_triggered = (",
            "            metrics['cpu_usage'] > self.scale_up_thresholds['cpu_threshold'] or",
            "            metrics['memory_usage'] > self.scale_up_thresholds['memory_threshold'] or",
            "            metrics['avg_response_time'] > self.scale_up_thresholds['response_time_threshold'] or",
            "            metrics['queue_size'] > self.scale_up_thresholds['queue_size_threshold'] or",
            "            metrics['error_rate'] > self.scale_up_thresholds['error_rate_threshold']",
            "        )",
            "",
            "        # Scale up if conditions are met and within limits",
            "        if scale_up_triggered and policy['current_instances'] < policy['max_instances']:",
            "            return {",
            "                'action': 'scale_up',",
            "                'target_instances': min(",
            "                    policy['current_instances'] + 1,",
            "                    policy['max_instances']",
            "                ),",
            "                'reason': 'high_load_detected',",
            "                'metrics': metrics",
            "            }"
          ],
          "line_count": 31
        },
        {
          "start_line": 870,
          "end_line": 878,
          "language": "python",
          "content": [
            "        # Check scale-down conditions (all must be met)",
            "        scale_down_conditions = (",
            "            metrics['cpu_usage'] < self.scale_down_thresholds['cpu_threshold'] and",
            "            metrics['memory_usage'] < self.scale_down_thresholds['memory_threshold'] and",
            "            metrics['avg_response_time'] < self.scale_down_thresholds['response_time_threshold'] and",
            "            metrics['queue_size'] < self.scale_down_thresholds['queue_size_threshold']",
            "        )"
          ],
          "line_count": 7
        },
        {
          "start_line": 882,
          "end_line": 895,
          "language": "python",
          "content": [
            "        # Track stability over time",
            "        policy['stability_window'].append({",
            "            'timestamp': current_time,",
            "            'stable': scale_down_conditions",
            "        })",
            "",
            "        # Keep only measurements within the stability window",
            "        stable_duration = self.scale_down_thresholds['stable_duration']",
            "        policy['stability_window'] = [",
            "            measurement for measurement in policy['stability_window']",
            "            if current_time - measurement['timestamp'] <= stable_duration",
            "        ]"
          ],
          "line_count": 12
        },
        {
          "start_line": 899,
          "end_line": 917,
          "language": "python",
          "content": [
            "        # Scale down only if consistently stable",
            "        if (len(policy['stability_window']) > 0 and",
            "            all(m['stable'] for m in policy['stability_window']) and",
            "            policy['current_instances'] > policy['min_instances'] and",
            "            current_time - policy['stability_window'][0]['timestamp'] >= stable_duration):",
            "",
            "            return {",
            "                'action': 'scale_down',",
            "                'target_instances': max(",
            "                    policy['current_instances'] - 1,",
            "                    policy['min_instances']",
            "                ),",
            "                'reason': 'sustained_low_usage',",
            "                'stability_duration': current_time - policy['stability_window'][0]['timestamp']",
            "            }",
            "",
            "        return {'action': 'none', 'reason': 'no_scaling_needed'}"
          ],
          "line_count": 17
        },
        {
          "start_line": 920,
          "end_line": 930,
          "language": "",
          "content": [
            "",
            "---",
            "",
            "## Part 2: Enterprise Integration and Security",
            "",
            "### Enterprise Integration Framework",
            "",
            "The enterprise integration framework connects advanced RAG capabilities to business systems:",
            ""
          ],
          "line_count": 9
        },
        {
          "start_line": 939,
          "end_line": 943,
          "language": "",
          "content": [
            "",
            "The integrator supports multiple enterprise data source types with dedicated connectors:",
            ""
          ],
          "line_count": 3
        },
        {
          "start_line": 953,
          "end_line": 957,
          "language": "",
          "content": [
            "",
            "Critical enterprise components include authentication, data transformation, and change detection:",
            ""
          ],
          "line_count": 3
        },
        {
          "start_line": 966,
          "end_line": 970,
          "language": "",
          "content": [
            "",
            "The integration setup process connects to each data source and establishes monitoring:",
            ""
          ],
          "line_count": 3
        },
        {
          "start_line": 985,
          "end_line": 989,
          "language": "",
          "content": [
            "",
            "For each successful connection, we establish change monitoring and track results:",
            ""
          ],
          "line_count": 3
        },
        {
          "start_line": 1010,
          "end_line": 1014,
          "language": "",
          "content": [
            "",
            "Finally, we return comprehensive integration status and statistics:",
            ""
          ],
          "line_count": 3
        },
        {
          "start_line": 1022,
          "end_line": 1026,
          "language": "",
          "content": [
            "",
            "The SharePoint connector provides secure access to enterprise document repositories:",
            ""
          ],
          "line_count": 3
        },
        {
          "start_line": 1039,
          "end_line": 1041,
          "language": "",
          "content": [
            ""
          ],
          "line_count": 1
        },
        {
          "start_line": 1052,
          "end_line": 1056,
          "language": "",
          "content": [
            "",
            "SharePoint connection initialization uses OAuth 2.0 client credentials flow for secure enterprise authentication. The ClientContext provides the foundation for all SharePoint operations, while ClientCredential handles the OAuth token exchange automatically. This approach is preferred for production systems as it avoids storing user passwords and provides proper enterprise security.",
            ""
          ],
          "line_count": 3
        },
        {
          "start_line": 1072,
          "end_line": 1124,
          "language": "",
          "content": [
            "",
            "Connection testing immediately validates authentication and network connectivity by retrieving basic site information. The returned metadata helps with debugging connection issues and provides confirmation of successful enterprise system integration. Error handling ensures graceful failure reporting for operations teams.",
            "",
            "    async def retrieve_documents(self, folder_path: str = None,",
            "                               modified_since: datetime = None) -> List[Dict[str, Any]]:",
            "        \"\"\"Retrieve documents from SharePoint with optional filtering.\"\"\"",
            "",
            "        if not self.sp_client:",
            "            raise RuntimeError(\"SharePoint client not initialized\")",
            "",
            "        documents = []",
            "",
            "        try:",
            "            # Get document library",
            "            if folder_path:",
            "                folder = self.sp_client.web.get_folder_by_server_relative_url(folder_path)",
            "            else:",
            "                folder = self.sp_client.web.default_document_library().root_folder",
            "",
            "            # Get files",
            "            files = folder.files.get().execute_query()",
            "",
            "            for file in files:",
            "                # Filter by modification date if specified",
            "                if modified_since and file.time_last_modified < modified_since:",
            "                    continue",
            "",
            "                # Download file content",
            "                file_content = file.get_content().execute_query()",
            "",
            "                documents.append({",
            "                    'id': file.unique_id,",
            "                    'name': file.name,",
            "                    'url': file.server_relative_url,",
            "                    'content': file_content.value,",
            "                    'modified': file.time_last_modified,",
            "                    'size': file.length,",
            "                    'content_type': file.content_type,",
            "                    'metadata': {",
            "                        'author': file.author.title if file.author else 'Unknown',",
            "                        'created': file.time_created,",
            "                        'version': file.ui_version_label",
            "                    }",
            "                })",
            "",
            "            return documents",
            "",
            "        except Exception as e:",
            "            self.logger.error(f\"SharePoint document retrieval error: {e}\")",
            "            return []",
            ""
          ],
          "line_count": 51
        },
        {
          "start_line": 1128,
          "end_line": 1135,
          "language": "python",
          "content": [
            "class EnterpriseAuthManager:",
            "    \"\"\"Enterprise authentication and authorization manager.\"\"\"",
            "",
            "    def __init__(self, auth_config: Dict[str, Any]):",
            "        self.config = auth_config",
            "        self.auth_providers = {}"
          ],
          "line_count": 6
        },
        {
          "start_line": 1139,
          "end_line": 1147,
          "language": "python",
          "content": [
            "        # Initialize authentication providers",
            "        if 'active_directory' in auth_config:",
            "            self.auth_providers['ad'] = ActiveDirectoryAuth(auth_config['active_directory'])",
            "        if 'oauth2' in auth_config:",
            "            self.auth_providers['oauth2'] = OAuth2Auth(auth_config['oauth2'])",
            "        if 'saml' in auth_config:",
            "            self.auth_providers['saml'] = SAMLAuth(auth_config['saml'])"
          ],
          "line_count": 7
        },
        {
          "start_line": 1151,
          "end_line": 1154,
          "language": "python",
          "content": [
            "        # Role-based access control",
            "        self.rbac_manager = RBACManager(auth_config.get('rbac', {}))"
          ],
          "line_count": 2
        },
        {
          "start_line": 1168,
          "end_line": 1172,
          "language": "",
          "content": [
            "",
            "User authentication begins by selecting the appropriate authentication method based on client credentials. OAuth2 serves as the default for modern applications, while other methods accommodate legacy systems. Provider validation ensures only configured authentication methods are accepted, preventing security vulnerabilities.",
            ""
          ],
          "line_count": 3
        },
        {
          "start_line": 1183,
          "end_line": 1187,
          "language": "",
          "content": [
            "",
            "Successful authentication triggers immediate permission retrieval, creating a complete user context for authorization decisions. This approach minimizes subsequent permission lookups and provides comprehensive access control information for RAG system components.",
            ""
          ],
          "line_count": 3
        },
        {
          "start_line": 1199,
          "end_line": 1215,
          "language": "",
          "content": [
            "",
            "Session token creation enables stateless authentication across RAG services. Tokens typically include user identity, permissions, and expiration information, supporting distributed service architectures. Error handling provides secure failure reporting without exposing sensitive authentication details.",
            "",
            "    async def authorize_request(self, session_token: str,",
            "                              resource: str, action: str) -> Dict[str, Any]:",
            "        \"\"\"Authorize user request for specific resource and action.\"\"\"",
            "",
            "        try:",
            "            # Validate session token",
            "            user_info = self._validate_session_token(session_token)",
            "            if not user_info:",
            "                return {",
            "                    'authorized': False,",
            "                    'error': 'Invalid or expired session token'",
            "                }"
          ],
          "line_count": 15
        },
        {
          "start_line": 1219,
          "end_line": 1230,
          "language": "python",
          "content": [
            "            # Check permissions",
            "            authorized = await self.rbac_manager.check_permission(",
            "                user_info, resource, action",
            "            )",
            "",
            "            return {",
            "                'authorized': authorized,",
            "                'user_id': user_info['user_id'],",
            "                'permissions_checked': f'{resource}:{action}'",
            "            }"
          ],
          "line_count": 10
        },
        {
          "start_line": 1234,
          "end_line": 1240,
          "language": "python",
          "content": [
            "        except Exception as e:",
            "            return {",
            "                'authorized': False,",
            "                'error': f'Authorization failed: {str(e)}'",
            "            }"
          ],
          "line_count": 5
        },
        {
          "start_line": 1244,
          "end_line": 1258,
          "language": "python",
          "content": [
            "class RBACManager:",
            "    \"\"\"Role-Based Access Control manager for RAG systems.\"\"\"",
            "",
            "    def __init__(self, rbac_config: Dict[str, Any]):",
            "        self.config = rbac_config",
            "",
            "        # Define roles and permissions",
            "        self.roles = rbac_config.get('roles', {",
            "            'admin': ['*'],  # Full access",
            "            'power_user': ['rag:query', 'rag:upload', 'rag:view_sources'],",
            "            'user': ['rag:query'],",
            "            'readonly': ['rag:query:readonly']",
            "        })"
          ],
          "line_count": 13
        },
        {
          "start_line": 1262,
          "end_line": 1269,
          "language": "python",
          "content": [
            "        # Resource-based permissions",
            "        self.resources = rbac_config.get('resources', {",
            "            'documents': ['read', 'write', 'delete'],",
            "            'queries': ['execute', 'view_history'],",
            "            'system': ['configure', 'monitor', 'admin']",
            "        })"
          ],
          "line_count": 6
        },
        {
          "start_line": 1285,
          "end_line": 1298,
          "language": "",
          "content": [
            "",
            "Permission aggregation combines all permissions from a user's assigned roles, using set operations to avoid duplicates. This approach supports users with multiple roles while maintaining efficient permission lookups. The system returns a deduplicated list of all permissions available to the user across their various roles.",
            "",
            "    async def check_permission(self, user_info: Dict[str, Any],",
            "                             resource: str, action: str) -> bool:",
            "        \"\"\"Check if user has permission for specific resource and action.\"\"\"",
            "",
            "        user_permissions = await self.get_user_permissions(user_info)",
            "",
            "        # Check for wildcard permission",
            "        if '*' in user_permissions:",
            "            return True"
          ],
          "line_count": 12
        },
        {
          "start_line": 1302,
          "end_line": 1314,
          "language": "python",
          "content": [
            "        # Check specific permission",
            "        required_permission = f\"{resource}:{action}\"",
            "        if required_permission in user_permissions:",
            "            return True",
            "",
            "        # Check resource-level permission",
            "        resource_permission = f\"{resource}:*\"",
            "        if resource_permission in user_permissions:",
            "            return True",
            "",
            "        return False"
          ],
          "line_count": 11
        },
        {
          "start_line": 1317,
          "end_line": 1327,
          "language": "",
          "content": [
            "",
            "### **Data Privacy and Compliance**",
            "",
            "Implement comprehensive privacy and compliance frameworks to ensure your RAG system meets regulatory requirements across different jurisdictions and industries.",
            "",
            "#### Step 1: Initialize Privacy Compliance Manager",
            "",
            "Set up the comprehensive compliance framework:",
            ""
          ],
          "line_count": 9
        },
        {
          "start_line": 1352,
          "end_line": 1358,
          "language": "",
          "content": [
            "",
            "#### Step 2: Data Processing with Compliance Checks",
            "",
            "Process data through comprehensive compliance validation:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1378,
          "end_line": 1384,
          "language": "",
          "content": [
            "",
            "#### Step 3: Apply Compliance Framework Checks",
            "",
            "Run data through each required compliance framework:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1417,
          "end_line": 1423,
          "language": "",
          "content": [
            "",
            "#### Step 4: GDPR Compliance Handler",
            "",
            "Implement specific GDPR requirements for European data protection:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1431,
          "end_line": 1437,
          "language": "",
          "content": [
            "",
            "#### Step 5: GDPR Data Processing Checks",
            "",
            "Implement comprehensive GDPR compliance validation:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1485,
          "end_line": 1491,
          "language": "",
          "content": [
            "",
            "#### Step 6: GDPR Data Subject Rights",
            "",
            "Handle individual rights requests under GDPR:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1507,
          "end_line": 1521,
          "language": "",
          "content": [
            "",
            "This privacy and compliance framework ensures your RAG system handles sensitive data according to regulatory requirements, protecting both your organization and your users' privacy rights.",
            "",
            "---",
            "",
            "## Part 3: Real-Time Indexing and Incremental Updates",
            "",
            "### Change Detection and Incremental Processing",
            "",
            "#### Step 1: Initialize Incremental Indexing System",
            "",
            "First, let's set up the core infrastructure for real-time indexing:",
            ""
          ],
          "line_count": 13
        },
        {
          "start_line": 1546,
          "end_line": 1552,
          "language": "",
          "content": [
            "",
            "#### Step 2: Configure Processing Queues and Background Processors",
            "",
            "Set up asynchronous queues and background processors for handling updates:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1562,
          "end_line": 1568,
          "language": "",
          "content": [
            "",
            "#### Step 3: Set Up Change Detection for Data Sources",
            "",
            "Configure monitoring for multiple data sources:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1609,
          "end_line": 1615,
          "language": "",
          "content": [
            "",
            "#### Step 4: Handle Change Events",
            "",
            "Route different types of changes to appropriate processing queues:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1626,
          "end_line": 1632,
          "language": "",
          "content": [
            "",
            "#### Step 5: Background Update Processor",
            "",
            "Implement the background processor that handles queued updates:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1656,
          "end_line": 1662,
          "language": "",
          "content": [
            "",
            "#### Step 6: Process Individual Updates",
            "",
            "Handle the processing of individual document changes:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1715,
          "end_line": 1721,
          "language": "",
          "content": [
            "",
            "#### Step 7: File System Change Detection",
            "",
            "Implement file system monitoring using OS-level change detection:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1728,
          "end_line": 1734,
          "language": "",
          "content": [
            "",
            "#### Step 8: Set Up File System Monitoring",
            "",
            "Configure file system watchers for specified directories:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1784,
          "end_line": 1790,
          "language": "",
          "content": [
            "",
            "#### Step 9: Handle File System Changes",
            "",
            "Process detected file system changes and create change events:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1823,
          "end_line": 1837,
          "language": "",
          "content": [
            "",
            "This incremental indexing system enables real-time updates to your RAG knowledge base, ensuring your system stays current with changing data sources.",
            "",
            "---",
            "",
            "## Part 4: Monitoring, Observability, and Analytics",
            "",
            "### Comprehensive Monitoring and Alerting",
            "",
            "#### Step 1: Initialize Monitoring System",
            "",
            "First, let's set up the basic monitoring infrastructure with Prometheus metrics and structured logging:",
            ""
          ],
          "line_count": 13
        },
        {
          "start_line": 1860,
          "end_line": 1866,
          "language": "",
          "content": [
            "",
            "#### Step 2: Set Up Prometheus Metrics",
            "",
            "Now let's configure the metrics that will track our RAG system's performance:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1882,
          "end_line": 1888,
          "language": "",
          "content": [
            "",
            "#### Step 3: Configure System and Quality Metrics",
            "",
            "Add metrics for system health and response quality monitoring:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1925,
          "end_line": 1931,
          "language": "",
          "content": [
            "",
            "#### Step 4: Request Tracking and Monitoring",
            "",
            "Implement comprehensive request tracking with success/error monitoring:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 1967,
          "end_line": 1973,
          "language": "",
          "content": [
            "",
            "#### Step 5: Error Handling and Alerting",
            "",
            "Handle request failures with comprehensive error tracking:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 2000,
          "end_line": 2006,
          "language": "",
          "content": [
            "",
            "#### Step 6: Analytics System for Performance Analysis",
            "",
            "Create an analytics system to analyze system performance patterns:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 2018,
          "end_line": 2024,
          "language": "",
          "content": [
            "",
            "#### Step 7: Performance Analysis Engine",
            "",
            "Implement comprehensive system performance analysis:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 2056,
          "end_line": 2062,
          "language": "",
          "content": [
            "",
            "#### Step 8: Request Volume Analysis",
            "",
            "Analyze request patterns and volume trends:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 2098,
          "end_line": 2104,
          "language": "",
          "content": [
            "",
            "#### Step 9: Performance Recommendations Generator",
            "",
            "Generate actionable recommendations based on performance analysis:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 2130,
          "end_line": 2136,
          "language": "",
          "content": [
            "",
            "#### Step 10: Resource and Error Rate Recommendations",
            "",
            "Add recommendations for resource usage and error handling:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 2158,
          "end_line": 2164,
          "language": "",
          "content": [
            "",
            "#### Step 11: Health Checking System",
            "",
            "Implement comprehensive health monitoring for all system components:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 2179,
          "end_line": 2185,
          "language": "",
          "content": [
            "",
            "#### Step 12: Comprehensive Health Check Execution",
            "",
            "Run health checks across all system components:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 2223,
          "end_line": 2229,
          "language": "",
          "content": [
            "",
            "#### Step 13: Vector Store Health Check Implementation",
            "",
            "Implement specific health checks for critical components:",
            ""
          ],
          "line_count": 5
        },
        {
          "start_line": 2260,
          "end_line": 2280,
          "language": "",
          "content": [
            "",
            "This comprehensive monitoring system provides complete observability for your production RAG deployment. The modular design allows you to track performance, identify issues, and maintain high service quality across all components.",
            "",
            "---",
            "",
            "## Hands-On Exercise: Deploy Production RAG System",
            "",
            "Build and deploy a complete production-ready RAG system with enterprise integration, security, monitoring, and auto-scaling.",
            "",
            "**Requirements:**",
            "",
            "1. Containerized architecture with Kubernetes orchestration",
            "2. Enterprise integration with at least one data source",
            "3. Security implementation with authentication and authorization",
            "4. Real-time updates with incremental indexing",
            "5. Comprehensive monitoring with metrics and alerting",
            "",
            "**Production Deployment Architecture:**",
            ""
          ],
          "line_count": 19
        }
      ],
      "large_blocks": [
        {
          "start_line": 832,
          "end_line": 864,
          "language": "python",
          "content": [
            "    async def _evaluate_scaling_decision(self, service_name: str,",
            "                                       metrics: Dict[str, Any]) -> Dict[str, Any]:",
            "        \"\"\"Evaluate whether scaling action is needed.\"\"\"",
            "",
            "        policy = self.scaling_policies[service_name]",
            "        current_time = time.time()",
            "",
            "        # Respect cooldown period to prevent rapid scaling",
            "        if current_time - policy['last_scaling_action'] < policy['scaling_cooldown']:",
            "            return {'action': 'none', 'reason': 'cooldown_active'}",
            "",
            "        # Check if any scale-up condition is met",
            "        scale_up_triggered = (",
            "            metrics['cpu_usage'] > self.scale_up_thresholds['cpu_threshold'] or",
            "            metrics['memory_usage'] > self.scale_up_thresholds['memory_threshold'] or",
            "            metrics['avg_response_time'] > self.scale_up_thresholds['response_time_threshold'] or",
            "            metrics['queue_size'] > self.scale_up_thresholds['queue_size_threshold'] or",
            "            metrics['error_rate'] > self.scale_up_thresholds['error_rate_threshold']",
            "        )",
            "",
            "        # Scale up if conditions are met and within limits",
            "        if scale_up_triggered and policy['current_instances'] < policy['max_instances']:",
            "            return {",
            "                'action': 'scale_up',",
            "                'target_instances': min(",
            "                    policy['current_instances'] + 1,",
            "                    policy['max_instances']",
            "                ),",
            "                'reason': 'high_load_detected',",
            "                'metrics': metrics",
            "            }"
          ],
          "line_count": 31
        },
        {
          "start_line": 1072,
          "end_line": 1124,
          "language": "",
          "content": [
            "",
            "Connection testing immediately validates authentication and network connectivity by retrieving basic site information. The returned metadata helps with debugging connection issues and provides confirmation of successful enterprise system integration. Error handling ensures graceful failure reporting for operations teams.",
            "",
            "    async def retrieve_documents(self, folder_path: str = None,",
            "                               modified_since: datetime = None) -> List[Dict[str, Any]]:",
            "        \"\"\"Retrieve documents from SharePoint with optional filtering.\"\"\"",
            "",
            "        if not self.sp_client:",
            "            raise RuntimeError(\"SharePoint client not initialized\")",
            "",
            "        documents = []",
            "",
            "        try:",
            "            # Get document library",
            "            if folder_path:",
            "                folder = self.sp_client.web.get_folder_by_server_relative_url(folder_path)",
            "            else:",
            "                folder = self.sp_client.web.default_document_library().root_folder",
            "",
            "            # Get files",
            "            files = folder.files.get().execute_query()",
            "",
            "            for file in files:",
            "                # Filter by modification date if specified",
            "                if modified_since and file.time_last_modified < modified_since:",
            "                    continue",
            "",
            "                # Download file content",
            "                file_content = file.get_content().execute_query()",
            "",
            "                documents.append({",
            "                    'id': file.unique_id,",
            "                    'name': file.name,",
            "                    'url': file.server_relative_url,",
            "                    'content': file_content.value,",
            "                    'modified': file.time_last_modified,",
            "                    'size': file.length,",
            "                    'content_type': file.content_type,",
            "                    'metadata': {",
            "                        'author': file.author.title if file.author else 'Unknown',",
            "                        'created': file.time_created,",
            "                        'version': file.ui_version_label",
            "                    }",
            "                })",
            "",
            "            return documents",
            "",
            "        except Exception as e:",
            "            self.logger.error(f\"SharePoint document retrieval error: {e}\")",
            "            return []",
            ""
          ],
          "line_count": 51
        }
      ],
      "needs_refactoring": true
    }
  ]
}