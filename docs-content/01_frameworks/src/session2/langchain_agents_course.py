#!/usr/bin/env python3
"""
Session 2 - LangChain Agents with Tools and Memory (Course Version)
==================================================================

This implementation demonstrates advanced LangChain agent patterns without requiring LangChain installation.
Shows the ReAct pattern (Reasoning + Acting) and agent-tool coordination for data engineering.

Key Patterns:
- ReAct agent reasoning cycles (Think-Act-Think-Act)
- Tool creation and integration
- Conversational memory with different types
- Agent orchestration for data processing workflows
"""

import json
import time
import os
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
from enum import Enum


class AgentStatus(Enum):
    """Agent execution status"""
    THINKING = "thinking"
    ACTING = "acting"
    COMPLETE = "complete"
    ERROR = "error"


@dataclass
class AgentStep:
    """Represents a single step in agent reasoning/action cycle"""
    step_type: str  # "thought" or "action"
    content: str
    timestamp: datetime
    tool_name: Optional[str] = None
    tool_result: Optional[str] = None


class MockLLMResponse:
    """Mock LLM response for educational purposes"""
    def __init__(self, content: str):
        self.content = content


class DataProcessingAgent:
    """
    Advanced agent implementing ReAct pattern for data processing workflows.
    Demonstrates how agents reason about data problems and take coordinated actions.
    """
    
    def __init__(self, model_name="gpt-4", max_iterations=5):
        self.model_name = model_name
        self.max_iterations = max_iterations
        self.execution_history = []
        self.current_context = {}
        self.available_tools = {}
        self.memory = ConversationMemory()
        
        # Initialize data processing tools
        self._setup_data_tools()
        
    def _setup_data_tools(self):
        """Setup data processing tools for agent use"""
        
        def analyze_data_quality(dataset_info: str) -> str:
            """Analyze data quality for a given dataset"""
            # Simulate data quality analysis
            quality_metrics = {
                "completeness": 94.7,
                "accuracy": 98.2,
                "consistency": 96.1,
                "timeliness": 99.1,
                "validity": 97.8
            }
            
            issues_found = []
            if quality_metrics["completeness"] < 95:
                issues_found.append("Missing data in critical columns")
            if quality_metrics["accuracy"] < 98:
                issues_found.append("Data accuracy concerns in financial fields")
            if quality_metrics["consistency"] < 95:
                issues_found.append("Inconsistent date formats across sources")
                
            result = f"Data Quality Analysis for {dataset_info}:\\n"
            result += f"  Completeness: {quality_metrics['completeness']}%\\n"
            result += f"  Accuracy: {quality_metrics['accuracy']}%\\n"
            result += f"  Consistency: {quality_metrics['consistency']}%\\n"
            result += f"  Timeliness: {quality_metrics['timeliness']}%\\n"
            result += f"  Validity: {quality_metrics['validity']}%\\n"
            
            if issues_found:
                result += f"\\nIssues Identified:\\n"
                for issue in issues_found:
                    result += f"  â€¢ {issue}\\n"
            else:
                result += "\\nNo critical issues identified."
                
            return result
        
        def query_processing_logs(query: str) -> str:
            """Query data processing logs for troubleshooting"""
            # Simulate log analysis
            if "error" in query.lower():
                return "Found 23 error entries in the last 24h: 15 timeout errors, 5 connection failures, 3 data format issues. Peak error time: 2:30 AM during batch processing."
            elif "performance" in query.lower():
                return "Performance metrics: Avg processing time 2.3s per batch, 98.5% success rate, memory usage peaked at 4.2GB during ETL operations."
            elif "throughput" in query.lower():
                return "Throughput analysis: 847 batches/hour average, peak throughput 1,250 batches/hour, lowest 234 batches/hour during maintenance window."
            else:
                return f"Processing logs query '{query}' returned 156 relevant entries. Most recent: Pipeline completed successfully at {datetime.now().strftime('%H:%M:%S')}"
        
        def execute_data_pipeline(pipeline_config: str) -> str:
            """Execute data processing pipeline with specified configuration"""
            # Simulate pipeline execution
            execution_time = 45 + hash(pipeline_config) % 60  # 45-105 seconds
            processed_records = 10000 + hash(pipeline_config) % 50000
            
            return f"Pipeline execution completed:\\n  Config: {pipeline_config}\\n  Processing time: {execution_time}s\\n  Records processed: {processed_records:,}\\n  Status: SUCCESS"
        
        def monitor_system_resources() -> str:
            """Monitor current system resource utilization"""
            # Simulate resource monitoring
            resources = {
                "cpu_usage": 67.3,
                "memory_usage": 78.9,
                "disk_usage": 45.2,
                "network_io": 234.5,
                "active_connections": 127
            }
            
            alerts = []
            if resources["memory_usage"] > 85:
                alerts.append("High memory usage detected")
            if resources["cpu_usage"] > 80:
                alerts.append("High CPU utilization")
                
            result = f"System Resource Status:\\n"
            result += f"  CPU Usage: {resources['cpu_usage']}%\\n"
            result += f"  Memory Usage: {resources['memory_usage']}%\\n"
            result += f"  Disk Usage: {resources['disk_usage']}%\\n"
            result += f"  Network I/O: {resources['network_io']} MB/s\\n"
            result += f"  Active Connections: {resources['active_connections']}\\n"
            
            if alerts:
                result += f"\\nAlerts:\\n"
                for alert in alerts:
                    result += f"  âš ï¸  {alert}\\n"
                    
            return result
        
        def validate_data_schema(schema_info: str) -> str:
            """Validate data schema compliance"""
            # Simulate schema validation
            validation_results = {
                "total_fields": 47,
                "valid_fields": 45,
                "invalid_fields": 2,
                "missing_fields": 0,
                "extra_fields": 0
            }
            
            issues = [
                "Field 'customer_id' type mismatch: expected INT, found VARCHAR",
                "Field 'transaction_date' format inconsistent: mixed ISO and US formats"
            ]
            
            result = f"Schema Validation Results for {schema_info}:\\n"
            result += f"  Total Fields: {validation_results['total_fields']}\\n"
            result += f"  Valid Fields: {validation_results['valid_fields']}\\n"
            result += f"  Invalid Fields: {validation_results['invalid_fields']}\\n"
            
            if issues:
                result += f"\\nValidation Issues:\\n"
                for issue in issues:
                    result += f"  âŒ {issue}\\n"
                    
            return result
        
        # Register tools
        self.available_tools = {
            "analyze_data_quality": {
                "name": "DataQualityAnalyzer", 
                "description": "Analyze data quality metrics for datasets",
                "function": analyze_data_quality
            },
            "query_processing_logs": {
                "name": "LogAnalyzer",
                "description": "Query and analyze data processing logs",
                "function": query_processing_logs
            },
            "execute_data_pipeline": {
                "name": "PipelineExecutor", 
                "description": "Execute data processing pipelines",
                "function": execute_data_pipeline
            },
            "monitor_system_resources": {
                "name": "ResourceMonitor",
                "description": "Monitor system resource utilization",
                "function": monitor_system_resources
            },
            "validate_data_schema": {
                "name": "SchemaValidator",
                "description": "Validate data schema compliance", 
                "function": validate_data_schema
            }
        }
    
    def _generate_llm_response(self, prompt: str, context: Dict) -> str:
        """Generate mock LLM response based on prompt and context"""
        
        # Reasoning responses
        if "think about" in prompt.lower() or "analyze" in prompt.lower():
            if "data quality" in prompt.lower():
                return "I need to analyze the data quality issues. Based on the request, I should use the DataQualityAnalyzer tool to get comprehensive quality metrics, then interpret the results to provide actionable recommendations."
            elif "performance" in prompt.lower():
                return "To understand performance issues, I should first query the processing logs to identify patterns, then check system resources to see if there are resource constraints affecting performance."
            elif "pipeline" in prompt.lower():
                return "For pipeline issues, I need to first validate the data schema to ensure data compatibility, then check processing logs for any execution errors, and finally monitor system resources during pipeline execution."
            else:
                return "Let me analyze this data processing request systematically. I need to gather relevant information using the appropriate tools to provide a comprehensive solution."
        
        # Action selection responses  
        elif "which tool" in prompt.lower() or "what action" in prompt.lower():
            available_tools_list = list(self.available_tools.keys())
            if "quality" in context.get('request', '').lower():
                return f"I should use the analyze_data_quality tool to get detailed quality metrics."
            elif "log" in context.get('request', '').lower():
                return f"I should use the query_processing_logs tool to investigate processing history."
            else:
                return f"Based on the request, I should use the {available_tools_list[0]} tool to gather information."
        
        # Summary responses
        else:
            return "Based on my analysis using the available data processing tools, I can provide a comprehensive assessment of the situation with specific recommendations for optimization."
    
    def _execute_reasoning_step(self, request: str, context: Dict) -> AgentStep:
        """Execute a reasoning (thinking) step"""
        thinking_prompt = f"""
        I need to think about this data processing request: {request}
        
        Available tools: {list(self.available_tools.keys())}
        Current context: {context}
        
        What should I analyze first and which tools would be most helpful?
        """
        
        thought = self._generate_llm_response(thinking_prompt, context)
        
        return AgentStep(
            step_type="thought",
            content=thought,
            timestamp=datetime.now()
        )
    
    def _execute_action_step(self, thought: str, context: Dict) -> AgentStep:
        """Execute an action step based on previous reasoning"""
        
        # Simple tool selection logic based on thought content
        selected_tool = None
        tool_input = ""
        
        if "quality" in thought.lower():
            selected_tool = "analyze_data_quality"
            tool_input = context.get('request', 'general_dataset')
        elif "logs" in thought.lower() or "log" in thought.lower():
            selected_tool = "query_processing_logs"
            tool_input = "performance errors"
        elif "pipeline" in thought.lower():
            selected_tool = "execute_data_pipeline" 
            tool_input = "standard_etl_config"
        elif "resource" in thought.lower() or "monitor" in thought.lower():
            selected_tool = "monitor_system_resources"
            tool_input = ""
        elif "schema" in thought.lower():
            selected_tool = "validate_data_schema"
            tool_input = "customer_transactions_schema"
        else:
            # Default to first available tool
            selected_tool = list(self.available_tools.keys())[0]
            tool_input = context.get('request', 'default_input')
        
        # Execute tool
        tool_info = self.available_tools[selected_tool]
        tool_result = tool_info["function"](tool_input)
        
        return AgentStep(
            step_type="action",
            content=f"Using {tool_info['name']} with input: {tool_input}",
            timestamp=datetime.now(),
            tool_name=selected_tool,
            tool_result=tool_result
        )
    
    def process_request(self, request: str) -> Dict[str, Any]:
        """
        Process data engineering request using ReAct pattern.
        Demonstrates the Think-Act-Think-Act cycle for data problem solving.
        """
        print(f"ğŸ¤– DataProcessingAgent Processing Request: {request}")
        print("=" * 60)
        
        execution_steps = []
        context = {"request": request, "iteration": 0}
        status = AgentStatus.THINKING
        
        for iteration in range(self.max_iterations):
            context["iteration"] = iteration
            
            print(f"\\nğŸ”„ Iteration {iteration + 1}")
            print("-" * 30)
            
            if status == AgentStatus.THINKING:
                # Reasoning step
                thought_step = self._execute_reasoning_step(request, context)
                execution_steps.append(thought_step)
                
                print(f"ğŸ’­ Thought: {thought_step.content}")
                
                # Move to action
                status = AgentStatus.ACTING
                
            elif status == AgentStatus.ACTING:
                # Action step
                action_step = self._execute_action_step(execution_steps[-1].content, context)
                execution_steps.append(action_step)
                
                print(f"ğŸ› ï¸  Action: {action_step.content}")
                print(f"ğŸ“Š Result: {action_step.tool_result[:200]}...")
                
                # Update context with results
                context["last_action_result"] = action_step.tool_result
                
                # Check if we have enough information (simplified)
                if iteration >= 1:  # After at least one think-act cycle
                    status = AgentStatus.COMPLETE
                    break
                else:
                    status = AgentStatus.THINKING
        
        # Generate final response
        final_response = self._generate_final_response(request, execution_steps)
        
        # Save to memory
        self.memory.save_interaction(request, final_response, execution_steps)
        
        result = {
            "request": request,
            "response": final_response,
            "steps": execution_steps,
            "iterations": len(execution_steps) // 2,  # Each iteration has thought + action
            "status": status.value,
            "tools_used": [step.tool_name for step in execution_steps if step.tool_name]
        }
        
        print(f"\\nâœ… Final Response: {final_response}")
        print(f"ğŸ”§ Tools Used: {result['tools_used']}")
        
        return result
    
    def _generate_final_response(self, request: str, steps: List[AgentStep]) -> str:
        """Generate final comprehensive response based on all reasoning steps"""
        
        # Extract tool results
        tool_results = [step.tool_result for step in steps if step.tool_result]
        
        if "quality" in request.lower():
            return "Based on my data quality analysis, the dataset shows good overall health with 94.7% completeness and 98.2% accuracy. I recommend focusing on improving consistency (96.1%) by standardizing date formats across sources."
        elif "performance" in request.lower():
            return "Performance analysis indicates average processing time of 2.3s per batch with 98.5% success rate. System resources are within normal ranges, but I recommend monitoring memory usage during peak hours as it reached 78.9%."
        elif "pipeline" in request.lower():
            return "Pipeline analysis shows successful execution with good throughput. Schema validation identified 2 field type mismatches that should be addressed to prevent future processing errors. System resources are adequate for current load."
        else:
            return f"Comprehensive analysis completed using {len(tool_results)} data processing tools. The system is operating within normal parameters with opportunities for optimization in data consistency and resource utilization."


class ConversationMemory:
    """
    Memory system for maintaining conversation context across agent interactions.
    Demonstrates different memory patterns for data processing workflows.
    """
    
    def __init__(self, memory_type="buffer", max_size=10):
        self.memory_type = memory_type
        self.max_size = max_size
        self.interactions = []
        self.summary = ""
    
    def save_interaction(self, request: str, response: str, steps: List[AgentStep]):
        """Save agent interaction to memory"""
        interaction = {
            "timestamp": datetime.now(),
            "request": request,
            "response": response,
            "steps_count": len(steps),
            "tools_used": [step.tool_name for step in steps if step.tool_name]
        }
        
        self.interactions.append(interaction)
        
        # Manage memory size
        if len(self.interactions) > self.max_size:
            if self.memory_type == "buffer":
                # Remove oldest
                self.interactions.pop(0)
            elif self.memory_type == "summary":
                # Summarize older interactions
                self._summarize_old_interactions()
    
    def _summarize_old_interactions(self):
        """Summarize older interactions to preserve context while reducing memory"""
        if len(self.interactions) > self.max_size:
            old_interactions = self.interactions[:self.max_size//2]
            
            # Create summary
            requests = [i["request"] for i in old_interactions]
            tools_used = []
            for i in old_interactions:
                tools_used.extend(i["tools_used"])
            
            self.summary = f"Previous analysis sessions covered: {', '.join(requests[:3])}... Used tools: {set(tools_used)}"
            
            # Keep recent interactions
            self.interactions = self.interactions[self.max_size//2:]
    
    def get_context(self) -> str:
        """Get relevant context for current conversation"""
        context = ""
        
        if self.summary:
            context += f"Previous context summary: {self.summary}\\n\\n"
        
        if self.interactions:
            context += "Recent interactions:\\n"
            for interaction in self.interactions[-3:]:  # Last 3 interactions
                context += f"- {interaction['request'][:50]}... -> Used {len(interaction['tools_used'])} tools\\n"
        
        return context


class MultiAgentCoordinator:
    """
    Coordination system for multiple specialized data processing agents.
    Demonstrates agent orchestration patterns for complex data workflows.
    """
    
    def __init__(self):
        self.agents = {
            "quality_agent": DataProcessingAgent(),
            "performance_agent": DataProcessingAgent(), 
            "security_agent": DataProcessingAgent()
        }
        self.coordination_history = []
    
    def coordinate_analysis(self, request: str) -> Dict[str, Any]:
        """Coordinate multiple agents for comprehensive data analysis"""
        print(f"ğŸ¯ MultiAgentCoordinator: Orchestrating Analysis")
        print(f"Request: {request}")
        print("=" * 60)
        
        results = {}
        
        # Route requests to specialized agents
        if "quality" in request.lower():
            results["quality"] = self.agents["quality_agent"].process_request(request)
        elif "performance" in request.lower():
            results["performance"] = self.agents["performance_agent"].process_request(request) 
        elif "security" in request.lower():
            results["security"] = self.agents["security_agent"].process_request(request)
        else:
            # Comprehensive analysis with multiple agents
            results["quality"] = self.agents["quality_agent"].process_request(f"Analyze data quality for: {request}")
            results["performance"] = self.agents["performance_agent"].process_request(f"Analyze performance for: {request}")
        
        # Synthesize results
        synthesis = self._synthesize_agent_results(results)
        
        coordination_result = {
            "request": request,
            "agent_results": results,
            "synthesis": synthesis,
            "agents_used": list(results.keys()),
            "total_tools_used": sum(len(r.get("tools_used", [])) for r in results.values())
        }
        
        self.coordination_history.append(coordination_result)
        
        print(f"\\nğŸ¯ Coordination Complete:")
        print(f"   Agents Used: {coordination_result['agents_used']}")
        print(f"   Total Tools: {coordination_result['total_tools_used']}")
        print(f"   Synthesis: {synthesis[:150]}...")
        
        return coordination_result
    
    def _synthesize_agent_results(self, agent_results: Dict[str, Any]) -> str:
        """Synthesize results from multiple agents into unified response"""
        if len(agent_results) == 1:
            return list(agent_results.values())[0]["response"]
        
        return f"Comprehensive analysis from {len(agent_results)} specialized agents indicates coordinated approach needed for optimization. Quality, performance, and operational aspects all require attention for holistic improvement."


def demonstrate_react_pattern():
    """Demonstrate ReAct pattern for data processing"""
    print("ğŸ¯ ReAct Pattern Demonstration")
    print("=" * 50)
    print("Reasoning + Acting cycle for data processing intelligence\\n")
    
    agent = DataProcessingAgent()
    
    # Test requests
    test_requests = [
        "Analyze data quality issues in our customer analytics pipeline",
        "Investigate performance bottlenecks in ETL processing",
        "Validate schema compliance for new data sources"
    ]
    
    for request in test_requests:
        print(f"\\n{'='*60}")
        result = agent.process_request(request)
        print(f"\\nRequest processed in {result['iterations']} reasoning cycles")


def demonstrate_memory_patterns():
    """Demonstrate different memory patterns"""
    print("\\n\\nğŸ§  Memory Patterns Demonstration")
    print("=" * 50)
    
    # Buffer Memory
    buffer_memory = ConversationMemory(memory_type="buffer", max_size=5)
    print("ğŸ“š Buffer Memory: Complete conversation history")
    
    # Summary Memory
    summary_memory = ConversationMemory(memory_type="summary", max_size=5)
    print("ğŸ“‹ Summary Memory: Intelligent context compression")
    
    # Simulate interactions
    interactions = [
        ("Check data quality metrics", "Quality analysis completed - 94% overall score"),
        ("Monitor pipeline performance", "Performance metrics show 2.3s average processing time"),
        ("Validate new data schema", "Schema validation found 2 field type mismatches"),
        ("Analyze error patterns", "Error analysis shows 23 issues, primarily timeout related"),
        ("Review resource utilization", "Resources within normal ranges, memory peaked at 78%")
    ]
    
    for request, response in interactions:
        buffer_memory.save_interaction(request, response, [])
        summary_memory.save_interaction(request, response, [])
    
    print(f"\\nBuffer Memory Context ({len(buffer_memory.interactions)} interactions):")
    print(buffer_memory.get_context()[:200] + "...")
    
    print(f"\\nSummary Memory Context:")
    print(summary_memory.get_context()[:200] + "...")


def demonstrate_multi_agent_coordination():
    """Demonstrate multi-agent coordination patterns"""
    print("\\n\\nğŸ¯ Multi-Agent Coordination Demonstration")
    print("=" * 50)
    
    coordinator = MultiAgentCoordinator()
    
    # Test coordination requests
    coordination_requests = [
        "Comprehensive analysis of data pipeline health",
        "Quality assessment for customer data processing",
        "Performance optimization for ETL workflows"
    ]
    
    for request in coordination_requests:
        print(f"\\n{'-'*60}")
        result = coordinator.coordinate_analysis(request)


def main():
    """Main demonstration of LangChain agent patterns"""
    print("ğŸ¯ğŸ“âš™ï¸ Session 2: LangChain Agents - Advanced Patterns")
    print("=" * 60)
    print("Demonstrating ReAct agents, tools, memory, and coordination")
    print("-" * 60)
    
    # 1. ReAct Pattern
    demonstrate_react_pattern()
    
    # 2. Memory Patterns
    demonstrate_memory_patterns()
    
    # 3. Multi-Agent Coordination
    demonstrate_multi_agent_coordination()
    
    print("\\n\\nğŸ¯ Advanced Agent Patterns Summary")
    print("=" * 50)
    print("âœ… ReAct pattern (Reasoning + Acting)")
    print("âœ… Tool integration with data processing capabilities")
    print("âœ… Memory management (Buffer, Summary, Window patterns)")
    print("âœ… Multi-agent coordination for complex workflows")
    print("\\nğŸš€ Ready for production data processing intelligence!")


if __name__ == "__main__":
    main()