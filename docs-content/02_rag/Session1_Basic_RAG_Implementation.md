# Session 1: Basic RAG Implementation - From Theory to Practice

## üéØ Learning Navigation Hub
**Total Time Investment**: 90 minutes (Core) + 60 minutes (Optional)
**Your Learning Path**: Choose your engagement level

### Quick Start Guide

- **üëÄ Observer (45 min)**: Read concepts + examine code patterns
- **üôã‚Äç‚ôÇÔ∏è Participant (90 min)**: Follow exercises + implement solutions  
- **üõ†Ô∏è Implementer (150 min)**: Build custom systems + explore advanced patterns

## üìã SESSION OVERVIEW DASHBOARD

### Core Learning Track (90 minutes) - REQUIRED
| Section | Concept Load | Time | Skills |
|---------|--------------|------|--------|
| Environment Setup | 3 concepts | 15 min | Configuration |
| Document Pipeline | 5 concepts | 25 min | Processing |
| Vector Integration | 4 concepts | 20 min | Storage |
| RAG System | 6 concepts | 25 min | Architecture |
| Testing Framework | 2 concepts | 15 min | Evaluation |

### Optional Deep Dive Modules (Choose Your Adventure)

- üî¨ **[Module A: Production RAG Patterns](Session1_ModuleA_Production_Patterns.md)** (30 min)
- üè≠ **[Module B: Enterprise Deployment](Session1_ModuleB_Enterprise_Deployment.md)** (30 min)

## üß≠ CORE SECTION (Required - 90 minutes)

### Learning Outcomes

By the end of this session, you will be able to:

- **Build** a complete RAG pipeline from scratch using Python
- **Implement** document parsing, chunking, and preprocessing strategies
- **Configure** vector databases and embedding models for optimal performance
- **Create** a functional retrieval and generation system
- **Evaluate** basic RAG performance using standard metrics

## üìö Chapter Introduction

### **From Theory to Code: Your First RAG System**

![RAG Architecture Overview](images/RAG-overview.png)

This session transforms RAG theory into working code. You'll build a complete, production-ready RAG system from scratch, learning essential skills for document processing, vector search, and intelligent response generation.

**What Makes This Implementation Special:**

- **Production-Ready**: Built with enterprise-grade tools (LangChain, ChromaDB)
- **Modular Design**: Clean architecture for easy extension and maintenance
- **Intelligent Chunking**: Token-aware splitting with semantic boundaries
- **Performance Focused**: Efficient indexing and retrieval mechanisms
- **Evaluation Ready**: Built-in metrics and testing framework

### **Your Learning Journey**

By the end of this hands-on session, you'll have:

- A working RAG system you can query in real-time
- Deep understanding of each pipeline component
- Skills to customize and optimize for different domains
- Foundation code to build upon in advanced sessions

Let's start building your RAG system step by step! üõ†Ô∏è

---

## **Part 1: Setting Up Your RAG Development Environment (15 minutes)**

### **Environment Configuration**

Before we start coding, let's set up a proper development environment:

```python
# requirements.txt for basic RAG
langchain==0.1.0
langchain-community==0.0.13
langchain-openai==0.0.5
chromadb==0.4.22
tiktoken==0.5.2
python-dotenv==1.0.0
beautifulsoup4==4.12.2
requests==2.31.0
numpy==1.24.3
```

**Step 1: Project Structure Setup**

```bash
# Create project structure
mkdir basic-rag-system
cd basic-rag-system
mkdir data documents src tests
touch .env README.md
```

**Step 2: Environment Variables**

```bash
# .env file
OPENAI_API_KEY=your_openai_api_key_here
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K=5
```

**Step 3: Core Dependencies**

```python
# src/config.py
import os
from dotenv import load_dotenv

load_dotenv()

class RAGConfig:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", 1000))
    CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", 200))
    TOP_K = int(os.getenv("TOP_K", 5))

    # Vector database settings
    VECTOR_DB_PATH = "./chroma_db"
    COLLECTION_NAME = "rag_documents"

    # Model settings
    EMBEDDING_MODEL = "text-embedding-ada-002"
    LLM_MODEL = "gpt-3.5-turbo"
```

---

## **Part 2: Document Ingestion Pipeline (25 minutes)**

### **Building the Document Loader**

The first step in any RAG system is loading and processing documents. Let's build a flexible document loader:

### **Document Loader Implementation**

**Step 1: Initialize Document Loader**

**Understanding Document Loading Architecture:**

A robust document loader forms the foundation of any RAG system. It must handle multiple file formats, provide error resilience, and maintain metadata for source attribution during retrieval.

**Core Design Principles:**

- **Format flexibility**: Support common document types without tight coupling
- **Error handling**: Graceful failure when documents can't be loaded
- **Metadata preservation**: Track source information for citation and debugging
- **Extensibility**: Easy to add new document formats

**Step 1: Document Loader Foundation**

```python
# src/document_loader.py - Core setup
from typing import List, Dict, Any
import requests
from bs4 import BeautifulSoup
from langchain.schema import Document
import os

class DocumentLoader:
    """Handles loading documents from various sources."""

    def __init__(self):
        self.supported_formats = ['.txt', '.md', '.html']
        print(f"Loader ready for formats: {self.supported_formats}")
```

*This initialization sets up format support and provides immediate feedback about capabilities.*

**Step 2: File Loading with Error Handling**

```python
    def load_from_file(self, file_path: str) -> List[Document]:
        """Load document from local file with robust error handling."""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")

        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
                
            print(f"Loaded {len(content)} characters from {file_path}")
            
            return [Document(
                page_content=content,
                metadata={"source": file_path, "type": "file"}
            )]
        except Exception as e:
            print(f"Error loading {file_path}: {e}")
            return []
```

*Enhanced with try-catch error handling and progress feedback for better debugging.*

**Step 3: Web Content Extraction Strategy**

**Why Web Loading Differs from File Loading:**
Web content requires special handling because HTML contains structural elements (scripts, styles, navigation) that interfere with semantic search but don't contain useful information for RAG.

```python
    def load_from_url(self, url: str) -> List[Document]:
        """Load and clean web content for RAG processing."""
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            print(f"Successfully fetched {url}")

            soup = BeautifulSoup(response.content, 'html.parser')
            return self._clean_html_content(soup, url)
            
        except Exception as e:
            print(f"Error loading URL {url}: {str(e)}")
            return []
```

*Separates URL fetching from content processing for better modularity and error tracking.*

**Step 4: HTML Content Cleaning Pipeline**

First, we remove noise elements that interfere with semantic search:

```python
    def _clean_html_content(self, soup, url: str) -> List[Document]:
        """Clean HTML content for optimal RAG processing."""
        # Remove non-content elements that interfere with search
        for element in soup(["script", "style", "nav", "header", "footer"]):
            element.extract()

        # Extract clean text
        text = soup.get_text()
```

*This first phase removes structural HTML elements that don't contain useful information for RAG.*

Next, we normalize whitespace while preserving semantic structure:

```python
        # Normalize whitespace while preserving paragraph structure
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines
                 for phrase in line.split("  "))
        clean_text = ' '.join(chunk for chunk in chunks if chunk)

        print(f"Extracted {len(clean_text)} characters of clean text")
        
        return [Document(
            page_content=clean_text,
            metadata={"source": url, "type": "web"}
        )]
```

*This cleaning pipeline normalizes whitespace and packages the clean text into a Document object with source metadata.*

**Step 5: Intelligent Batch Processing**

**Why Batch Processing Matters:**
RAG systems typically need to process multiple documents from various sources. Smart batch processing provides progress tracking, error isolation, and performance optimization.

```python
    def load_documents(self, sources: List[str]) -> List[Document]:
        """Load multiple documents with progress tracking and error isolation."""
        all_documents = []
        success_count = 0
        error_count = 0

        print(f"Processing {len(sources)} document sources...")
        
        for i, source in enumerate(sources, 1):
            print(f"[{i}/{len(sources)}] Processing: {source[:50]}...")
```

*This initialization sets up tracking variables and begins iterating through document sources.*

The processing logic handles different source types with error isolation:

```python
            try:
                if source.startswith(('http://', 'https://')):
                    docs = self.load_from_url(source)
                elif os.path.isfile(source):
                    docs = self.load_from_file(source)
                else:
                    print(f"  ‚ùå Unsupported source type: {source}")
                    error_count += 1
                    continue
```

*Source type detection routes to appropriate loading methods while handling unsupported formats gracefully.*

Finally, we collect successful results and provide comprehensive feedback:

```python
                if docs:  # Only add if documents were successfully loaded
                    all_documents.extend(docs)
                    success_count += 1
                    print(f"  ‚úÖ Loaded {len(docs)} documents")
                else:
                    error_count += 1
                    print(f"  ‚ö†Ô∏è No documents loaded from {source}")
                    
            except Exception as e:
                error_count += 1
                print(f"  ‚ùå Error processing {source}: {e}")

        print(f"\nüìä Batch Results: {success_count} successful, {error_count} errors")
        print(f"Total documents loaded: {len(all_documents)}")
        return all_documents
```

*This enhanced batch processor provides detailed progress feedback and error isolation, crucial for debugging document loading issues.*

### **Document Chunking Strategies**

Effective chunking is crucial for RAG performance. Let's implement smart chunking:

```python
# src/text_splitter.py
from typing import List
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import tiktoken

class IntelligentTextSplitter:
    """Advanced text splitting with multiple strategies."""

    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
```

**Step 6: Token-Aware Chunking**

First, we implement token counting for accurate length measurement:

```python
    def count_tokens(self, text: str) -> int:
        """Count tokens in text using tiktoken."""
        return len(self.encoding.encode(text))
```

*This token counting ensures chunks respect LLM token limits rather than just character counts.*

Next, we implement recursive character splitting with intelligent separators:

```python
    def recursive_split(self, documents: List[Document]) -> List[Document]:
        """Split documents using recursive character splitting."""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=self.count_tokens,
            separators=["\n\n", "\n", " ", ""]
        )

        split_docs = text_splitter.split_documents(documents)
```

*The hierarchical separators ensure splits happen at paragraph, sentence, and word boundaries first.*

Finally, we add metadata tracking for each chunk:

```python
        # Add chunk metadata
        for i, doc in enumerate(split_docs):
            doc.metadata.update({
                "chunk_id": i,
                "chunk_size": self.count_tokens(doc.page_content)
            })

        return split_docs
```

**Step 7: Semantic Chunking Implementation**

### **Paragraph-Based Splitting**

```python
    def semantic_split(self, documents: List[Document]) -> List[Document]:
        """Split based on semantic boundaries (paragraphs, sections)."""
        semantic_chunks = []

        for doc in documents:
            content = doc.page_content

            # Split by double newlines (paragraphs)
            paragraphs = content.split('\n\n')

            current_chunk = ""
            current_tokens = 0
```

*Initializes semantic splitting by dividing content at paragraph boundaries (double newlines), which preserves natural content structure.*

### **Intelligent Chunk Building**

```python
            for paragraph in paragraphs:
                paragraph_tokens = self.count_tokens(paragraph)

                # If adding this paragraph exceeds chunk size, save current chunk
                if current_tokens + paragraph_tokens > self.chunk_size and current_chunk:
                    semantic_chunks.append(Document(
                        page_content=current_chunk.strip(),
                        metadata={
                            **doc.metadata,
                            "chunk_type": "semantic",
                            "token_count": current_tokens
                        }
                    ))
```

*Builds chunks by adding paragraphs until the token limit is reached, then saves the chunk with metadata tracking.*

### **Overlap Management**

```python
                    # Start new chunk with overlap
                    overlap_text = current_chunk[-self.chunk_overlap:] if len(current_chunk) > self.chunk_overlap else current_chunk
                    current_chunk = overlap_text + paragraph
                    current_tokens = self.count_tokens(current_chunk)
                else:
                    current_chunk += "\n\n" + paragraph if current_chunk else paragraph
                    current_tokens += paragraph_tokens

            # Add the final chunk
            if current_chunk:
                semantic_chunks.append(Document(
                    page_content=current_chunk.strip(),
                    metadata={
                        **doc.metadata,
                        "chunk_type": "semantic",
                        "token_count": current_tokens
                    }
                ))

        return semantic_chunks
```

*Ensures continuity between chunks by including overlap text, then handles the final chunk to avoid losing content at document ends.*

---

## **Part 3: Vector Database Setup and Indexing (20 minutes)**

### **ChromaDB Integration**

Let's set up a persistent vector database for our RAG system:

```python
# src/vector_store.py
from typing import List, Optional
import chromadb
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
from src.config import RAGConfig

class VectorStore:
    """Manages vector storage and retrieval operations."""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=config.OPENAI_API_KEY,
            model=config.EMBEDDING_MODEL
        )
        self.vectorstore = None
        self._initialize_store()
```

**Step 8: Vector Store Initialization**

```python
    def _initialize_store(self):
        """Initialize ChromaDB vector store."""
        try:
            # Try to load existing store
            self.vectorstore = Chroma(
                persist_directory=self.config.VECTOR_DB_PATH,
                embedding_function=self.embeddings,
                collection_name=self.config.COLLECTION_NAME
            )
            print("Loaded existing vector store")

        except Exception as e:
            print(f"Creating new vector store: {e}")
            self.vectorstore = Chroma(
                persist_directory=self.config.VECTOR_DB_PATH,
                embedding_function=self.embeddings,
                collection_name=self.config.COLLECTION_NAME
            )
```

**Step 9: Document Indexing**

```python
    def add_documents(self, documents: List[Document]) -> List[str]:
        """Add documents to vector store."""
        if not documents:
            return []

        print(f"Adding {len(documents)} documents to vector store...")

        # Generate unique IDs for documents
        doc_ids = [f"doc_{i}_{hash(doc.page_content[:100])}"
                  for i, doc in enumerate(documents)]

        # Add documents with IDs
        self.vectorstore.add_documents(documents, ids=doc_ids)

        # Persist the store
        self.vectorstore.persist()

        print(f"Successfully indexed {len(documents)} document chunks")
        return doc_ids
```

**Step 10: Similarity Search Implementation**

First, we implement basic similarity search for document retrieval:

```python
    def similarity_search(self, query: str, k: Optional[int] = None) -> List[Document]:
        """Perform similarity search."""
        k = k or self.config.TOP_K

        results = self.vectorstore.similarity_search(
            query=query,
            k=k
        )

        return results
```

*This method returns the most relevant documents without similarity scores.*

For more detailed analysis, we also provide search with relevance scores:

```python
    def similarity_search_with_scores(self, query: str, k: Optional[int] = None) -> List[tuple]:
        """Perform similarity search with relevance scores."""
        k = k or self.config.TOP_K

        results = self.vectorstore.similarity_search_with_score(
            query=query,
            k=k
        )

        return results
```

---

## **Part 4: Building the RAG Pipeline (25 minutes)**

### **Core RAG System**

Now let's build the main RAG system that combines retrieval and generation:

```python
# src/rag_system.py
from typing import List, Dict, Any, Optional
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from src.config import RAGConfig
from src.vector_store import VectorStore
```

*These imports provide the essential components for building our integrated RAG system.*

Now we implement the core RAG system class with initialization:

```python
class BasicRAGSystem:
    """Core RAG system combining retrieval and generation."""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.vector_store = VectorStore(config)

        # Initialize LLM
        self.llm = ChatOpenAI(
            openai_api_key=config.OPENAI_API_KEY,
            model_name=config.LLM_MODEL,
            temperature=0.7
        )

        # Define RAG prompt template
        self.prompt_template = self._create_prompt_template()
```

**Step 11: Prompt Template Design**

```python
    def _create_prompt_template(self) -> PromptTemplate:
        """Create the RAG prompt template."""
        template = """You are a helpful AI assistant. Use the following context to answer the user's question.
If you cannot find the answer in the context, say "I don't have enough information to answer that question."

Context:
{context}

Question: {question}

Answer: Let me help you with that based on the provided information."""

        return PromptTemplate(
            input_variables=["context", "question"],
            template=template
        )
```

**Step 12: Context Preparation**

```python
    def _prepare_context(self, documents: List[Document]) -> str:
        """Prepare context from retrieved documents."""
        if not documents:
            return "No relevant information found."

        context_parts = []
        for i, doc in enumerate(documents, 1):
            source = doc.metadata.get("source", "Unknown")
            chunk_info = f"[Source {i}: {source}]"
            context_parts.append(f"{chunk_info}\n{doc.page_content}")

        return "\n\n".join(context_parts)
```

**Step 13: RAG Query Processing Pipeline**

### **Document Retrieval Phase**

```python
    def query(self, question: str, k: Optional[int] = None) -> Dict[str, Any]:
        """Process a RAG query and return detailed results."""
        # Step 1: Retrieve relevant documents
        retrieved_docs = self.vector_store.similarity_search_with_scores(
            query=question,
            k=k or self.config.TOP_K
        )

        if not retrieved_docs:
            return {
                "answer": "I don't have any relevant information to answer your question.",
                "sources": [],
                "confidence": 0.0
            }
```

*Retrieves the most relevant documents using vector similarity search with scores to measure relevance quality.*

### **Context Preparation and Generation**

```python
        # Step 2: Prepare context
        documents = [doc for doc, score in retrieved_docs]
        context = self._prepare_context(documents)

        # Step 3: Generate response
        prompt = self.prompt_template.format(
            context=context,
            question=question
        )

        response = self.llm.predict(prompt)
```

*Formats retrieved documents into context, then generates an answer using the LLM with the augmented prompt.*

### **Response Quality Assessment**

```python
        # Step 4: Calculate confidence based on retrieval scores
        scores = [score for doc, score in retrieved_docs]
        avg_score = sum(scores) / len(scores)
        confidence = max(0.0, min(1.0, 1.0 - avg_score))  # Convert distance to similarity

        return {
            "answer": response,
            "sources": [
                {
                    "content": doc.page_content[:200] + "...",
                    "metadata": doc.metadata,
                    "relevance_score": round(1.0 - score, 3)
                }
                for doc, score in retrieved_docs
            ],
            "confidence": round(confidence, 3),
            "num_sources": len(documents)
        }
```

*Calculates confidence scores based on retrieval quality and returns structured results with answer, sources, and metadata.*

### **Interactive RAG Interface**

Let's create a simple interface to test our RAG system:

```python
# src/interactive_rag.py
from typing import List
from src.rag_system import BasicRAGSystem
from src.document_loader import DocumentLoader
from src.text_splitter import IntelligentTextSplitter
from src.config import RAGConfig

class InteractiveRAG:
    """Interactive interface for RAG system."""

    def __init__(self):
        self.config = RAGConfig()
        self.rag_system = BasicRAGSystem(self.config)
        self.document_loader = DocumentLoader()
        self.text_splitter = IntelligentTextSplitter(
            chunk_size=self.config.CHUNK_SIZE,
            chunk_overlap=self.config.CHUNK_OVERLAP
        )
```

*This initialization sets up all the components needed for an interactive RAG session.*

Now we implement the document loading and indexing pipeline:

```python
    def load_and_index_documents(self, sources: List[str]):
        """Load documents and add them to the vector store."""
        print("Loading documents...")
        documents = self.document_loader.load_documents(sources)

        if not documents:
            print("No documents loaded.")
            return

        print("Splitting documents into chunks...")
        chunks = self.text_splitter.recursive_split(documents)

        print("Indexing documents...")
        self.rag_system.vector_store.add_documents(chunks)

        print(f"Successfully processed and indexed {len(chunks)} chunks from {len(documents)} documents.")
```

**Step 14: Chat Interface**

### **Interactive Chat Interface**

```python
    def start_chat(self):
        """Start interactive chat session."""
        print("=" * 60)
        print("ü§ñ RAG System Ready!")
        print("=" * 60)
        print("Type your questions. Use 'quit' or 'exit' to stop.")
        print("-" * 60)
```

*Sets up a user-friendly chat interface with clear instructions and visual formatting.*

### **Chat Loop Implementation**

```python
        while True:
            try:
                question = input("\nüìù Your question: ").strip()

                if question.lower() in ['quit', 'exit', 'bye']:
                    print("üëã Goodbye!")
                    break

                if not question:
                    print("Please enter a question.")
                    continue
```

*This input handling validates user questions and provides exit conditions.*

The main processing logic handles query execution and error management:

```python
                print("\nüîç Searching and generating answer...")
                result = self.rag_system.query(question)

                self._display_result(result)

            except KeyboardInterrupt:
                print("\nüëã Goodbye!")
                break
            except Exception as e:
                print(f"‚ùå Error: {str(e)}")
```

*Handles user input, processes queries through the RAG system, and provides robust error handling for a smooth user experience.*

    def _display_result(self, result: dict):
        """Display query result in a formatted way."""
        print(f"\nü§ñ **Answer** (Confidence: {result['confidence']})")
        print("-" * 40)
        print(result['answer'])

        if result['sources']:
            print(f"\nüìö **Sources** ({result['num_sources']} documents)")
            print("-" * 40)
            for i, source in enumerate(result['sources'], 1):
                print(f"{i}. Relevance: {source['relevance_score']}")
                print(f"   Source: {source['metadata'].get('source', 'Unknown')}")
                print(f"   Preview: {source['content']}")
                print()

```

---

## **Part 5: Testing and Evaluation (15 minutes)**

### **Basic Performance Metrics**

Let's implement basic evaluation metrics for our RAG system:

```python
# tests/test_rag_performance.py
import time
from typing import List, Dict
from src.interactive_rag import InteractiveRAG

class RAGEvaluator:
    """Basic evaluation metrics for RAG system."""

    def __init__(self, rag_system: InteractiveRAG):
        self.rag_system = rag_system
```

*This evaluator class provides comprehensive metrics for testing RAG system performance.*

**Response Time Evaluation:**

```python
    def evaluate_response_time(self, questions: List[str]) -> Dict[str, float]:
        """Evaluate average response time."""
        times = []

        for question in questions:
            start_time = time.time()
            self.rag_system.rag_system.query(question)
            end_time = time.time()

            times.append(end_time - start_time)

        return {
            "avg_response_time": sum(times) / len(times),
            "min_response_time": min(times),
            "max_response_time": max(times)
        }
```

*This method measures system performance by timing query responses across multiple test questions.*

**Retrieval Quality Assessment:**

```python
    def evaluate_retrieval_quality(self, test_cases: List[Dict]) -> Dict[str, float]:
        """Evaluate retrieval quality using test cases."""
        total_precision = 0
        total_recall = 0

        for case in test_cases:
            question = case["question"]
            expected_sources = set(case["expected_sources"])

            result = self.rag_system.rag_system.query(question)
            retrieved_sources = set([
                source["metadata"].get("source", "")
                for source in result["sources"]
            ])
```

*This evaluation compares retrieved documents against expected sources for precision and recall.*

**Precision and Recall Calculation:**

```python
            if retrieved_sources:
                precision = len(expected_sources & retrieved_sources) / len(retrieved_sources)
                recall = len(expected_sources & retrieved_sources) / len(expected_sources)
            else:
                precision = recall = 0.0

            total_precision += precision
            total_recall += recall

        avg_precision = total_precision / len(test_cases)
        avg_recall = total_recall / len(test_cases)
        f1_score = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0

        return {
            "precision": avg_precision,
            "recall": avg_recall,
            "f1_score": f1_score
        }
```

### **Running Your First RAG System**

Let's create a main script to run everything:

```python
# main.py
from src.interactive_rag import InteractiveRAG

def main():
    # Initialize RAG system
    rag = InteractiveRAG()

    # Sample documents to index
    sample_sources = [
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://en.wikipedia.org/wiki/Machine_learning",
        "https://en.wikipedia.org/wiki/Natural_language_processing"
    ]
```

*This setup initializes the RAG system and defines sample Wikipedia sources for demonstration.*

Now we load the documents and start the interactive session:

```python
    # Load and index documents
    rag.load_and_index_documents(sample_sources)

    # Start interactive chat
    rag.start_chat()

if __name__ == "__main__":
    main()
```

---

## **üß™ Hands-On Exercise: Build Your RAG System**

### **Your Mission**

Create a specialized RAG system for a domain of your choice (e.g., cooking, technology, history).

### **Requirements:**

1. **Data Collection**: Find 3-5 relevant documents (web pages, PDFs, or text files)
2. **Customization**: Modify the prompt template for your domain
3. **Testing**: Create 5 test questions and evaluate the responses
4. **Optimization**: Experiment with different chunk sizes and overlap values

### **Implementation Steps:**

1. Clone the basic RAG code provided above
2. Modify the sources list in `main.py` with your chosen documents
3. Customize the prompt template in `BasicRAGSystem._create_prompt_template()`
4. Run the system and test with domain-specific questions
5. Document your findings and optimizations

---

## **üìù Chapter Summary**

### **What You've Built**

- ‚úÖ Complete document loading and processing pipeline
- ‚úÖ Intelligent text chunking with token awareness
- ‚úÖ Vector database integration with ChromaDB
- ‚úÖ Full RAG system with retrieval and generation
- ‚úÖ Interactive chat interface
- ‚úÖ Basic evaluation framework

### **Key Technical Skills Learned**

1. **Document Processing**: Web scraping, file loading, content cleaning
2. **Text Chunking**: Recursive splitting, semantic boundaries, overlap handling
3. **Vector Operations**: Embedding generation, similarity search, scoring
4. **Prompt Engineering**: Context integration, template design
5. **System Integration**: Combining components into a cohesive pipeline

### **Performance Optimization Tips**

- **Chunk Size**: Experiment with 500-1500 tokens based on your content
- **Overlap**: Use 10-20% overlap for better context continuity
- **Top-K**: Start with 3-5 retrieved documents, adjust based on needs
- **Embedding Model**: Consider different models for domain-specific content

---


## üìù Multiple Choice Test - Session 1

Test your understanding of RAG implementation fundamentals:

**Question 1:** What is the primary advantage of using metadata tracking in document loading?  
A) Improves embedding quality  
B) Speeds up chunking operations  
C) Reduces memory usage during processing  
D) Enables source attribution and filtering capabilities  

 Enables source attribution and filtering capabilities  


**Question 2:** Which chunking approach is most likely to preserve semantic coherence in documents?  
A) Random boundary splitting  
B) Token-count only splitting  
C) Fixed character-length splitting  
D) Semantic paragraph-based splitting  

 Semantic paragraph-based splitting  


**Question 3:** In ChromaDB vector store initialization, what is the purpose of the `persist_directory` parameter?  
A) Speeds up similarity searches  
B) Enables persistent storage between sessions  
C) Improves embedding accuracy  
D) Reduces memory consumption  

 Reduces memory consumption  


**Question 4:** What is the primary benefit of including confidence scores in RAG responses?  
A) Reduces retrieval time  
B) Improves LLM generation quality  
C) Provides transparency about answer reliability  
D) Enables faster document indexing  

 Enables faster document indexing  


**Question 5:** Why does the RAG system separate retrieval and generation into distinct phases?  
A) To reduce computational costs  
B) To support multiple languages  
C) To enable modular optimization and debugging  
D) To prevent embedding conflicts  

 To prevent embedding conflicts  


---

[**üóÇÔ∏è View Test Solutions ‚Üí**](Session1_Test_Solutions.md)

---

## üéØ Session 1 Foundation Complete

**Your RAG Implementation Achievement:**
You've built a complete RAG system from scratch, mastering document processing, vector storage, and retrieval-augmented generation. This foundation enables everything that follows.

---

## üß≠ Navigation

**Previous:** [Session 0 - Introduction to RAG Architecture](Session0_Introduction_to_RAG_Architecture.md)

**Optional Deep Dive Modules:**

- üî¨ **[Module A: Production RAG Patterns](Session1_ModuleA_Production_Patterns.md)** (30 min) - Advanced patterns for production RAG systems
- üè≠ **[Module B: Enterprise Deployment](Session1_ModuleB_Enterprise_Deployment.md)** (30 min) - Enterprise-scale deployment strategies


**Next:** [Session 2 - Advanced Chunking Preprocessing ‚Üí](Session2_Advanced_Chunking_Preprocessing.md)

---

**The Foundation is Built:** Your working RAG system provides the platform for advanced techniques. Ready to make it intelligent? üöÄ
