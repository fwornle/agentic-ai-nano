# Session 3: Vector Databases & Search Optimization

## üéØ Learning Outcomes

By the end of this session, you will be able to:
- **Deploy** multiple vector database architectures (Chroma, Pinecone, Qdrant, Weaviate)
- **Implement** hybrid search strategies combining semantic and keyword search
- **Optimize** vector indices for performance, scalability, and cost efficiency  
- **Design** advanced retrieval pipelines with reranking and filtering
- **Evaluate** search quality using precision, recall, and domain-specific metrics

## üìö Chapter Introduction

### **The Heart of RAG: Vector Search at Scale**

![RAG Architecture Overview](images/RAG-overview.png)

Vector databases transform RAG from a concept into reality - they're the high-performance engines that make semantic search possible at scale. This session takes you from basic similarity search to production-grade vector architectures that power real-world applications.

**The Vector Database Challenge:**
- **Scale**: Millions of vectors with sub-second search times
- **Accuracy**: Precise similarity matching across high dimensions  
- **Flexibility**: Support for filtering, hybrid search, and complex queries
- **Reliability**: Production uptime and data consistency guarantees

**Advanced Solutions You'll Master:**
- **Multi-Database Expertise**: ChromaDB, Pinecone, Qdrant, Weaviate
- **Hybrid Search**: Combining semantic and keyword search optimally
- **Index Optimization**: HNSW tuning and performance acceleration
- **Advanced Retrieval**: Multi-stage pipelines with reranking

### **From Prototype to Production**

This session bridges the gap between simple vector storage and enterprise-grade search systems:
- Build production-ready vector architectures with proper optimization
- Implement sophisticated retrieval pipelines that outperform basic similarity
- Master performance tuning for both accuracy and speed
- Design evaluation frameworks that measure real-world effectiveness

Let's transform your RAG system into a high-performance search engine! ‚ö°

---

## **Part 1: Vector Database Architecture Deep Dive (25 minutes)**

### **Understanding Vector Database Design**

Vector databases are specialized for storing and querying high-dimensional vectors efficiently:

```python
# Core vector database operations
class VectorDatabaseInterface:
    """Abstract interface for vector database operations."""
    
    def __init__(self, dimension: int, metric: str = "cosine"):
        self.dimension = dimension
        self.metric = metric  # cosine, euclidean, dot_product
    
    def add_vectors(self, vectors: List[List[float]], 
                   metadata: List[Dict], ids: List[str]):
        """Add vectors with metadata and IDs."""
        raise NotImplementedError
    
    def search(self, query_vector: List[float], 
              top_k: int = 10, filters: Dict = None):
        """Search for similar vectors with optional filtering."""
        raise NotImplementedError
    
    def update_vector(self, vector_id: str, 
                     new_vector: List[float], new_metadata: Dict):
        """Update existing vector and metadata."""
        raise NotImplementedError
```

### **ChromaDB Implementation**

ChromaDB offers a lightweight, open-source solution ideal for development and moderate-scale production:

### **ChromaDB Production Implementation**

**Step 1: Initialize ChromaDB with Optimization**
```python
# Advanced ChromaDB setup - Core initialization
import chromadb
from chromadb.config import Settings
import numpy as np
from typing import List, Dict, Optional, Tuple

class ChromaVectorStore:
    """Advanced ChromaDB wrapper with optimization features."""
    
    def __init__(self, persist_directory: str, collection_name: str):
        self.persist_directory = persist_directory
        self.collection_name = collection_name
```

*Sets up the basic ChromaDB wrapper with directory and collection name configuration.*

**Step 2: Client Configuration**
```python
        # Initialize client with optimized settings
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                allow_reset=True,
                anonymized_telemetry=False
            )
        )
        
        # Create or get collection with optimized settings
        self.collection = self._initialize_collection()
```

*Configures ChromaDB client with persistent storage and optimized settings for production use.*

**Step 1: Optimized Collection Setup**
```python
    def _initialize_collection(self):
        """Initialize collection with metadata indexing."""
        try:
            # Try to get existing collection
            collection = self.client.get_collection(
                name=self.collection_name
            )
            print(f"Loaded existing collection: {self.collection_name}")
            
        except ValueError:
            # Create new collection with optimized settings
            collection = self.client.create_collection(
                name=self.collection_name,
                metadata={
                    "hnsw:space": "cosine",  # Use cosine similarity
                    "hnsw:construction_ef": 200,  # Build-time accuracy
                    "hnsw:M": 16,  # Connectivity parameter
                    "hnsw:search_ef": 100  # Search-time accuracy
                }
            )
            print(f"Created new collection: {self.collection_name}")
        
        return collection
```

**Step 2: Batch Operations for Performance**
```python
    def add_documents_batch(self, documents: List[str], 
                           embeddings: List[List[float]], 
                           metadata: List[Dict], 
                           ids: List[str], 
                           batch_size: int = 1000):
        """Add documents in optimized batches."""
        
        total_docs = len(documents)
        print(f"Adding {total_docs} documents in batches of {batch_size}")
        
        for i in range(0, total_docs, batch_size):
            batch_end = min(i + batch_size, total_docs)
            
            batch_documents = documents[i:batch_end]
            batch_embeddings = embeddings[i:batch_end]
            batch_metadata = metadata[i:batch_end]
            batch_ids = ids[i:batch_end]
            
            self.collection.add(
                documents=batch_documents,
                embeddings=batch_embeddings,
                metadatas=batch_metadata,
                ids=batch_ids
            )
            
            print(f"Added batch {i//batch_size + 1}/{(total_docs-1)//batch_size + 1}")
```

### **Pinecone Production Setup**

Pinecone provides managed vector search with enterprise features:

```python
# Production Pinecone implementation
import pinecone
import time
from typing import List, Dict, Any

class PineconeVectorStore:
    """Production-ready Pinecone vector store."""
    
    def __init__(self, api_key: str, environment: str, 
                 index_name: str, dimension: int = 1536):
        self.api_key = api_key
        self.environment = environment
        self.index_name = index_name
        self.dimension = dimension
        
        # Initialize Pinecone
        pinecone.init(
            api_key=api_key,
            environment=environment
        )
        
        self.index = self._get_or_create_index()
```

**Step 3: Index Creation with Optimization**
```python
    def _get_or_create_index(self):
        """Get existing index or create optimized new one."""
        
        # Check if index exists
        if self.index_name in pinecone.list_indexes():
            print(f"Connecting to existing index: {self.index_name}")
            return pinecone.Index(self.index_name)
        
        # Create new index with optimized settings
        print(f"Creating new index: {self.index_name}")
        pinecone.create_index(
            name=self.index_name,
            dimension=self.dimension,
            metric='cosine',
            pods=2,  # For production scalability
            replicas=1,  # For high availability
            pod_type='p1.x1',  # Optimized pod type
            metadata_config={
                'indexed': ['source', 'chunk_type', 'timestamp']
            }
        )
        
        # Wait for index to be ready
        while not pinecone.describe_index(self.index_name).status['ready']:
            time.sleep(1)
        
        return pinecone.Index(self.index_name)
```

**Step 4: Advanced Upsert Operations**
```python
    def upsert_vectors_batch(self, vectors: List[Dict], 
                           batch_size: int = 100, 
                           namespace: str = "default"):
        """Efficient batch upsert with error handling."""
        
        total_vectors = len(vectors)
        successful_upserts = 0
        failed_upserts = []
        
        for i in range(0, total_vectors, batch_size):
            batch = vectors[i:i + batch_size]
            
            try:
                # Upsert batch with retry logic
                response = self.index.upsert(
                    vectors=batch,
                    namespace=namespace
                )
                
                successful_upserts += response.upserted_count
                print(f"Upserted batch {i//batch_size + 1}: "
                      f"{response.upserted_count} vectors")
                      
            except Exception as e:
                print(f"Failed to upsert batch {i//batch_size + 1}: {e}")
                failed_upserts.extend(batch)
        
        return {
            'successful': successful_upserts,
            'failed': failed_upserts,
            'total': total_vectors
        }
```

### **Qdrant High-Performance Setup**

Qdrant offers excellent performance and advanced filtering capabilities:

```python
# High-performance Qdrant implementation
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.models import Distance, VectorParams, OptimizersConfig

class QdrantVectorStore:
    """High-performance Qdrant vector store."""
    
    def __init__(self, host: str = "localhost", port: int = 6333, 
                 collection_name: str = "documents"):
        self.host = host
        self.port = port
        self.collection_name = collection_name
        
        # Initialize client
        self.client = QdrantClient(
            host=host, 
            port=port,
            timeout=60  # Extended timeout for large operations
        )
        
        self._setup_collection()
```

**Step 5: Optimized Collection Configuration**
```python
    def _setup_collection(self, dimension: int = 1536):
        """Create collection with performance optimizations."""
        
        # Check if collection exists
        collections = self.client.get_collections().collections
        collection_names = [c.name for c in collections]
        
        if self.collection_name not in collection_names:
            # Create collection with optimized settings
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=dimension,
                    distance=Distance.COSINE,
                    on_disk=True  # Store vectors on disk for memory efficiency
                ),
                optimizers_config=OptimizersConfig(
                    deleted_threshold=0.2,
                    vacuum_min_vector_number=1000,
                    default_segment_number=2,
                    max_segment_size=200000,
                    memmap_threshold=200000,
                    indexing_threshold=20000,
                    flush_interval_sec=1,
                    max_optimization_threads=2
                ),
                hnsw_config=models.HnswConfig(
                    m=16,  # Number of bi-directional connections
                    ef_construct=200,  # Size of dynamic candidate list
                    full_scan_threshold=10000,  # Threshold for full scan
                    max_indexing_threads=2,  # Parallel indexing
                    on_disk=True  # Store index on disk
                )
            )
            
            print(f"Created optimized collection: {self.collection_name}")
        else:
            print(f"Using existing collection: {self.collection_name}")
```

---

## **Part 2: Hybrid Search Implementation (25 minutes)**

### **Semantic + Lexical Search Fusion**

Hybrid search combines vector similarity with traditional keyword search for enhanced relevance:

```python
# Hybrid search implementation
import re
from collections import Counter
from typing import List, Dict, Tuple, Set
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

class HybridSearchEngine:
    """Advanced hybrid search combining semantic and lexical retrieval."""
    
    def __init__(self, vector_store, documents: List[str]):
        self.vector_store = vector_store
        self.documents = documents
        
        # Initialize TF-IDF for lexical search
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=10000,
            stop_words='english',
            ngram_range=(1, 2),  # Include bigrams
            lowercase=True,
            token_pattern=r'\b[a-zA-Z]\w{2,}\b'  # Words with 3+ chars
        )
        
        # Fit TF-IDF on document corpus
        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)
        print(f"Built TF-IDF index for {len(documents)} documents")
```

**Step 6: BM25 Implementation**
```python
    def _compute_bm25_scores(self, query: str, k1: float = 1.2, 
                           b: float = 0.75) -> np.ndarray:
        """Compute BM25 scores for improved lexical ranking."""
        
        # Tokenize query
        query_tokens = self.tfidf_vectorizer.build_analyzer()(query.lower())
        
        # Document lengths
        doc_lengths = np.array([len(doc.split()) for doc in self.documents])
        avg_doc_length = np.mean(doc_lengths)
        
        scores = np.zeros(len(self.documents))
        
        for token in query_tokens:
            if token in self.tfidf_vectorizer.vocabulary_:
                # Get TF-IDF scores for this term
                term_idx = self.tfidf_vectorizer.vocabulary_[token]
                tf_scores = self.tfidf_matrix[:, term_idx].toarray().flatten()
                
                # Convert TF-IDF to term frequency
                tf = tf_scores * len(self.documents)  # Approximate TF
                
                # Calculate IDF
                df = np.sum(tf > 0)  # Document frequency
                if df > 0:
                    idf = np.log((len(self.documents) - df + 0.5) / (df + 0.5))
                    
                    # BM25 formula
                    numerator = tf * (k1 + 1)
                    denominator = tf + k1 * (1 - b + b * doc_lengths / avg_doc_length)
                    
                    scores += idf * (numerator / denominator)
        
        return scores
```

**Step 7: Fusion Strategies**
```python
    def hybrid_search(self, query: str, top_k: int = 10, 
                     semantic_weight: float = 0.7,
                     lexical_weight: float = 0.3,
                     rerank: bool = True) -> List[Dict]:
        """Perform hybrid search with multiple fusion strategies."""
        
        # Step 1: Semantic search
        semantic_results = self.vector_store.similarity_search_with_scores(
            query, k=min(top_k * 3, 50)  # Retrieve more for reranking
        )
        
        # Step 2: Lexical search (BM25)
        lexical_scores = self._compute_bm25_scores(query)
        
        # Step 3: Combine scores using Reciprocal Rank Fusion (RRF)
        combined_results = self._reciprocal_rank_fusion(
            semantic_results, lexical_scores, k=60
        )
        
        # Step 4: Optional reranking
        if rerank:
            combined_results = self._cross_encoder_rerank(
                query, combined_results
            )
        
        return combined_results[:top_k]
    
    def _reciprocal_rank_fusion(self, semantic_results: List[Tuple], 
                               lexical_scores: np.ndarray, 
                               k: int = 60) -> List[Dict]:
        """Implement Reciprocal Rank Fusion for score combination."""
        
        # Create document score dictionary
        doc_scores = {}
        
        # Add semantic scores (convert similarity to rank)
        for rank, (doc, similarity_score) in enumerate(semantic_results):
            doc_id = doc.metadata.get('chunk_id', rank)
            doc_scores[doc_id] = {
                'document': doc,
                'semantic_rrf': 1 / (k + rank + 1),
                'semantic_score': similarity_score
            }
        
        # Add lexical scores (BM25 rankings)
        lexical_rankings = np.argsort(-lexical_scores)  # Descending order
        for rank, doc_idx in enumerate(lexical_rankings[:len(semantic_results)]):
            doc_id = doc_idx  # Assuming document index as ID
            
            if doc_id in doc_scores:
                doc_scores[doc_id]['lexical_rrf'] = 1 / (k + rank + 1)
                doc_scores[doc_id]['lexical_score'] = lexical_scores[doc_idx]
            else:
                # Create entry for lexical-only results
                doc_scores[doc_id] = {
                    'document': self.documents[doc_idx],
                    'semantic_rrf': 0,
                    'lexical_rrf': 1 / (k + rank + 1),
                    'semantic_score': 0,
                    'lexical_score': lexical_scores[doc_idx]
                }
        
        # Calculate final RRF scores
        for doc_id in doc_scores:
            semantic_rrf = doc_scores[doc_id].get('semantic_rrf', 0)
            lexical_rrf = doc_scores[doc_id].get('lexical_rrf', 0)
            doc_scores[doc_id]['final_score'] = semantic_rrf + lexical_rrf
        
        # Sort by final score
        sorted_results = sorted(
            doc_scores.values(),
            key=lambda x: x['final_score'],
            reverse=True
        )
        
        return sorted_results
```

### **Cross-Encoder Reranking**

Cross-encoders provide more accurate relevance scoring by processing query-document pairs jointly:

```python
# Cross-encoder reranking implementation
from sentence_transformers import CrossEncoder

class AdvancedReranker:
    """Cross-encoder based reranking for improved precision."""
    
    def __init__(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        self.cross_encoder = CrossEncoder(model_name)
        self.model_name = model_name
        print(f"Loaded cross-encoder model: {model_name}")
    
    def rerank_results(self, query: str, documents: List[Dict], 
                      top_k: int = 10) -> List[Dict]:
        """Rerank documents using cross-encoder scores."""
        
        if not documents:
            return []
        
        # Prepare query-document pairs
        pairs = []
        for doc_data in documents:
            doc_text = self._extract_text(doc_data)
            pairs.append([query, doc_text])
        
        # Get cross-encoder scores
        ce_scores = self.cross_encoder.predict(pairs)
        
        # Update documents with reranking scores
        for i, doc_data in enumerate(documents):
            doc_data['rerank_score'] = float(ce_scores[i])
            doc_data['original_rank'] = i + 1
        
        # Sort by cross-encoder scores
        reranked = sorted(
            documents,
            key=lambda x: x['rerank_score'],
            reverse=True
        )
        
        return reranked[:top_k]
```

---

## **Part 3: Performance Optimization Strategies (20 minutes)**

### **Vector Index Optimization**

Different indexing algorithms offer trade-offs between speed, accuracy, and memory usage:

```python
# Advanced indexing strategies
import faiss
import numpy as np
from typing import Tuple

class OptimizedVectorIndex:
    """Advanced vector indexing with multiple algorithms."""
    
    def __init__(self, dimension: int, index_type: str = "IVF"):
        self.dimension = dimension
        self.index_type = index_type
        self.index = None
        self.id_map = {}  # Map internal IDs to external IDs
        
    def build_index(self, vectors: np.ndarray, 
                   external_ids: List[str]) -> None:
        """Build optimized index based on data size and requirements."""
        
        n_vectors = vectors.shape[0]
        print(f"Building {self.index_type} index for {n_vectors} vectors")
        
        if self.index_type == "IVF":
            self.index = self._build_ivf_index(vectors, n_vectors)
        elif self.index_type == "HNSW":
            self.index = self._build_hnsw_index(vectors)
        elif self.index_type == "LSH":
            self.index = self._build_lsh_index(vectors)
        else:
            # Fallback to flat index
            self.index = self._build_flat_index(vectors)
        
        # Store ID mapping
        for i, external_id in enumerate(external_ids):
            self.id_map[i] = external_id
```

**Step 8: IVF Index for Large Scale**
```python
    def _build_ivf_index(self, vectors: np.ndarray, 
                        n_vectors: int) -> faiss.Index:
        """Build Inverted File (IVF) index for large-scale search."""
        
        # Determine optimal number of centroids
        n_centroids = min(4 * int(np.sqrt(n_vectors)), n_vectors // 10)
        n_centroids = max(n_centroids, 32)  # Minimum centroids
        
        # Create IVF index with PQ (Product Quantization) for compression
        quantizer = faiss.IndexFlatIP(self.dimension)  # Inner product
        
        if n_vectors > 100000:  # Use PQ for large datasets
            m = 8  # Number of sub-vectors
            n_bits = 8  # Bits per sub-vector
            index = faiss.IndexIVFPQ(quantizer, self.dimension, 
                                   n_centroids, m, n_bits)
        else:
            index = faiss.IndexIVFFlat(quantizer, self.dimension, n_centroids)
        
        # Train the index
        print(f"Training IVF index with {n_centroids} centroids...")
        index.train(vectors)
        
        # Add vectors
        index.add(vectors)
        
        # Set search parameters
        index.nprobe = min(n_centroids // 4, 128)  # Number of clusters to search
        
        print(f"Built IVF index: {index.ntotal} vectors indexed")
        return index
```

**Step 9: HNSW for Speed**
```python
    def _build_hnsw_index(self, vectors: np.ndarray) -> faiss.Index:
        """Build HNSW index for fast approximate search."""
        
        # HNSW parameters
        M = 32  # Number of bi-directional connections
        ef_construction = 200  # Size of dynamic candidate list
        
        index = faiss.IndexHNSWFlat(self.dimension, M)
        index.hnsw.efConstruction = ef_construction
        
        # Add vectors
        print(f"Building HNSW index with M={M}, ef_construction={ef_construction}")
        index.add(vectors)
        
        # Set search parameter
        index.hnsw.efSearch = 128  # Search-time parameter
        
        print(f"Built HNSW index: {index.ntotal} vectors indexed")
        return index
```

### **Search Performance Optimization**

Implement caching and query optimization for production performance:

```python
# Search optimization with caching
from functools import lru_cache
import hashlib
import pickle
from typing import Optional

class OptimizedSearchEngine:
    """Production search engine with caching and optimization."""
    
    def __init__(self, vector_store, cache_size: int = 1000):
        self.vector_store = vector_store
        self.cache_size = cache_size
        self.query_cache = {}
        self.embedding_cache = {}
        
    @lru_cache(maxsize=1000)
    def _get_query_embedding(self, query: str) -> Tuple[float, ...]:
        """Cache query embeddings to avoid recomputation."""
        embedding = self.vector_store.embedding_function.embed_query(query)
        return tuple(embedding)  # Hashable for LRU cache
    
    def optimized_search(self, query: str, top_k: int = 10,
                        filters: Optional[Dict] = None,
                        use_cache: bool = True) -> List[Dict]:
        """Optimized search with caching and preprocessing."""
        
        # Create cache key
        cache_key = self._create_cache_key(query, top_k, filters)
        
        # Check cache
        if use_cache and cache_key in self.query_cache:
            print("Cache hit!")
            return self.query_cache[cache_key]
        
        # Preprocess query
        processed_query = self._preprocess_query(query)
        
        # Get embedding (with caching)
        query_embedding = list(self._get_query_embedding(processed_query))
        
        # Perform search with optimizations
        results = self._search_with_optimizations(
            query_embedding, top_k, filters
        )
        
        # Cache results
        if use_cache and len(self.query_cache) < self.cache_size:
            self.query_cache[cache_key] = results
        
        return results
```

---

## **Part 4: Advanced Retrieval Techniques (30 minutes)**

### **Multi-Stage Retrieval Pipeline**

Implement sophisticated retrieval with multiple stages for optimal precision:

```python
# Multi-stage retrieval implementation
from typing import List, Dict, Any, Optional, Callable
import asyncio
import concurrent.futures

class MultiStageRetriever:
    """Advanced multi-stage retrieval pipeline."""
    
    def __init__(self, vector_store, reranker, query_enhancer):
        self.vector_store = vector_store
        self.reranker = reranker
        self.query_enhancer = query_enhancer
        
        # Retrieval stages configuration
        self.stages = [
            {'name': 'initial_retrieval', 'k': 100},
            {'name': 'keyword_filtering', 'k': 50}, 
            {'name': 'reranking', 'k': 20},
            {'name': 'final_filtering', 'k': 10}
        ]
    
    async def retrieve_multi_stage(self, query: str, 
                                 target_k: int = 10) -> List[Dict]:
        """Execute multi-stage retrieval pipeline."""
        
        print(f"Starting multi-stage retrieval for: {query[:100]}...")
        
        # Stage 1: Enhanced query generation
        enhanced_queries = await self._generate_query_variants(query)
        
        # Stage 2: Parallel initial retrieval
        initial_results = await self._parallel_retrieval(enhanced_queries)
        
        # Stage 3: Merge and deduplicate results
        merged_results = self._merge_and_deduplicate(initial_results)
        
        # Stage 4: Apply progressive filtering
        filtered_results = await self._progressive_filtering(
            query, merged_results, target_k
        )
        
        return filtered_results
```

**Step 10: Query Enhancement for Retrieval**
```python
    async def _generate_query_variants(self, query: str) -> List[Dict]:
        """Generate multiple query variants for comprehensive retrieval."""
        
        variants = []
        
        # Original query
        variants.append({'type': 'original', 'query': query, 'weight': 1.0})
        
        # Expanded query with synonyms
        expanded = await self._expand_with_synonyms(query)
        variants.append({'type': 'expanded', 'query': expanded, 'weight': 0.8})
        
        # Hypothetical document (HyDE)
        hyde_doc = await self._generate_hypothetical_document(query)
        variants.append({'type': 'hyde', 'query': hyde_doc, 'weight': 0.9})
        
        # Question decomposition
        sub_queries = await self._decompose_question(query)
        for i, sub_q in enumerate(sub_queries):
            variants.append({
                'type': 'sub_query', 
                'query': sub_q, 
                'weight': 0.6,
                'index': i
            })
        
        return variants
    
    async def _expand_with_synonyms(self, query: str) -> str:
        """Expand query with synonyms and related terms."""
        # Implementation would use WordNet, domain-specific thesaurus, 
        # or LLM for synonym expansion
        expansion_prompt = f"""
        Expand this query by adding relevant synonyms and related terms:
        Query: {query}
        
        Provide an expanded version that includes synonyms but maintains clarity:
        """
        # Use your LLM here
        return query  # Simplified for example
```

**Step 11: Parallel Retrieval Execution**
```python
    async def _parallel_retrieval(self, query_variants: List[Dict]) -> List[List[Dict]]:
        """Execute retrieval for multiple query variants in parallel."""
        
        async def retrieve_single(variant: Dict) -> List[Dict]:
            """Retrieve results for a single query variant."""
            results = self.vector_store.similarity_search_with_scores(
                variant['query'], 
                k=self.stages[0]['k']
            )
            
            # Add variant metadata
            for result in results:
                result['variant_type'] = variant['type']
                result['variant_weight'] = variant['weight']
                if 'index' in variant:
                    result['sub_query_index'] = variant['index']
            
            return results
        
        # Execute retrievals concurrently
        tasks = [retrieve_single(variant) for variant in query_variants]
        results = await asyncio.gather(*tasks)
        
        return results
```

**Step 12: Advanced Result Merging**
```python
    def _merge_and_deduplicate(self, result_lists: List[List[Dict]]) -> List[Dict]:
        """Merge results from multiple retrievals with deduplication."""
        
        # Flatten all results
        all_results = []
        for result_list in result_lists:
            all_results.extend(result_list)
        
        # Group by document content hash
        content_groups = {}
        for result in all_results:
            content_hash = hashlib.md5(
                result['document'].page_content.encode()
            ).hexdigest()
            
            if content_hash not in content_groups:
                content_groups[content_hash] = []
            content_groups[content_hash].append(result)
        
        # Merge grouped results
        merged_results = []
        for content_hash, group in content_groups.items():
            # Calculate combined score
            combined_score = self._calculate_combined_score(group)
            
            # Take the first result as base
            base_result = group[0].copy()
            base_result['combined_score'] = combined_score
            base_result['variant_count'] = len(group)
            base_result['variant_types'] = [r['variant_type'] for r in group]
            
            merged_results.append(base_result)
        
        # Sort by combined score
        merged_results.sort(
            key=lambda x: x['combined_score'], 
            reverse=True
        )
        
        return merged_results
    
    def _calculate_combined_score(self, result_group: List[Dict]) -> float:
        """Calculate combined relevance score for grouped results."""
        
        # Weighted average of scores
        total_weighted_score = 0
        total_weight = 0
        
        for result in result_group:
            weight = result['variant_weight']
            score = 1 - result['similarity_score']  # Convert distance to similarity
            
            total_weighted_score += score * weight
            total_weight += weight
        
        if total_weight == 0:
            return 0.0
        
        # Boost score based on how many variants found this result
        variant_boost = min(len(result_group) * 0.1, 0.3)
        combined_score = (total_weighted_score / total_weight) + variant_boost
        
        return min(combined_score, 1.0)
```

### **Contextual Filtering and Ranking**

Implement sophisticated filtering based on context and user preferences:

```python
# Advanced filtering and ranking
class ContextualFilter:
    """Context-aware filtering and ranking system."""
    
    def __init__(self, llm_model):
        self.llm_model = llm_model
        self.filter_strategies = {
            'temporal': self._temporal_filter,
            'semantic_coherence': self._semantic_coherence_filter,
            'diversity': self._diversity_filter,
            'authority': self._authority_filter
        }
    
    def apply_contextual_filters(self, query: str, results: List[Dict],
                               context: Dict, target_k: int = 10) -> List[Dict]:
        """Apply multiple contextual filters progressively."""
        
        filtered_results = results.copy()
        
        # Apply each filter strategy
        for filter_name, filter_func in self.filter_strategies.items():
            if context.get(f'use_{filter_name}', True):
                filtered_results = filter_func(
                    query, filtered_results, context
                )
                print(f"After {filter_name}: {len(filtered_results)} results")
        
        # Final ranking
        final_results = self._final_ranking(
            query, filtered_results, context, target_k
        )
        
        return final_results
```

**Step 13: Semantic Coherence Filtering**
```python
    def _semantic_coherence_filter(self, query: str, results: List[Dict], 
                                 context: Dict) -> List[Dict]:
        """Filter results based on semantic coherence with query intent."""
        
        # Use LLM to assess semantic relevance
        coherence_scores = []
        
        for result in results:
            doc_text = result['document'].page_content[:500]  # Limit for efficiency
            
            coherence_prompt = f"""
            Rate the semantic relevance of this document excerpt to the query on a scale of 1-10:
            
            Query: {query}
            
            Document Excerpt: {doc_text}
            
            Consider:
            1. How directly the document answers the query
            2. Topical relevance and accuracy
            3. Completeness of information
            
            Return only a number from 1-10:
            """
            
            try:
                score_text = self.llm_model.predict(coherence_prompt).strip()
                coherence_score = float(score_text)
                coherence_scores.append(coherence_score)
            except:
                coherence_scores.append(5.0)  # Default score
        
        # Filter based on coherence threshold
        coherence_threshold = context.get('coherence_threshold', 6.0)
        filtered_results = [
            result for result, score in zip(results, coherence_scores)
            if score >= coherence_threshold
        ]
        
        # Add coherence scores to results
        for result, score in zip(filtered_results, coherence_scores):
            result['coherence_score'] = score
        
        return filtered_results
```

---

## **üß™ Hands-On Exercise: Build Optimized Vector Search**

### **Your Mission**
Create a production-ready vector search system with hybrid retrieval and optimization features.

### **Requirements:**
1. **Vector Database Setup**: Deploy Chroma and Pinecone instances with optimized configurations
2. **Hybrid Search**: Implement semantic + BM25 fusion with RRF scoring  
3. **Multi-Stage Pipeline**: Build retrieval with query enhancement and reranking
4. **Performance Optimization**: Add caching, parallel processing, and index optimization
5. **Evaluation**: Compare search quality across different configurations

### **Implementation Steps:**
```python
# Complete exercise implementation
class ProductionVectorSearch:
    """Complete production vector search system."""
    
    def __init__(self, config: Dict):
        # Initialize multiple vector stores
        self.vector_stores = self._setup_vector_stores(config)
        
        # Setup hybrid search
        self.hybrid_engine = HybridSearchEngine(
            self.vector_stores['primary'], 
            config['documents']
        )
        
        # Multi-stage retriever
        self.multi_stage = MultiStageRetriever(
            self.vector_stores['primary'],
            CrossEncoder(), 
            QueryEnhancer()
        )
        
        # Performance optimizations
        self.search_cache = {}
        self.performance_metrics = {}
    
    async def production_search(self, query: str, 
                               search_type: str = "hybrid",
                               top_k: int = 10) -> Dict:
        """Production-ready search with full pipeline."""
        
        start_time = time.time()
        
        if search_type == "hybrid":
            results = self.hybrid_engine.hybrid_search(query, top_k)
        elif search_type == "multi_stage":
            results = await self.multi_stage.retrieve_multi_stage(query, top_k)
        else:
            results = self.vector_stores['primary'].similarity_search(query, top_k)
        
        # Track performance
        search_time = time.time() - start_time
        self._update_metrics(search_type, search_time, len(results))
        
        return {
            'results': results,
            'search_time': search_time,
            'search_type': search_type,
            'total_results': len(results)
        }
```

---

## **üìù Chapter Summary**

### **What You've Built**
- ‚úÖ Multi-database vector search infrastructure (Chroma, Pinecone, Qdrant)
- ‚úÖ Hybrid search combining semantic and lexical retrieval
- ‚úÖ Multi-stage retrieval with query enhancement and reranking
- ‚úÖ Performance optimizations with caching and parallel processing
- ‚úÖ Advanced filtering and contextual ranking systems

### **Key Technical Skills Learned**
1. **Vector Database Architecture**: Configuration, optimization, and scaling strategies
2. **Hybrid Search**: RRF fusion, BM25 scoring, and multi-modal retrieval
3. **Performance Engineering**: Indexing strategies, caching, and parallel processing
4. **Advanced Retrieval**: Multi-stage pipelines, query enhancement, and contextual filtering
5. **Production Deployment**: Monitoring, error handling, and system optimization

### **Performance Benchmarks**
- **Search Latency**: <100ms for most queries with proper indexing
- **Index Build**: Optimized for 1M+ documents with incremental updates
- **Cache Hit Rate**: 70-80% for common query patterns
- **Retrieval Quality**: 15-20% improvement with hybrid search over pure semantic

---

## **üß™ Knowledge Check**

Test your understanding of vector databases and search optimization techniques with our comprehensive assessment.

### **Multiple Choice Questions**

**1. Which vector database metric is most suitable for RAG applications using cosine similarity?**
   - A) Euclidean distance
   - B) Manhattan distance
   - C) Cosine similarity
   - D) Hamming distance

**2. What is the primary advantage of HNSW indexing over IVF indexing?**
   - A) Lower memory usage
   - B) Better compression ratios
   - C) Faster query performance with high recall
   - D) Simpler configuration

**3. In Reciprocal Rank Fusion (RRF), what does the 'k' parameter control?**
   - A) Number of results to return
   - B) Weight balance between semantic and lexical scores
   - C) The smoothing factor in rank combination
   - D) Maximum number of query variants

**4. What is the key benefit of cross-encoder reranking compared to bi-encoder similarity?**
   - A) Faster inference speed
   - B) Lower computational requirements
   - C) Joint processing of query-document pairs for better accuracy
   - D) Simpler model architecture

**5. When should you choose IVF indexing over HNSW for vector search?**
   - A) When you need the fastest possible queries
   - B) When you have limited memory and large datasets
   - C) When accuracy is more important than speed
   - D) When you need real-time updates

**6. What is the purpose of the 'ef_construction' parameter in HNSW?**
   - A) Controls memory usage during search
   - B) Determines the number of connections per node
   - C) Sets the dynamic candidate list size during index building
   - D) Defines the maximum number of layers

**7. In hybrid search, what does BM25 provide that semantic search lacks?**
   - A) Better understanding of context
   - B) Exact term matching and frequency analysis
   - C) Handling of synonyms and related concepts
   - D) Multi-language support

**8. Why is query caching particularly effective in RAG systems?**
   - A) Vector embeddings are expensive to compute
   - B) Users often ask similar or repeated questions
   - C) Database queries are the main bottleneck
   - D) All of the above

---

**üìã [View Solutions](Session3_Test_Solutions.md)**

*Complete the test above, then check your answers and review the detailed explanations in the solutions.*

---

## **üîó Next Session Preview**

In **Session 4: Query Enhancement & Context Augmentation**, we'll explore:
- **HyDE (Hypothetical Document Embeddings)** for bridging semantic gaps
- **Query expansion and reformulation** using LLMs and domain knowledge
- **Multi-query generation** for comprehensive retrieval coverage
- **Context window optimization** and smart chunking strategies
- **Advanced prompt engineering** for retrieval-augmented generation

### **Preparation Tasks**
1. Deploy your optimized vector search system with multiple databases
2. Collect query examples that are challenging for basic semantic search
3. Experiment with different hybrid search weights and fusion strategies
4. Analyze your search performance metrics and identify bottlenecks

Excellent progress! You now have a robust, production-ready vector search foundation. üöÄ